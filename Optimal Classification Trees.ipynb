{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#package installer to use as needed\n",
    "#import Pkg\n",
    "#Pkg.add(\"MLDataUtils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-03 15:21:16.301"
     ]
    }
   ],
   "source": [
    "using Dates\n",
    "print(Dates.today(), \" \", Dates.Time(Dates.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Classification Trees\n",
    "\n",
    "## Stephen Ronkowski\n",
    "\n",
    "The following is an Julia 1.x implementation of the Optimal Classification Tree model developed by Bertsimas and Dunn in their [2017 paper](https://www.mit.edu/~dbertsim/papers/Machine%20Learning%20under%20a%20Modern%20Optimization%20Lens/Optimal_classification_trees_MachineLearning.pdf).  Traditional Decision Tree models are perhaps the most easily interpretated classification model available to the data scientist. These models, however, suffer from the inherently greedy nature of their solving methods, which use a loss function at each level of depth for the Tree. This methodology is computationally efficient, but it gains that efficiency at the possible cost of accuracy. \n",
    "\n",
    "Given the massive advances in both computational power and solver performance, it is now computationally feasible to compose a Decision Tree using a single loss function using Mixed Integer Optimization, ensuring that splits made in early branches of the tree generate the lowest possible error rates in the leaf nodes.\n",
    "\n",
    "I begin reproducing this model by loading the needed Julia packages.  While I chose Gurobi for a solver, other MIO-compatible solvers can easily be deployed by changing the declaration below and then modifying the model delcaration appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load needed modules\n",
    "using JuMP\n",
    "using CSV\n",
    "using DecisionTree\n",
    "using StatsBase\n",
    "using DataFrames\n",
    "using MLDataUtils\n",
    "\n",
    "#note - Gurobi is not FOSS - licensing required!\n",
    "#this model can be solved using any MIO solver compatible with JuMP\n",
    "#see http://www.juliaopt.org/JuMP.jl/v0.20.0/installation/#Getting-Solvers-1\n",
    "using Gurobi\n",
    "\n",
    "#data path declaration\n",
    "FILEDIR = \"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion & Processing\n",
    "\n",
    "Both for ease of reference when establishing the model variables \\& constraints and as a point of comparison with a \"traditional\" Decision Tree, I will load in a dataset to classify.  I chose the UCI \"Wine\" dataset available on [their website](https://archive.ics.uci.edu/ml/datasets/Wine), but as I will demonstrate in other notebooks, this general model should work for any classification dataset.\n",
    "\n",
    "I will load the dataset from CSV, and shuffle the rows to ensure an even distribution of classes in the test and train sets.  Once the data is split into test and train sets, I will extract the label column and place it into a seperate array.  Also, since the model requires that all feature values are $0 \\leq x_i \\leq 1$, I will use a unit range transformation on each feature column to meet this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Array{Int64,1}:\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " â‹®\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load wine file\n",
    "csv = CSV.File(FILEDIR * \"wine.data\")\n",
    "df = DataFrame(csv)\n",
    "\n",
    "#extract feature array, fit to UnitTransformer\n",
    "features_to_fit = Matrix(select(df, Not(:Label)))\n",
    "unit_transformer = fit(UnitRangeTransform, transpose(features_to_fit))\n",
    "\n",
    "#shuffle observations and split into train/test\n",
    "df = shuffleobs(df)\n",
    "train, test = splitobs(df, at = 0.7)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "features = Matrix(select(train, Not(:Label)))\n",
    "features = StatsBase.transform(unit_transformer, transpose(features))\n",
    "features = Matrix(transpose(features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "labels = Array(train.Label)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "test_features = Matrix(select(test, Not(:Label)))\n",
    "test_features = StatsBase.transform(unit_transformer, transpose(test_features))\n",
    "test_features = Matrix(transpose(test_features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "test_labels = Array(test.Label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels must be one-indexed for the model to function properly, so I confirm as much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 1\n",
       " 3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = labels .- 2\n",
    "unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of comparison, I will now fit a simple Decision Tree over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 13, Threshold 0.31990014265335237\n",
      "L-> Feature 12, Threshold 0.3095238095238095\n",
      "    L-> Feature 11, Threshold 0.39837398373983735\n",
      "        L-> 3 : 23/24\n",
      "        R-> 2 : 3/3\n",
      "    R-> Feature 7, Threshold 0.08544303797468353\n",
      "        L-> 3 : 1/1\n",
      "        R-> 2 : 44/45\n",
      "R-> Feature 7, Threshold 0.38502109704641346\n",
      "    L-> Feature 11, Threshold 0.26260162601626014\n",
      "        L-> 3 : 6/6\n",
      "        R-> 2 : 2/2\n",
      "    R-> Feature 4, Threshold 0.8711340206185567\n",
      "        L-> 1 : 43/43\n",
      "        R-> 2 : 1/1\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "DecisionTree.fit!(dt_model, features, labels)\n",
    "print_tree(dt_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warmStartODT (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function warmStartODT(inputs)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "The algorithm requires a set of dynamically generated constants that are derived from the dataset.  The first of these is epsilon, $\\epsilon_t$. Epsilon is essentially a \"fudge factor\" to overcome MIO solvers' inability to solve strict inequalities. Since the solver will not accept $a^Tx < b_t$, we find the smallest non-zero difference in array $x$ and instead use $a^Tx  + \\epsilon_t \\leq b_t$ for this constraint.\n",
    "\n",
    "$A_L$ and $A_R$ are a way of mathematically modelling the Decision Tree itself.  For a given leaf node $t$ within the Decison Tree, $A_L$ is the set of all left turns made to arrive at that leaf, and $A_L$ is the set of all right turns. For example, in the tree below, $A_L(4) = \\{1, 2\\}$.\n",
    "\n",
    "![title](tree_diagram.png)\n",
    "\n",
    "Additionally, we need to generate matrix $Y$ for use in the loss calculation constraints within the model space.  For a given $x_i$ with known label $k$ in the training set, $Y_{i,k} = 1$ if $x_i = k$; otherwise $Y_{i,k} = -1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeYMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function findEpsilon(array)\n",
    "    #capture array length for iteration\n",
    "    array_len = size(array, 1)\n",
    "    \n",
    "    #sort array\n",
    "    sorted_array = sort(array, rev = true)\n",
    "    \n",
    "    #initialize epsilon with arbitrarily large value\n",
    "    epsilon = 1e5\n",
    "    \n",
    "    #iterate through consecutive values to find smallest non-zero difference\n",
    "    for i in 2:(array_len - 1)\n",
    "        diff = abs(sorted_array[i-1] - sorted_array[i])\n",
    "        if 0 < diff < epsilon\n",
    "            epsilon = diff\n",
    "        end\n",
    "    end\n",
    "    #output smallest non-zero difference\n",
    "    return epsilon\n",
    "end\n",
    "\n",
    "function epsilonArrayGenerator(matrix)\n",
    "    #capture number of features in dataset, i.e. the number of columns\n",
    "    matrix_rows = size(matrix, 2)\n",
    "    \n",
    "    #initialize output DataFrame\n",
    "    epsilon_array = Vector{Float64}(undef,matrix_rows)\n",
    "    \n",
    "    #iterate findEpsilon function over each column\n",
    "    for col in 1:matrix_rows\n",
    "        epsilon_array[col] = findEpsilon(matrix[:,col])\n",
    "    end\n",
    "    return epsilon_array\n",
    "end\n",
    "\n",
    "function makeAncestorDict(max_nodes)\n",
    "    #initialize empty dictionaries\n",
    "    A_left = Dict{Int64, Vector{Int64}}()\n",
    "    A_right = Dict{Int64, Vector{Int64}}()\n",
    "    #A_left[1] = [1]\n",
    "    #A_right[1] = [1]\n",
    "    #generate keys with empty array values for each node\n",
    "    for i in 1:max_nodes\n",
    "        A_left[i] = []\n",
    "        A_right[i] = []\n",
    "    end\n",
    "    #loop over all nodes, copying the left and right ancestors of the node above it\n",
    "    for i in 2:max_nodes\n",
    "        left_ancestors = copy(A_left[i Ã· 2])\n",
    "        right_ancestors = copy(A_right[i Ã· 2])\n",
    "        direct_ancestor = i Ã· 2\n",
    "        A_left[i] = left_ancestors\n",
    "        A_right[i] = right_ancestors\n",
    "        #add a left ancestor to even nodes\n",
    "        if i/2 == i Ã· 2\n",
    "            append!(left_ancestors, direct_ancestor)\n",
    "            A_left[i] = left_ancestors\n",
    "        #add a right ancestor to odd nodes\n",
    "        else\n",
    "            append!(right_ancestors, direct_ancestor)\n",
    "            A_right[i] = right_ancestors\n",
    "        end\n",
    "    end\n",
    "    return A_left, A_right\n",
    "end\n",
    "\n",
    "function makeYMatrix(labels)    \n",
    "    #extract dimensions for Y from label array\n",
    "    num_labels = length(unique(labels))\n",
    "    len_df = length(labels)\n",
    "    \n",
    "    #initialize empty matrix\n",
    "    Y = zeros(len_df, num_labels)\n",
    "    \n",
    "    #set all values to -1 - this will apply a penalty to incorrect predictions\n",
    "    Y = Y  .- 1\n",
    "    \n",
    "    #iterate n over each column, setting Y[n,k] = 1 when the label for x[i] = k\n",
    "    for k in 1:num_labels\n",
    "        for n in 1:len_df\n",
    "            if labels[n] == k\n",
    "                Y[n,k] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration of Model Constants\n",
    "\n",
    "In a given model space for this optimization function, there are a variety of constants that are either set by the user as a hyperparameter or generated dynamically based on those hyperparameters or the input data set.\n",
    "\n",
    "I will begin by establishing the numerical values related to the structure of the tree, $D, N_{min}, t$, and $\\alpha$  Here, $D$ is the maximum depth of the Tree, $N_{min}$ is the minimum number of samples needed to compose a leaf node, and $t$ is the total possible number of nodes in a tree.\n",
    "\n",
    "The last of these user-declared variables, $\\alpha$, is a complexity parameter that penalizes overly complex tree structures. (elaborate here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Int64,1}:\n",
       "  8\n",
       "  9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set maximum depth of tree as a constant\n",
    "max_depth = 3\n",
    "\n",
    "#minimum number of values for a given leaf node\n",
    "leaf_n_min = 3\n",
    "\n",
    "#declare complexity parameter alpha\n",
    "alpha = 0.1\n",
    "\n",
    "#find total number of nodes in tree using max_depth, t\n",
    "max_nodes = 2^(max_depth+1) - 1\n",
    "\n",
    "#initialize branch and leaf node arrays - first, find the split point between branch and leaf indices\n",
    "#split point is by definition the number of nodes integer divided by two\n",
    "leaf_branch_split = max_nodes Ã· 2\n",
    "\n",
    "#total number of branches\n",
    "t_b = collect(1:leaf_branch_split)\n",
    "\n",
    "#total number of leaves\n",
    "t_l = collect(leaf_branch_split+1:max_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will establish the model constants that are derived from the input dataset: $n, k$, and $p$.  Here, $n$ represents the number of samples within the dataset, $p$ represents the number of features in the dataset, and $k$ represents the total number of labels.\n",
    "\n",
    "I will also establish the constant $\\hat{L}$. This constant represents the \"naive\" prediction, namely, that every value in the dataset is a member of the most common class.  This value is then used within the optimization function (elaborate here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull number of samples in dataset, n\n",
    "num_samples = size(features, 1)\n",
    "\n",
    "#find total number of columns in the feature space, p\n",
    "num_features = size(features, 2)\n",
    "\n",
    "#find total number of labels, k\n",
    "num_labels = length(unique(labels))\n",
    "\n",
    "#create dictionary with prediction labels as key and count of each prediction as value\n",
    "output_count = countmap(labels)\n",
    "\n",
    "#extract the count for most common label to form l_hat, which is baseline accuracy rate\n",
    "l_hat = sort(collect(output_count), by = tuple -> last(tuple), rev=true)[1,1][2]/length(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I will build out the other dataset-specific constants - $A_L, A_R$, and $\\epsilon_j$ - using the helper functions declared earlier.  With these values established, we can then define $M_1$ as 1 + $max(\\epsilon_j)$ and $M$ as the number of features in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125Ã—3 Array{Float64,2}:\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  â‹®              \n",
       " -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build ancestor dictionaries using definitions given above\n",
    "A_left, A_right = makeAncestorDict(max_nodes)\n",
    "\n",
    "#generate the epsilon array as defined earlier\n",
    "epsilon_array = epsilonArrayGenerator(features)\n",
    "\n",
    "#M_1 constant - defined as 1 plus the largest epsilon value\n",
    "M_1 = 1 + maximum(epsilon_array)\n",
    "\n",
    "#M constant - set equal to number of samples as rule of thumb\n",
    "M = length(labels)\n",
    "\n",
    "#generate Y matrix\n",
    "Y = makeYMatrix(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable and Constraint Declarations\n",
    "\n",
    "With the constants now fixed, the model itself can now be built inside JuMP.  The first step is to declare the model itself, at which point I also limit the runtime of the optimizer to two hours.\n",
    "\n",
    "From there, I will begin building the model by establishing the variables that model the structure of the tree itself.  The first of these are $b$, an array that captures the decision point for each node that applies a split, and $a_{j,t}$, a hot-coded matrix that indicates when feature $j$ is used to split at node $t$. Additionally, we initialize array $d$, which is hot-coded to indicate when a given branch is active (i.e. a split is applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model = Model(with_optimizer(Gurobi.Optimizer, Presolve=0, OutputFlag=1, TimeLimit=7200))\n",
    "\n",
    "#b is the decision point for each branch node\n",
    "#s/t a.T*x < b at a given split \n",
    "@variable(model, b[i=t_b])\n",
    "\n",
    "#a is a hot-coded matrix that captures the variable being used to split at given branch node\n",
    "@variable(model, a[j = 1:num_features, t = 1:leaf_branch_split])\n",
    "\n",
    "#4 - establish binary constraint on a\n",
    "for j in 1:num_features\n",
    "    for t in t_b\n",
    "        @constraint(model, a[j,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#d is an indicator array equal to one when a split is applied at a given node\n",
    "@variable(model, d[1:leaf_branch_split])\n",
    "\n",
    "#constrain d to binary values\n",
    "for t in t_b\n",
    "    @constraint(model, d[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#2 - establish that row-wise sum of a must equal 1 for all rows - yes\n",
    "for t in t_b\n",
    "    #@constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == 1)\n",
    "end\n",
    "\n",
    "#3 - establish that b split point must be 0 <= b[i] <= d[i] - yes\n",
    "for t in t_b\n",
    "    @constraint(model, b[t] >= 0)\n",
    "    @constraint(model, d[t] >= b[t])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is possible that certain branches will not be needed to achieve an optimal solution, I will establish that only those nodes which have a split applied above them can also apply a split.  This constraint ensures that once the optimizer no longer needs to split along a given branch path (i.e. that a given branch has already achieved perfect purity) it will simply route the input features to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- constrain d s/t splits cannot be applied below a node that does not also split\n",
    "#this does not apply to d[1] since that is the parent node and must always split\n",
    "@constraint(model, d[1] == 1)\n",
    "\n",
    "for t in 2:leaf_branch_split\n",
    "    parent = t Ã· 2\n",
    "    @constraint(model, d[t] <= d[parent])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z is a hot-coded matrix captures which values are assigned to which node\n",
    "@variable(model, z[i = 1:num_samples, t = t_l])\n",
    "\n",
    "#constrain z to binary values {0,1}\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#l is a hot-coded array s/t l(t) = 1 when leaf node t contains any values \n",
    "@variable(model, l[t = t_l])\n",
    "\n",
    "#contrain l to binary values {0,1}\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#6 - constrain predictions to only be fit into nodes containing points\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] <= l[t])\n",
    "    end\n",
    "end\n",
    "\n",
    "#7- constrain number of samples assigned to a given leaf by lower bound \n",
    "#s/t number of samples is always greater/equal to min leaf size constant\n",
    "for t in t_l\n",
    "    @constraint(model, sum(z[i,t] for i in 1:num_samples) >= leaf_n_min * l[t])\n",
    "end\n",
    "\n",
    "#8 - constrain each point in data set so it can only be assigned to one leaf node\n",
    "for i in 1:num_samples\n",
    "    @constraint(model, sum(z[i,t] for t in t_l) == 1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tree structure and variable spaces now established, I move on to the splitting constraints.  These constraints capture the \"path\" that leads to each leaf node.  For leaf node $t$, the left-hand path is taken at ancestor node(s) $A_{left}(t) = m$ when $A^T(x_i + \\epsilon) \\leq b(m)$, and the right-hand path is taken at ancestor node(s) $A_{right}(t) = m$ when $A^Tx_i \\geq b(m)$.\n",
    "\n",
    "(Note that equations 9-12 are intermediate steps that give the derivations below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 - establish left split constraints\n",
    "for i in 1:num_samples  \n",
    "    for t in t_l\n",
    "        for m in A_left[t]\n",
    "            @constraint(model, transpose(a[:,m]) * (features[i,:] + epsilon_array) <= b[m] + M_1*(1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "#14 - establish right split contraints\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        for m in A_right[t]      \n",
    "            @constraint(model, transpose(a[:,m]) * features[i,:] >= b[m] - (1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the branch nodes now fully modelled, we turn to a series of variables that capture the $x_i$ features present within each node.  The first of these is $N_{k,t}$, which gives the total number of inputs with label $k$ in leaf node $t$.  Paired with this is $N_t$, which gives the sum total of inputs assigned to each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_kt is the number of points with label k in leaf node t\n",
    "@variable(model, N_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#15 - establish values for N_kt[t]\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, N_kt[k,t] == 0.5 * sum((1 + Y[i,k])*z[i,t] for i = 1:num_samples))\n",
    "    end\n",
    "end\n",
    "\n",
    "#N_t is the total number of values in a leaf node t\n",
    "@variable(model, N_t[i = t_l])\n",
    "\n",
    "#16 - establish values for N_t[t] as sum of z[i,t] for each t\n",
    "for t in t_l\n",
    "    @constraint(model, N_t[t] == sum(z[i,t] for i = 1:num_samples))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with these variables, we capture the prediction made by each node in matrix $c_{k,t}$, which is hot-coded such that the prediction for leaf $t$ is $k$ when $c_{k,t} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_kt is a matrix that holds the label count of each variable within a given leaf nodes\n",
    "@variable(model, c_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#constrain c_kt to binary values {0,1}\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, c_kt[k,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#18 - force prediction for each node with values\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] == sum(c_kt[k,t] for k = 1:num_labels))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define loss array $L_t$, which is derived by applying a penalty factor for each prediction not in the majority class present in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L is the loss for a given leaf node t\n",
    "@variable(model, L[i = t_l])\n",
    "\n",
    "#20 - set loss function lower bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] >= N_t[t] - N_kt[k,t] - (M * (1 - c_kt[k,t])))\n",
    "    end\n",
    "end\n",
    "\n",
    "#21 - set loss function upper bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] <= N_t[t] - N_kt[k,t] + (M * c_kt[k,t]))\n",
    "    end\n",
    "end\n",
    "\n",
    "#22 - set all L values to be positive\n",
    "for t in t_l\n",
    "   @constraint(model, L[t] >= 0) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now set the objective function, which is to minimize loss relative to the complexity of the tree as measured by the number of active branch nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ 2.450980392156863 L_{8} + 2.450980392156863 L_{9} + 2.450980392156863 L_{10} + 2.450980392156863 L_{11} + 2.450980392156863 L_{12} + 2.450980392156863 L_{13} + 2.450980392156863 L_{14} + 2.450980392156863 L_{15} + 0.1 d_{1} + 0.1 d_{2} + 0.1 d_{3} + 0.1 d_{4} + 0.1 d_{5} + 0.1 d_{6} + 0.1 d_{7} $$"
      ],
      "text/plain": [
       "2.450980392156863 L[8] + 2.450980392156863 L[9] + 2.450980392156863 L[10] + 2.450980392156863 L[11] + 2.450980392156863 L[12] + 2.450980392156863 L[13] + 2.450980392156863 L[14] + 2.450980392156863 L[15] + 0.1 d[1] + 0.1 d[2] + 0.1 d[3] + 0.1 d[4] + 0.1 d[5] + 0.1 d[6] + 0.1 d[7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MOI.set(model, MOI.RawParameter(\"TimeLimit\"), 100.0)\n",
    "#Gurobi.setparam!(backend(model).optimizer.model.inner, \"TimeLimit\", 100.0)\n",
    "@objective(model, Min, (1/l_hat) * sum(L[t] for t in t_l)) + (alpha * sum(d[t] for t in t_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective function now established, we call the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Optimize a model with 4264 rows, 1177 columns and 51387 nonzeros\n",
      "Variable types: 47 continuous, 1130 integer (1130 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [2e-03, 1e+02]\n",
      "  Objective range  [2e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 1e+02]\n",
      "Variable types: 15 continuous, 1162 integer (1130 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 864 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    3          -    0.00000      -     -    0s\n",
      "H    0     0                     181.3725490    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0  264  181.37255    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0  263  181.37255    0.00000   100%     -    0s\n",
      "H    0     0                     174.0196078    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   40  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   40  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   36  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   31  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   31  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   34  174.01961    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   34  174.01961    0.00000   100%     -    1s\n",
      "H    0     0                     139.7058824    0.00000   100%     -    1s\n",
      "     0     2    0.00000    0   32  139.70588    0.00000   100%     -    1s\n",
      "H  358   187                     129.9019608    0.00000   100%  47.0    1s\n",
      "H  368   194                     127.4509804    0.00000   100%  47.5    1s\n",
      "H  497   266                     125.0000000    0.00000   100%  38.8    2s\n",
      "H  773   417                     115.1960784    0.00000   100%  34.9    2s\n",
      "H  941   512                     112.7450980    0.00000   100%  34.7    3s\n",
      "H  949   518                     107.8431373    0.00000   100%  34.9    3s\n",
      "  1082   621    2.45162   77    3  107.84314    0.00000   100%  37.4    5s\n",
      "H 1092   596                      98.0392157    0.00000   100%  37.1    8s\n",
      "H 1093   567                      95.5882353    0.00000   100%  37.0    8s\n",
      "H 1095   539                      93.1372549    0.00000   100%  37.0    8s\n",
      "H 1095   512                      88.2352941    0.00000   100%  37.0    8s\n",
      "  1102   520    0.00000   11  137   88.23529    0.00000   100%  50.8   10s\n",
      "H 1430   566                      68.6274510    0.00000   100%  43.5   11s\n",
      "H 1588   542                      61.2745098    0.00000   100%  40.3   12s\n",
      "H 1734   549                      58.8235294    0.00000   100%  37.6   13s\n",
      "H 1834   533                      53.9215686    0.00000   100%  36.1   13s\n",
      "H 1975   507                      51.4705882    0.00000   100%  34.2   14s\n",
      "H 2148   586                      46.5686275    0.00000   100%  32.4   16s\n",
      "H 2479   848                      29.4117647    0.00000   100%  28.9   16s\n",
      "H 4715  2085                      19.6078431    0.00000   100%  18.3   17s\n",
      "H 8039  3919                      14.7058824    0.00000   100%  12.8   18s\n",
      "H 8041  3711                       9.8039216    0.00000   100%  12.8   18s\n",
      "H 9117  3830                       7.3529412    0.00000   100%  12.4   18s\n",
      "H11402  4074                       4.9019608    0.00000   100%  11.4   19s\n",
      " 13237  5053    0.00000  118  159    4.90196    0.00000   100%  11.0   20s\n",
      " 20831  7086    0.00000   90  105    4.90196    0.00000   100%   9.1   36s\n",
      " 24280  7525    0.00000  105   70    4.90196    0.00000   100%   8.8   40s\n",
      " 40204 10016     cutoff  127         4.90196    0.00000   100%   8.7   45s\n",
      " 49276 10933     cutoff  121         4.90196    0.00000   100%   9.4   50s\n",
      "H55542  4044                       2.4509804    0.00000   100%   9.8   54s\n",
      " 56700  4001    0.00000  118  117    2.45098    0.00000   100%   9.9   55s\n",
      " 69640  4983 infeasible   94         2.45098    0.00000   100%  10.9   60s\n",
      " 76950  5430    0.00000  104   83    2.45098    0.00000   100%  11.4   66s\n",
      " 86372  5722    0.06598  106   95    2.45098    0.00000   100%  11.8   70s\n",
      " 98922  6821     cutoff  139         2.45098    0.00000   100%  12.4   75s\n",
      " 101655  7071    0.24742   81  142    2.45098    0.00000   100%  12.5   80s\n",
      " 113546  8104    0.05805  109  150    2.45098    0.00000   100%  12.9   85s\n",
      " 125941  8448     cutoff  104         2.45098    0.00000   100%  13.2   90s\n",
      " 130836  8909    0.00000  138  136    2.45098    0.00000   100%  13.1   95s\n",
      " 143546  9445    0.00000   85   97    2.45098    0.00000   100%  13.3  100s\n",
      " 153840 11730 infeasible   97         2.45098    0.00000   100%  13.4  106s\n",
      " 160953 12633    0.00000   94  105    2.45098    0.00000   100%  13.6  110s\n",
      " 173773 13144     cutoff  117         2.45098    0.00000   100%  13.7  115s\n",
      " 183672 13777     cutoff   84         2.45098    0.00000   100%  13.8  120s\n",
      " 194190 15402    0.00000   70  121    2.45098    0.00000   100%  13.9  127s\n",
      " 201022 15888    0.00000   80  103    2.45098    0.00000   100%  14.2  130s\n",
      " 209098 16914    0.00000   92   94    2.45098    0.00000   100%  14.3  136s\n",
      " 217958 18011    0.36260   84  137    2.45098    0.00000   100%  14.6  140s\n",
      " 228246 18708     cutoff  108         2.45098    0.00000   100%  14.8  146s\n",
      " 234906 19546 infeasible   91         2.45098    0.00000   100%  14.8  150s\n",
      " 242952 20328     cutoff  102         2.45098    0.00000   100%  15.2  155s\n",
      " 248484 20629     cutoff   96         2.45098    0.00000   100%  15.4  160s\n",
      " 254819 21295    0.00000   91  106    2.45098    0.00000   100%  15.6  166s\n",
      " 261897 22177    0.00000   92  106    2.45098    0.00000   100%  15.9  170s\n",
      " 272093 22734    0.00777   86  114    2.45098    0.00000   100%  16.2  175s\n",
      " 277387 23168    0.00000   88  120    2.45098    0.00000   100%  16.4  180s\n",
      " 285260 23919    0.00000   89  131    2.45098    0.00000   100%  16.6  186s\n",
      " 292764 25214    0.00000  102  110    2.45098    0.00000   100%  16.6  190s\n",
      " 305160 26969     cutoff  101         2.45098    0.00000   100%  16.7  195s\n",
      " 311950 27674    0.00000  102  147    2.45098    0.00000   100%  16.7  200s\n",
      " 321121 28286     cutoff   77         2.45098    0.00000   100%  16.8  206s\n",
      " 330483 29421 infeasible  116         2.45098    0.00000   100%  16.9  210s\n",
      " 334876 30471    0.00000  109  110    2.45098    0.00000   100%  16.9  215s\n",
      " 344400 31094 infeasible  109         2.45098    0.00000   100%  17.0  220s\n",
      " 355275 32463     cutoff   99         2.45098    0.00000   100%  17.2  225s\n",
      " 360835 33087    0.00000   85  102    2.45098    0.00000   100%  17.4  230s\n",
      " 369708 33962    0.00000  100  100    2.45098    0.00000   100%  17.5  235s\n",
      " 381224 34838     cutoff  108         2.45098    0.00000   100%  17.6  240s\n",
      " 389179 35654    0.00000   96  145    2.45098    0.00000   100%  17.6  246s\n",
      " 397623 36380 infeasible  101         2.45098    0.00000   100%  17.5  250s\n",
      " 407462 38315    0.00000   82  105    2.45098    0.00000   100%  17.6  257s\n",
      " 412399 38925    1.14577   81  146    2.45098    0.00000   100%  17.6  260s\n",
      " 423782 39694    0.00000   52  188    2.45098    0.00000   100%  17.7  265s\n",
      " 432863 40345 infeasible   50         2.45098    0.00000   100%  17.7  271s\n",
      " 440940 40954    0.00000  104   93    2.45098    0.00000   100%  17.7  275s\n",
      " 446179 41286    0.00000   86  100    2.45098    0.00000   100%  17.6  280s\n",
      " 458069 42172    0.00000   66  167    2.45098    0.00000   100%  17.6  285s\n",
      " 465938 44297    0.00000   79  114    2.45098    0.00000   100%  17.6  290s\n",
      " 473458 45016     cutoff  111         2.45098    0.00000   100%  17.6  295s\n",
      " 484605 45678     cutoff  101         2.45098    0.00000   100%  17.7  300s\n",
      " 489414 45808    0.00000  106  117    2.45098    0.00000   100%  17.7  305s\n",
      " 500725 46167     cutoff  100         2.45098    0.00000   100%  17.8  310s\n",
      " 508948 46526     cutoff  107         2.45098    0.00000   100%  17.8  315s\n",
      " 513566 46809    0.00000  105  174    2.45098    0.00000   100%  17.8  320s\n",
      " 524775 47607     cutoff  101         2.45098    0.00000   100%  17.8  325s\n",
      " 532724 48103 infeasible   95         2.45098    0.00000   100%  17.9  331s\n",
      " 540499 48527    0.00000   78  108    2.45098    0.00000   100%  17.9  335s\n",
      " 552040 49075 infeasible  111         2.45098    0.00000   100%  17.9  342s\n",
      " 558879 49601    0.00000   97   89    2.45098    0.00000   100%  17.9  345s\n",
      " 566350 50093    0.00000   97  175    2.45098    0.00000   100%  17.9  352s\n",
      " 571600 50237     cutoff  112         2.45098    0.00000   100%  17.9  355s\n",
      " 583607 51098    0.00000  104   79    2.45098    0.00000   100%  18.0  360s\n",
      " 594752 51823    0.00830  107  112    2.45098    0.00000   100%  18.0  365s\n",
      " 600619 52246    0.00000  103  121    2.45098    0.00000   100%  18.0  371s\n",
      " 608236 52137    0.00000  100   73    2.45098    0.00000   100%  18.1  375s\n",
      " 613520 52483    0.00000   97   86    2.45098    0.00000   100%  18.0  380s\n",
      " 617974 52623     cutoff  104         2.45098    0.00000   100%  18.1  385s\n",
      " 628435 52601    0.00000   94  110    2.45098    0.00000   100%  18.1  390s\n",
      " 637080 53020     cutoff  108         2.45098    0.00000   100%  18.1  395s\n",
      " 645550 53212    0.00000  100   82    2.45098    0.00000   100%  18.1  400s\n",
      " 651057 53036 infeasible  105         2.45098    0.00000   100%  18.1  405s\n",
      " 661490 53488 infeasible  107         2.45098    0.00000   100%  18.2  410s\n",
      " 672857 53588 infeasible  120         2.45098    0.00000   100%  18.2  415s\n",
      " 678097 53822    0.00000   92   71    2.45098    0.00000   100%  18.2  420s\n",
      " 689541 54620     cutoff  131         2.45098    0.00000   100%  18.2  425s\n",
      " 696168 55368     cutoff  121         2.45098    0.00000   100%  18.2  430s\n",
      " 707916 55910     cutoff   81         2.45098    0.00000   100%  18.1  435s\n",
      " 715902 56345     cutoff  109         2.45098    0.00000   100%  18.1  440s\n",
      " 725075 56878     cutoff  116         2.45098    0.00000   100%  18.0  448s\n",
      " 727170 56816    0.00000   92  127    2.45098    0.00000   100%  18.0  450s\n",
      " 739260 57351    0.01034  110   80    2.45098    0.00000   100%  18.0  455s\n",
      " 752197 57685    0.00000  130   84    2.45098    0.00000   100%  18.0  460s\n",
      " 761667 58290    0.00000  110  125    2.45098    0.00000   100%  17.9  466s\n",
      " 772927 59194    0.00000  164   76    2.45098    0.00000   100%  17.8  470s\n",
      " 786230 59464     cutoff  146         2.45098    0.00000   100%  17.6  475s\n",
      " 798167 59834     cutoff  135         2.45098    0.00000   100%  17.5  480s\n",
      " 806742 60080    0.00000  162   66    2.45098    0.00000   100%  17.4  485s\n",
      " 823153 60346     cutoff  150         2.45098    0.00000   100%  17.2  490s\n",
      " 831224 61244     cutoff  133         2.45098    0.00000   100%  17.1  496s\n",
      " 841214 61354    0.24433  120  151    2.45098    0.00000   100%  17.1  500s\n",
      " 855387 61772    0.42381  103  120    2.45098    0.00000   100%  17.0  505s\n",
      "H863001 62553                       2.4509804    0.00000   100%  16.9  509s\n",
      " 863914 62527    0.00000  176   55    2.45098    0.00000   100%  16.9  510s\n",
      " 878777 62046 infeasible  154         2.45098    0.00000   100%  16.8  515s\n",
      " 887349 62042 infeasible  141         2.45098    0.00000   100%  16.8  520s\n",
      " 899185 62252 infeasible  146         2.45098    0.00000   100%  16.7  527s\n",
      " 907848 62163 infeasible  124         2.45098    0.00000   100%  16.7  530s\n",
      " 924763 62305    0.40979   94  106    2.45098    0.00000   100%  16.5  535s\n",
      " 935138 62508    0.00000  140   46    2.45098    0.00000   100%  16.4  540s\n",
      " 953699 62563    0.00000  127   39    2.45098    0.00000   100%  16.2  545s\n",
      " 967449 62609    0.36084  132  106    2.45098    0.00000   100%  16.1  550s\n",
      " 975733 62593    0.00000  122   55    2.45098    0.00000   100%  16.0  555s\n",
      " 991170 62558    0.00000  117   60    2.45098    0.00000   100%  15.9  560s\n",
      " 999809 62850 infeasible  135         2.45098    0.00000   100%  15.9  565s\n",
      " 1011727 63988    0.00000  108   99    2.45098    0.00000   100%  15.8  572s\n",
      " 1018921 64648 infeasible  136         2.45098    0.00000   100%  15.8  575s\n",
      " 1033664 65458 infeasible  137         2.45098    0.00000   100%  15.7  580s\n",
      " 1043593 65805    0.00000  122   83    2.45098    0.00000   100%  15.7  586s\n",
      " 1054221 66656    0.08790  172   86    2.45098    0.00000   100%  15.7  590s\n",
      " 1071032 67331     cutoff  142         2.45098    0.00000   100%  15.6  595s\n",
      " 1081406 67787    0.36142  161   74    2.45098    0.00000   100%  15.5  600s\n",
      " 1095873 68485     cutoff  150         2.45098    0.00000   100%  15.4  605s\n",
      " 1106462 69140     cutoff  146         2.45098    0.00000   100%  15.3  610s\n",
      " 1125327 69590    0.00000  148   30    2.45098    0.00000   100%  15.2  615s\n",
      " 1135625 69956    0.00000  142   91    2.45098    0.00000   100%  15.1  620s\n",
      " 1153041 70149    0.00000  151   57    2.45098    0.00000   100%  14.9  625s\n",
      " 1172417 70726    0.16137  160   34    2.45098    0.00000   100%  14.8  630s\n",
      " 1182855 71011    0.00000  127   64    2.45098    0.00000   100%  14.7  635s\n",
      " 1201766 71475    0.85936  149   42    2.45098    0.00000   100%  14.6  642s\n",
      " 1210035 71547     cutoff  153         2.45098    0.00000   100%  14.5  645s\n",
      " 1227730 72000    2.43617  170   33    2.45098    0.00000   100%  14.4  650s\n",
      " 1236182 72187    0.00000  141   73    2.45098    0.00000   100%  14.3  655s\n",
      " 1254605 72931    0.00000  132   53    2.45098    0.00000   100%  14.2  660s\n",
      " 1265806 73148 infeasible  158         2.45098    0.00000   100%  14.1  665s\n",
      " 1283665 73512    0.00000  141    4    2.45098    0.00000   100%  14.0  670s\n",
      " 1293692 73984    1.43359  147   25    2.45098    0.00000   100%  14.0  675s\n",
      " 1307340 74299    0.00000  149   20    2.45098    0.00000   100%  13.9  681s\n",
      " 1319944 74381    0.00000  177   98    2.45098    0.00000   100%  13.9  685s\n",
      " 1332541 74876    0.02615  135   81    2.45098    0.00000   100%  13.8  690s\n",
      " 1342712 75425 infeasible  148         2.45098    0.00000   100%  13.7  695s\n",
      " 1357023 76696    0.29436  138   52    2.45098    0.00000   100%  13.7  700s\n",
      " 1366873 76538    0.00000  119   47    2.45098    0.00000   100%  13.7  705s\n",
      " 1374389 76755 infeasible  127         2.45098    0.00000   100%  13.6  710s\n",
      " 1390229 76993     cutoff  128         2.45098    0.00000   100%  13.6  715s\n",
      " 1399827 76903    0.00000  120  123    2.45098    0.00000   100%  13.5  720s\n",
      " 1412955 77179    0.00000  107   88    2.45098    0.00000   100%  13.5  727s\n",
      " 1421570 77741    0.00000  132   43    2.45098    0.00000   100%  13.4  730s\n",
      " 1440578 78312 infeasible  119         2.45098    0.00000   100%  13.4  735s\n",
      " 1451114 78642    0.00000  125   26    2.45098    0.00000   100%  13.3  740s\n",
      " 1467143 79377    0.00000  128   39    2.45098    0.00000   100%  13.2  745s\n",
      " 1484364 79647     cutoff  114         2.45098    0.00000   100%  13.1  750s\n",
      " 1488539 79672     cutoff   96         2.45098    0.00000   100%  13.1  755s\n",
      " 1503223 81940    0.12688  141   48    2.45098    0.00000   100%  13.1  760s\n",
      " 1516876 83103 infeasible  150         2.45098    0.00000   100%  13.1  766s\n",
      " 1525648 83833    0.00000  136   70    2.45098    0.00000   100%  13.0  770s\n",
      " 1542015 86048 infeasible  161         2.45098    0.00000   100%  13.0  775s\n",
      " 1552928 86849    0.47954  145  103    2.45098    0.00000   100%  13.0  780s\n",
      " 1564436 87557    0.00000  141   61    2.45098    0.00000   100%  12.9  785s\n",
      " 1578109 88499    0.44153  176  109    2.45098    0.00000   100%  12.9  790s\n",
      " 1593853 89640    0.00000  136   74    2.45098    0.00000   100%  12.9  795s\n",
      " 1599334 90490     cutoff  136         2.45098    0.00000   100%  12.9  800s\n",
      " 1614334 92254    0.00000  138   45    2.45098    0.00000   100%  12.9  805s\n",
      " 1624273 93282    0.00000  147   50    2.45098    0.00000   100%  12.8  810s\n",
      " 1640844 94655    0.00000  115   93    2.45098    0.00000   100%  12.8  815s\n",
      " 1647762 95332     cutoff  133         2.45098    0.00000   100%  12.8  820s\n",
      " 1661265 96691     cutoff  150         2.45098    0.00000   100%  12.8  825s\n",
      " 1671169 97060     cutoff  140         2.45098    0.00000   100%  12.8  831s\n",
      " 1682026 97506     cutoff  153         2.45098    0.00000   100%  12.8  835s\n",
      " 1689275 97943    0.00000  145   48    2.45098    0.00000   100%  12.8  840s\n",
      " 1704505 98935    0.00000  133   66    2.45098    0.00000   100%  12.8  845s\n",
      " 1713183 99589    0.00000  129   76    2.45098    0.00000   100%  12.8  850s\n",
      " 1724240 100039    0.00000  144   45    2.45098    0.00000   100%  12.8  856s\n",
      " 1734494 100271    0.00000  148   90    2.45098    0.00000   100%  12.8  860s\n",
      " 1745052 101224 infeasible  154         2.45098    0.00000   100%  12.8  866s\n",
      " 1755034 101567 infeasible  134         2.45098    0.00000   100%  12.8  870s\n",
      " 1768137 101810    0.28738  139   94    2.45098    0.00000   100%  12.8  876s\n",
      " 1778203 102116     cutoff  150         2.45098    0.00000   100%  12.8  880s\n",
      " 1786225 103070    0.16939  150   72    2.45098    0.00000   100%  12.7  885s\n",
      " 1802683 104470    0.00000  161   75    2.45098    0.00000   100%  12.7  890s\n",
      " 1808318 104624    0.00000  155  107    2.45098    0.00000   100%  12.7  895s\n",
      " 1823220 106027 infeasible  139         2.45098    0.00000   100%  12.7  900s\n",
      " 1831325 106847     cutoff  135         2.45098    0.00000   100%  12.7  905s\n",
      " 1846080 107859     cutoff  136         2.45098    0.00000   100%  12.6  910s\n",
      " 1853919 108435    0.00000  135   63    2.45098    0.00000   100%  12.6  915s\n",
      " 1865754 109765     cutoff  154         2.45098    0.00000   100%  12.6  921s\n",
      " 1874940 110450    0.74709  141  101    2.45098    0.00000   100%  12.6  925s\n",
      " 1885994 111583 infeasible  146         2.45098    0.00000   100%  12.6  931s\n",
      " 1896853 112369 infeasible  138         2.45098    0.00000   100%  12.6  935s\n",
      " 1913819 112844    0.00000  134   52    2.45098    0.00000   100%  12.5  940s\n",
      " 1923865 113331     cutoff  133         2.45098    0.00000   100%  12.5  945s\n",
      " 1934610 113428     cutoff  150         2.45098    0.00000   100%  12.5  950s\n",
      " 1946148 114040     cutoff  158         2.45098    0.00000   100%  12.5  956s\n",
      " 1956029 114271    0.00000  138   30    2.45098    0.00000   100%  12.5  960s\n",
      " 1971815 115228     cutoff  176         2.45098    0.00000   100%  12.4  965s\n",
      " 1984214 115679    0.00000  166   80    2.45098    0.00000   100%  12.4  970s\n",
      " 1990044 115889    0.00000  170  101    2.45098    0.00000   100%  12.4  975s\n",
      " 2003042 116201    0.16130  148  108    2.45098    0.00000   100%  12.4  980s\n",
      " 2014751 116997     cutoff  157         2.45098    0.00000   100%  12.4  987s\n",
      " 2021362 117247    0.00000  151   95    2.45098    0.00000   100%  12.4  990s\n",
      " 2036187 118101 infeasible  131         2.45098    0.00000   100%  12.4  995s\n",
      " 2051597 118330    0.00000  148   97    2.45098    0.00000   100%  12.4 1000s\n",
      " 2056205 118491    1.12407   96  152    2.45098    0.00000   100%  12.3 1005s\n",
      " 2071062 119546    0.20114  176   55    2.45098    0.00000   100%  12.3 1010s\n",
      " 2082755 120172    0.00000  134   92    2.45098    0.00000   100%  12.3 1015s\n",
      " 2093681 120523    0.00000  104  112    2.45098    0.00000   100%  12.3 1020s\n",
      " 2103623 121033    0.00000  155   25    2.45098    0.00000   100%  12.3 1025s\n",
      " 2111426 121616 infeasible  169         2.45098    0.00000   100%  12.3 1031s\n",
      " 2122197 122061    0.00000  153   52    2.45098    0.00000   100%  12.3 1035s\n",
      " 2137436 122737    0.00000  142   71    2.45098    0.00000   100%  12.3 1040s\n",
      " 2148343 123430    0.00000  150   60    2.45098    0.00000   100%  12.3 1045s\n",
      " 2155446 123941    0.00000  150   80    2.45098    0.00000   100%  12.3 1050s\n",
      " 2168990 124526    0.12677  150   71    2.45098    0.00000   100%  12.3 1055s\n",
      " 2176119 124754     cutoff  149         2.45098    0.00000   100%  12.3 1060s\n",
      " 2188273 125189    0.00000  143   94    2.45098    0.00000   100%  12.3 1065s\n",
      " 2196346 125315    0.00000  145   58    2.45098    0.00000   100%  12.3 1070s\n",
      " 2209231 126987    0.00000  147   27    2.45098    0.00000   100%  12.2 1075s\n",
      " 2219749 127450    0.14012  155   47    2.45098    0.00000   100%  12.2 1080s\n",
      " 2233611 128403 infeasible  153         2.45098    0.00000   100%  12.2 1086s\n",
      " 2244299 129234     cutoff  159         2.45098    0.00000   100%  12.2 1090s\n",
      " 2255249 129782     cutoff  148         2.45098    0.00000   100%  12.2 1096s\n",
      " 2265487 130470 infeasible  143         2.45098    0.00000   100%  12.2 1100s\n",
      " 2272249 130772    0.00000  152   18    2.45098    0.00000   100%  12.2 1105s\n",
      " 2283431 131344     cutoff  166         2.45098    0.00000   100%  12.2 1110s\n",
      " 2300312 132899    0.00000  162   85    2.45098    0.00000   100%  12.2 1115s\n",
      " 2315393 133963    0.00000  147   62    2.45098    0.00000   100%  12.2 1121s\n",
      " 2325002 134624    0.53592  153   72    2.45098    0.00000   100%  12.2 1128s\n",
      " 2329578 134708 infeasible  166         2.45098    0.00000   100%  12.2 1130s\n",
      " 2344535 135247     cutoff  152         2.45098    0.00000   100%  12.1 1135s\n",
      " 2359001 135934     cutoff  156         2.45098    0.00000   100%  12.1 1140s\n",
      " 2366061 136104 infeasible  160         2.45098    0.00000   100%  12.1 1145s\n",
      " 2380882 136655    0.00000  150   53    2.45098    0.00000   100%  12.1 1152s\n",
      " 2387591 136865    0.08922  154   76    2.45098    0.00000   100%  12.1 1155s\n",
      " 2402756 137657    0.02258  155   70    2.45098    0.00000   100%  12.1 1160s\n",
      " 2412211 138239    0.00000  167   56    2.45098    0.00000   100%  12.1 1165s\n",
      " 2422029 138768     cutoff  163         2.45098    0.00000   100%  12.1 1170s\n",
      " 2428137 138918     cutoff  163         2.45098    0.00000   100%  12.1 1175s\n",
      " 2438298 139261 infeasible  154         2.45098    0.00000   100%  12.1 1180s\n",
      " 2450465 140414     cutoff  158         2.45098    0.00000   100%  12.1 1185s\n",
      " 2458011 140937    0.46538  166   85    2.45098    0.00000   100%  12.1 1190s\n",
      " 2472707 141826    0.00000  129   90    2.45098    0.00000   100%  12.1 1195s\n",
      " 2483449 142605    0.00000  141   53    2.45098    0.00000   100%  12.1 1202s\n",
      " 2490019 143077    0.00000  139   27    2.45098    0.00000   100%  12.0 1205s\n",
      " 2504170 143617    0.20971  143   90    2.45098    0.00000   100%  12.0 1210s\n",
      " 2517643 144568 infeasible  138         2.45098    0.00000   100%  12.0 1215s\n",
      " 2526865 144685    0.54128  137   69    2.45098    0.00000   100%  12.0 1220s\n",
      " 2534688 145053    0.00000  139   38    2.45098    0.00000   100%  12.0 1225s\n",
      " 2546028 145287    0.00000  122   78    2.45098    0.00000   100%  12.0 1230s\n",
      " 2551839 145560    0.00000  133   37    2.45098    0.00000   100%  12.0 1235s\n",
      " 2565018 145901 infeasible  118         2.45098    0.00000   100%  12.0 1240s\n",
      " 2574465 146276    0.00000  154  124    2.45098    0.00000   100%  12.0 1245s\n",
      " 2586470 147031 infeasible  131         2.45098    0.00000   100%  12.0 1250s\n",
      " 2594370 147430     cutoff  126         2.45098    0.00000   100%  12.0 1255s\n",
      " 2608199 148233    0.00000  136   67    2.45098    0.00000   100%  12.0 1262s\n",
      " 2614985 148312 infeasible  126         2.45098    0.00000   100%  12.0 1265s\n",
      " 2628008 148430    0.00000  123   87    2.45098    0.00000   100%  12.0 1272s\n",
      " 2635528 148456 infeasible  153         2.45098    0.00000   100%  12.0 1275s\n",
      " 2645005 148688    0.00000  133   64    2.45098    0.00000   100%  12.0 1280s\n",
      " 2659448 150015    0.27957  176   58    2.45098    0.00000   100%  12.0 1287s\n",
      " 2665671 150594    0.00000  153  107    2.45098    0.00000   100%  12.0 1290s\n",
      " 2681061 152022    0.28107  151  111    2.45098    0.00000   100%  12.0 1295s\n",
      " 2691427 153064     cutoff  142         2.45098    0.00000   100%  11.9 1300s\n",
      " 2703326 153490    0.77899  167   49    2.45098    0.00000   100%  11.9 1305s\n",
      " 2710030 153802 infeasible  156         2.45098    0.00000   100%  11.9 1310s\n",
      " 2724798 154374    0.50173  165   47    2.45098    0.00000   100%  11.9 1315s\n",
      " 2739344 155094     cutoff  169         2.45098    0.00000   100%  11.9 1320s\n",
      " 2750409 155442    0.00000  156   89    2.45098    0.00000   100%  11.9 1327s\n",
      " 2759318 155770    0.00000  146   12    2.45098    0.00000   100%  11.9 1330s\n",
      " 2766766 155891    0.18226  163   90    2.45098    0.00000   100%  11.9 1335s\n",
      " 2782118 156600 infeasible  172         2.45098    0.00000   100%  11.9 1340s\n",
      " 2789351 156551    0.00000  120  113    2.45098    0.00000   100%  11.9 1345s\n",
      " 2805532 157232 infeasible  153         2.45098    0.00000   100%  11.9 1350s\n",
      " 2812380 157567    0.00000  156   60    2.45098    0.00000   100%  11.9 1355s\n",
      " 2827587 158044    0.77145  182   84    2.45098    0.00000   100%  11.8 1360s\n",
      " 2833278 158062    0.00000  179   86    2.45098    0.00000   100%  11.8 1365s\n",
      " 2846311 158574    0.00000  137   45    2.45098    0.00000   100%  11.8 1370s\n",
      " 2859601 159458     cutoff  159         2.45098    0.00000   100%  11.8 1376s\n",
      " 2870982 159432    0.00000  145   62    2.45098    0.00000   100%  11.8 1380s\n",
      " 2876389 159441    0.00508  142   26    2.45098    0.00000   100%  11.8 1385s\n",
      " 2889980 159607    0.00000  121  115    2.45098    0.00000   100%  11.8 1390s\n",
      " 2901202 160539 infeasible  132         2.45098    0.00000   100%  11.8 1396s\n",
      " 2910453 160489 infeasible  167         2.45098    0.00000   100%  11.8 1400s\n",
      " 2925827 161811    0.41620  113  163    2.45098    0.00000   100%  11.8 1405s\n",
      " 2937129 162322 infeasible  159         2.45098    0.00000   100%  11.8 1410s\n",
      " 2943173 162498    0.00000  136   53    2.45098    0.00000   100%  11.8 1415s\n",
      " 2956939 162788     cutoff  147         2.45098    0.00000   100%  11.8 1420s\n",
      " 2971237 164163     cutoff  136         2.45098    0.00000   100%  11.7 1426s\n",
      " 2981509 164678    0.00000  151   60    2.45098    0.00000   100%  11.7 1431s\n",
      " 2991508 165417    0.19031  137   91    2.45098    0.00000   100%  11.7 1437s\n",
      " 3000079 166016    0.04339  127  141    2.45098    0.00000   100%  11.7 1440s\n",
      " 3014849 166996    0.00000  116  117    2.45098    0.00000   100%  11.7 1445s\n",
      " 3018681 167312    0.01123  146   30    2.45098    0.00000   100%  11.7 1450s\n",
      " 3030999 168299    0.00000  106  113    2.45098    0.00000   100%  11.7 1457s\n",
      " 3037673 168518    0.14151  134  113    2.45098    0.00000   100%  11.7 1460s\n",
      " 3052686 169748 infeasible  128         2.45098    0.00000   100%  11.7 1465s\n",
      " 3061759 169829 infeasible  133         2.45098    0.00000   100%  11.7 1470s\n",
      " 3074877 171069     cutoff  130         2.45098    0.00000   100%  11.7 1475s\n",
      " 3086691 172095     cutoff  125         2.45098    0.00000   100%  11.7 1480s\n",
      " 3089655 172405    0.18594  169   80    2.45098    0.00000   100%  11.7 1485s\n",
      " 3104387 173450    0.70186  163  102    2.45098    0.00000   100%  11.7 1490s\n",
      " 3118698 174421     cutoff  135         2.45098    0.00000   100%  11.7 1495s\n",
      " 3128608 174805    0.00000  115  109    2.45098    0.00000   100%  11.7 1501s\n",
      " 3138406 175682    0.26939  151   48    2.45098    0.00000   100%  11.7 1505s\n",
      " 3152329 176438    0.43253  137  123    2.45098    0.00000   100%  11.7 1510s\n",
      " 3161753 177305 infeasible  150         2.45098    0.00000   100%  11.7 1516s\n",
      " 3170661 177741    0.00000  180   40    2.45098    0.00000   100%  11.7 1520s\n",
      " 3181764 178744    0.05646  184   27    2.45098    0.00000   100%  11.6 1527s\n",
      " 3188881 178827 infeasible  151         2.45098    0.00000   100%  11.6 1530s\n",
      " 3204806 179707    0.45676  165   95    2.45098    0.00000   100%  11.6 1535s\n",
      " 3213690 180470    0.00000  211   23    2.45098    0.00000   100%  11.6 1540s\n",
      " 3226651 180822    0.22649  149   71    2.45098    0.00000   100%  11.6 1547s\n",
      " 3233901 181118    0.00000  194   37    2.45098    0.00000   100%  11.6 1550s\n",
      " 3249272 182551    0.14369  130  134    2.45098    0.00000   100%  11.6 1555s\n",
      " 3262676 183697    0.00000  124   50    2.45098    0.00000   100%  11.6 1561s\n",
      " 3272278 184574    0.00000  180   51    2.45098    0.00000   100%  11.6 1565s\n",
      " 3279825 185000 infeasible  185         2.45098    0.00000   100%  11.6 1570s\n",
      " 3292698 185414    0.73481  125  114    2.45098    0.00000   100%  11.6 1575s\n",
      " 3308523 186766 infeasible  108         2.45098    0.00000   100%  11.6 1580s\n",
      " 3314418 187805 infeasible  278         2.45098    0.00000   100%  11.5 1585s\n",
      " 3329164 188463    0.00000  232   67    2.45098    0.00000   100%  11.5 1590s\n",
      " 3337472 188499    0.00000  226   82    2.45098    0.00000   100%  11.5 1595s\n",
      " 3349463 189193    0.13873  210   51    2.45098    0.00000   100%  11.5 1600s\n",
      " 3362563 189945     cutoff  223         2.45098    0.00000   100%  11.5 1607s\n",
      " 3368487 190074    0.00000  213  100    2.45098    0.00000   100%  11.5 1610s\n",
      " 3383703 191259     cutoff  239         2.45098    0.00000   100%  11.5 1615s\n",
      " 3395589 191636 infeasible  170         2.45098    0.00000   100%  11.5 1620s\n",
      " 3405865 193024    0.00000  217   69    2.45098    0.00000   100%  11.5 1627s\n",
      " 3410752 193198    0.19692  213   75    2.45098    0.00000   100%  11.5 1630s\n",
      " 3425780 194148    0.03732  205   60    2.45098    0.00000   100%  11.5 1635s\n",
      " 3440881 194686     cutoff  207         2.45098    0.00000   100%  11.5 1640s\n",
      " 3456035 195326 infeasible  223         2.45098    0.00000   100%  11.5 1645s\n",
      " 3460116 195421    0.58091  183   93    2.45098    0.00000   100%  11.5 1650s\n",
      " 3475006 197216    0.00000  154   82    2.45098    0.00000   100%  11.4 1655s\n",
      " 3491030 198660     cutoff  159         2.45098    0.00000   100%  11.4 1660s\n",
      " 3501026 199589    0.00000  112   81    2.45098    0.00000   100%  11.4 1665s\n",
      " 3512131 200359    0.00000  152   34    2.45098    0.00000   100%  11.4 1670s\n",
      " 3520248 200843    0.14151  142  107    2.45098    0.00000   100%  11.4 1675s\n",
      " 3532718 201960    0.18810  117   89    2.45098    0.00000   100%  11.4 1680s\n",
      " 3544626 203013     cutoff  143         2.45098    0.00000   100%  11.4 1685s\n",
      " 3552797 203841 infeasible  140         2.45098    0.00000   100%  11.4 1690s\n",
      " 3566934 204938 infeasible  160         2.45098    0.00000   100%  11.4 1696s\n",
      " 3574567 205298    0.00000  134   76    2.45098    0.00000   100%  11.4 1700s\n",
      " 3583848 205869    0.04392  132   87    2.45098    0.00000   100%  11.4 1706s\n",
      " 3591088 206310    0.00000  133  134    2.45098    0.00000   100%  11.4 1710s\n",
      " 3605209 207484    0.00000  157   63    2.45098    0.00000   100%  11.4 1715s\n",
      " 3617978 209011 infeasible  194         2.45098    0.00000   100%  11.4 1721s\n",
      " 3627173 209714    0.04810  187   30    2.45098    0.00000   100%  11.4 1725s\n",
      " 3639596 210033     cutoff  175         2.45098    0.00000   100%  11.4 1730s\n",
      " 3646328 210366 infeasible  176         2.45098    0.00000   100%  11.4 1735s\n",
      " 3655565 210891    0.05230  201   56    2.45098    0.00000   100%  11.4 1740s\n",
      " 3667194 210969    0.00000  195   55    2.45098    0.00000   100%  11.4 1745s\n",
      " 3674670 211416     cutoff  219         2.45098    0.00000   100%  11.4 1750s\n",
      " 3689377 211560    0.87865  124  126    2.45098    0.00000   100%  11.4 1755s\n",
      " 3700144 212730     cutoff  153         2.45098    0.00000   100%  11.4 1760s\n",
      " 3710475 213480 infeasible  186         2.45098    0.00000   100%  11.4 1766s\n",
      " 3721722 214277    0.00000  145  119    2.45098    0.00000   100%  11.4 1770s\n",
      " 3733726 215367    0.00000  146   62    2.45098    0.00000   100%  11.3 1775s\n",
      " 3749022 216020 infeasible  175         2.45098    0.00000   100%  11.3 1780s\n",
      " 3751491 216142    0.00000  150   66    2.45098    0.00000   100%  11.3 1785s\n",
      " 3765829 216564 infeasible  153         2.45098    0.00000   100%  11.3 1790s\n",
      " 3780854 217558     cutoff  151         2.45098    0.00000   100%  11.3 1795s\n",
      " 3795440 218381    0.00000  142  107    2.45098    0.00000   100%  11.3 1801s\n",
      " 3806423 219614    0.00000  203   76    2.45098    0.00000   100%  11.3 1805s\n",
      " 3816923 219935    0.00000  136  113    2.45098    0.00000   100%  11.3 1810s\n",
      " 3831123 221137     cutoff  190         2.45098    0.00000   100%  11.3 1817s\n",
      " 3838731 221397 infeasible  180         2.45098    0.00000   100%  11.3 1820s\n",
      " 3851023 221668    0.39943  128  151    2.45098    0.00000   100%  11.3 1826s\n",
      " 3860787 222186 infeasible  154         2.45098    0.00000   100%  11.3 1830s\n",
      " 3872241 223253 infeasible  128         2.45098    0.00000   100%  11.3 1836s\n",
      " 3882350 223762     cutoff  136         2.45098    0.00000   100%  11.3 1840s\n",
      " 3897165 224569    0.00000  200   77    2.45098    0.00000   100%  11.3 1845s\n",
      " 3907905 225160    0.00000  143   88    2.45098    0.00000   100%  11.3 1850s\n",
      " 3914890 225768    0.00000  177   54    2.45098    0.00000   100%  11.2 1855s\n",
      " 3928922 226016 infeasible  162         2.45098    0.00000   100%  11.2 1860s\n",
      " 3936244 226533    0.00000  187   78    2.45098    0.00000   100%  11.2 1865s\n",
      " 3952250 227154 infeasible  147         2.45098    0.00000   100%  11.2 1870s\n",
      " 3960103 227795    0.05688  147   76    2.45098    0.00000   100%  11.2 1875s\n",
      " 3974303 229225 infeasible  187         2.45098    0.00000   100%  11.2 1882s\n",
      " 3981080 229598    0.00000  185   74    2.45098    0.00000   100%  11.2 1885s\n",
      " 3996702 230193    0.60308  195   89    2.45098    0.00000   100%  11.2 1890s\n",
      " 4006588 231067     cutoff  192         2.45098    0.00000   100%  11.2 1895s\n",
      " 4020098 231738    0.00000  173   38    2.45098    0.00000   100%  11.2 1900s\n",
      " 4028692 232059    0.43253  178   80    2.45098    0.00000   100%  11.2 1905s\n",
      " 4043353 233136    0.03973  188   32    2.45098    0.00000   100%  11.2 1912s\n",
      " 4049653 233323    0.00000  187   36    2.45098    0.00000   100%  11.2 1915s\n",
      " 4064675 234325    0.00000  184   68    2.45098    0.00000   100%  11.2 1920s\n",
      " 4073226 234552 infeasible  176         2.45098    0.00000   100%  11.2 1926s\n",
      " 4082952 234928    0.00000  173   90    2.45098    0.00000   100%  11.2 1930s\n",
      " 4098618 236263    0.00000  149   89    2.45098    0.00000   100%  11.1 1935s\n",
      " 4110339 236788    0.00000  154   85    2.45098    0.00000   100%  11.1 1940s\n",
      " 4116061 237358    0.00000  162   95    2.45098    0.00000   100%  11.1 1945s\n",
      " 4131724 238414 infeasible  157         2.45098    0.00000   100%  11.1 1950s\n",
      " 4140668 239174    0.00000  174  117    2.45098    0.00000   100%  11.1 1955s\n",
      " 4149332 239687    0.13601  171   85    2.45098    0.00000   100%  11.1 1960s\n",
      " 4163445 240192     cutoff  161         2.45098    0.00000   100%  11.1 1965s\n",
      " 4170830 240697     cutoff  164         2.45098    0.00000   100%  11.1 1970s\n",
      " 4182535 241952    0.07529  152   76    2.45098    0.00000   100%  11.1 1975s\n",
      " 4193589 243150 infeasible  161         2.45098    0.00000   100%  11.1 1980s\n",
      " 4199232 243456    0.74709  169   51    2.45098    0.00000   100%  11.1 1985s\n",
      " 4211393 243811    0.00000  160   83    2.45098    0.00000   100%  11.1 1990s\n",
      " 4225923 245320    0.19089  240   70    2.45098    0.00000   100%  11.1 1995s\n",
      " 4240763 246226    0.00000  189   74    2.45098    0.00000   100%  11.1 2000s\n",
      " 4248198 246656    0.04310  195   27    2.45098    0.00000   100%  11.1 2005s\n",
      " 4259387 247198    0.00000  217   60    2.45098    0.00000   100%  11.1 2010s\n",
      " 4271099 247454    0.00000   91  131    2.45098    0.00000   100%  11.1 2015s\n",
      " 4279551 248192    0.92490  164  112    2.45098    0.00000   100%  11.1 2020s\n",
      " 4293065 249553    0.00000  145   64    2.45098    0.00000   100%  11.1 2025s\n",
      " 4307448 250669 infeasible  154         2.45098    0.00000   100%  11.1 2030s\n",
      " 4317055 251420 infeasible  151         2.45098    0.00000   100%  11.1 2035s\n",
      " 4327460 252361    0.04720  181   65    2.45098    0.00000   100%  11.1 2041s\n",
      " 4337917 253336    0.00000  160   61    2.45098    0.00000   100%  11.1 2045s\n",
      " 4352214 254819    0.00000  163   24    2.45098    0.00000   100%  11.1 2050s\n",
      " 4359964 255391 infeasible  173         2.45098    0.00000   100%  11.1 2055s\n",
      " 4370313 256314 infeasible  160         2.45098    0.00000   100%  11.1 2061s\n",
      " 4377876 256067     cutoff  155         2.45098    0.00000   100%  11.1 2065s\n",
      " 4391852 256759    0.00000  156  108    2.45098    0.00000   100%  11.1 2070s\n",
      " 4401014 257314 infeasible  129         2.45098    0.00000   100%  11.1 2075s\n",
      " 4410116 257816    0.00000  128   33    2.45098    0.00000   100%  11.1 2080s\n",
      " 4422675 258439    0.00000  166   81    2.45098    0.00000   100%  11.1 2085s\n",
      " 4434101 258800 infeasible  182         2.45098    0.00000   100%  11.1 2090s\n",
      " 4444066 259020     cutoff  149         2.45098    0.00000   100%  11.1 2095s\n",
      " 4457076 259755    0.00000  127   77    2.45098    0.00000   100%  11.1 2100s\n",
      " 4467853 260737    0.70599  161   70    2.45098    0.00000   100%  11.1 2105s\n",
      " 4476581 261457    0.00000   93  111    2.45098    0.00000   100%  11.1 2111s\n",
      " 4482284 261725    0.00000  163  130    2.45098    0.00000   100%  11.1 2115s\n",
      " 4494219 262397    0.16174  167  112    2.45098    0.00000   100%  11.1 2120s\n",
      " 4503708 263061    0.04539  155   67    2.45098    0.00000   100%  11.1 2125s\n",
      " 4518973 264468     cutoff  173         2.45098    0.00000   100%  11.1 2130s\n",
      " 4525597 265012    0.00000  162   67    2.45098    0.00000   100%  11.1 2135s\n",
      " 4539110 265568 infeasible  184         2.45098    0.00000   100%  11.1 2140s\n",
      " 4548279 265863    0.00000  192   99    2.45098    0.00000   100%  11.1 2145s\n",
      " 4563434 266624    0.86288  196   74    2.45098    0.00000   100%  11.1 2150s\n",
      " 4566932 266622    0.19132  114  117    2.45098    0.00000   100%  11.1 2155s\n",
      " 4581940 268401 infeasible  186         2.45098    0.00000   100%  11.0 2160s\n",
      " 4596546 269316     cutoff  186         2.45098    0.00000   100%  11.0 2165s\n",
      " 4606914 269813    0.00000  143   72    2.45098    0.00000   100%  11.0 2170s\n",
      " 4614051 270061    0.00000  130   84    2.45098    0.00000   100%  11.0 2175s\n",
      " 4629444 271359     cutoff  171         2.45098    0.00000   100%  11.0 2180s\n",
      " 4633782 271615    0.00000  161  111    2.45098    0.00000   100%  11.0 2185s\n",
      " 4648627 272615    0.00209  135  101    2.45098    0.00000   100%  11.0 2190s\n",
      " 4663344 273472    0.02377  153   85    2.45098    0.00000   100%  11.0 2195s\n",
      " 4669193 273422     cutoff  177         2.45098    0.00000   100%  11.0 2200s\n",
      " 4684880 275014 infeasible  194         2.45098    0.00000   100%  11.0 2205s\n",
      " 4694185 275657 infeasible  194         2.45098    0.00000   100%  11.0 2210s\n",
      " 4702798 276212     cutoff  202         2.45098    0.00000   100%  11.0 2215s\n",
      " 4717134 277061 infeasible  197         2.45098    0.00000   100%  11.0 2220s\n",
      " 4721744 277118    0.02098  178   71    2.45098    0.00000   100%  11.0 2225s\n",
      " 4735652 278141    0.00000  192   71    2.45098    0.00000   100%  11.0 2230s\n",
      " 4751171 279486    0.00000  175   84    2.45098    0.00000   100%  11.0 2235s\n",
      " 4758679 280054     cutoff  195         2.45098    0.00000   100%  11.0 2240s\n",
      " 4772569 280702     cutoff  163         2.45098    0.00000   100%  11.0 2246s\n",
      " 4781845 281098    0.00000  181  122    2.45098    0.00000   100%  11.0 2250s\n",
      " 4786517 281256 infeasible  104         2.45098    0.00000   100%  11.0 2255s\n",
      " 4800245 282325    0.00000  142  107    2.45098    0.00000   100%  11.0 2260s\n",
      " 4813993 282843     cutoff  160         2.45098    0.00000   100%  11.0 2265s\n",
      " 4824573 283832    0.00000  197   25    2.45098    0.00000   100%  11.0 2270s\n",
      " 4836403 285139    0.00000  171   86    2.45098    0.00000   100%  11.0 2275s\n",
      " 4847301 285755    0.21122  168   65    2.45098    0.00000   100%  11.0 2280s\n",
      " 4855157 286080    0.00000  169   48    2.45098    0.00000   100%  11.0 2285s\n",
      " 4869259 286687 infeasible  160         2.45098    0.00000   100%  11.0 2290s\n",
      " 4877373 287169     cutoff  198         2.45098    0.00000   100%  11.0 2296s\n",
      " 4887274 287714    0.00000  176   49    2.45098    0.00000   100%  11.0 2300s\n",
      " 4901852 288443    0.00000  164   33    2.45098    0.00000   100%  11.0 2305s\n",
      " 4916488 289290    0.21129  156  125    2.45098    0.00000   100%  11.0 2310s\n",
      " 4921148 289519    0.00000  165   72    2.45098    0.00000   100%  11.0 2315s\n",
      " 4935287 290169 infeasible  165         2.45098    0.00000   100%  11.0 2320s\n",
      " 4950822 291226    1.03900  213  100    2.45098    0.00000   100%  11.0 2325s\n",
      " 4960835 292426    0.16903  174   59    2.45098    0.00000   100%  11.0 2330s\n",
      " 4967741 292788    0.00000  155   56    2.45098    0.00000   100%  11.0 2335s\n",
      " 4981943 293629    1.01739  109  182    2.45098    0.00000   100%  11.0 2340s\n",
      " 4993227 294400     cutoff  189         2.45098    0.00000   100%  11.0 2345s\n",
      " 4999076 294644    0.00000  189   68    2.45098    0.00000   100%  11.0 2350s\n",
      " 5012925 296029 infeasible  153         2.45098    0.00000   100%  10.9 2355s\n",
      " 5026011 297146    0.00000  164   57    2.45098    0.00000   100%  11.0 2360s\n",
      " 5030644 297404    1.08071  130   85    2.45098    0.00000   100%  11.0 2365s\n",
      " 5044906 297741    0.00000  169   99    2.45098    0.00000   100%  10.9 2370s\n",
      " 5053110 297979    0.00000  168   57    2.45098    0.00000   100%  10.9 2375s\n",
      " 5067301 299079    0.46245  149   96    2.45098    0.00000   100%  10.9 2380s\n",
      " 5076984 299670     cutoff  147         2.45098    0.00000   100%  10.9 2385s\n",
      " 5086859 299954    0.00000   72  129    2.45098    0.00000   100%  10.9 2390s\n",
      " 5100060 301411    0.04568   99  111    2.45098    0.00000   100%  10.9 2397s\n",
      " 5106931 302244 infeasible  125         2.45098    0.00000   100%  10.9 2400s\n",
      " 5121529 303349    0.00000  183  101    2.45098    0.00000   100%  10.9 2405s\n",
      " 5124063 303434    0.08574  185   56    2.45098    0.00000   100%  10.9 2410s\n",
      " 5139147 304360    0.00000  130  106    2.45098    0.00000   100%  10.9 2415s\n",
      " 5154090 305593    0.00000  135   90    2.45098    0.00000   100%  10.9 2420s\n",
      " 5164650 306266 infeasible  150         2.45098    0.00000   100%  10.9 2425s\n",
      " 5174906 306722    0.00000  140  122    2.45098    0.00000   100%  10.9 2430s\n",
      " 5183706 307558     cutoff  193         2.45098    0.00000   100%  10.9 2435s\n",
      " 5193135 308270    0.00000  195   87    2.45098    0.00000   100%  10.9 2440s\n",
      " 5204245 308981    0.41772  182   98    2.45098    0.00000   100%  10.9 2445s\n",
      " 5214386 309879    0.39882  172   55    2.45098    0.00000   100%  10.9 2450s\n",
      " 5219590 310196    0.00000  191   24    2.45098    0.00000   100%  10.9 2455s\n",
      " 5232651 310537     cutoff  189         2.45098    0.00000   100%  10.9 2460s\n",
      " 5241469 310954    0.27747  175   57    2.45098    0.00000   100%  10.9 2465s\n",
      " 5252724 311253    0.00000  180   99    2.45098    0.00000   100%  10.9 2470s\n",
      " 5259154 311861    0.00000  199   83    2.45098    0.00000   100%  10.9 2475s\n",
      " 5273590 312857     cutoff  190         2.45098    0.00000   100%  10.9 2480s\n",
      " 5281792 313144     cutoff  181         2.45098    0.00000   100%  10.9 2485s\n",
      " 5296410 313447    1.62024  195   92    2.45098    0.00000   100%  10.9 2490s\n",
      " 5303204 313767 infeasible  212         2.45098    0.00000   100%  10.9 2495s\n",
      " 5318213 314261 infeasible  151         2.45098    0.00000   100%  10.9 2500s\n",
      " 5328149 314872    0.00209  191   56    2.45098    0.00000   100%  10.9 2506s\n",
      " 5336971 315352    0.13802  131  106    2.45098    0.00000   100%  10.9 2510s\n",
      " 5351024 316367     cutoff  162         2.45098    0.00000   100%  10.9 2518s\n",
      " 5354863 316616 infeasible  158         2.45098    0.00000   100%  10.9 2520s\n",
      " 5368905 317793    0.00000  124  101    2.45098    0.00000   100%  10.9 2525s\n",
      " 5375911 318940    0.00000  134   77    2.45098    0.00000   100%  10.9 2530s\n",
      " 5386494 319586 infeasible  158         2.45098    0.00000   100%  10.9 2535s\n",
      " 5399896 320414    0.06233  180   46    2.45098    0.00000   100%  10.9 2540s\n",
      " 5405548 320735    0.04539  170   33    2.45098    0.00000   100%  10.9 2545s\n",
      " 5417542 320971    0.00000  152   87    2.45098    0.00000   100%  10.9 2550s\n",
      " 5430747 321892 infeasible  163         2.45098    0.00000   100%  10.9 2557s\n",
      " 5436581 321880    0.00000  157  117    2.45098    0.00000   100%  10.9 2560s\n",
      " 5450033 322061    0.02258   97  141    2.45098    0.00000   100%  10.9 2565s\n",
      " 5459718 323271     cutoff  156         2.45098    0.00000   100%  10.9 2570s\n",
      " 5466966 323766    0.00000  131   71    2.45098    0.00000   100%  10.9 2575s\n",
      " 5479807 325205     cutoff  136         2.45098    0.00000   100%  10.9 2580s\n",
      " 5493776 325934     cutoff  165         2.45098    0.00000   100%  10.9 2586s\n",
      " 5501589 326184    0.00000  107  101    2.45098    0.00000   100%  10.9 2590s\n",
      " 5507697 326940    0.00000  154  125    2.45098    0.00000   100%  10.9 2595s\n",
      " 5521221 328020    0.00000  168   64    2.45098    0.00000   100%  10.9 2600s\n",
      " 5530689 328794    0.00000  125   70    2.45098    0.00000   100%  10.9 2605s\n",
      " 5543040 329369 infeasible  127         2.45098    0.00000   100%  10.9 2610s\n",
      " 5550326 329849    0.29355  130   68    2.45098    0.00000   100%  10.9 2615s\n",
      " 5563622 330816    0.00000  136  139    2.45098    0.00000   100%  10.9 2621s\n",
      " 5572203 331718    1.55978  145   63    2.45098    0.00000   100%  10.9 2625s\n",
      " 5577835 332028    0.00000  159   29    2.45098    0.00000   100%  10.9 2630s\n",
      " 5589881 332731    0.31787  152   86    2.45098    0.00000   100%  10.9 2635s\n",
      " 5602094 333543 infeasible  151         2.45098    0.00000   100%  10.9 2641s\n",
      " 5613612 334226     cutoff  146         2.45098    0.00000   100%  10.9 2645s\n",
      " 5621977 335024 infeasible  185         2.45098    0.00000   100%  10.9 2650s\n",
      " 5632777 335666     cutoff  169         2.45098    0.00000   100%  10.9 2657s\n",
      " 5638593 335926 infeasible  173         2.45098    0.00000   100%  10.9 2660s\n",
      " 5651777 336355    0.00000  123   92    2.45098    0.00000   100%  10.9 2665s\n",
      " 5659286 336823    0.00000  160   57    2.45098    0.00000   100%  10.9 2671s\n",
      " 5669394 337463    0.00000  144   66    2.45098    0.00000   100%  10.9 2675s\n",
      " 5683070 337892     cutoff  123         2.45098    0.00000   100%  10.9 2680s\n",
      " 5696367 338990    0.29265  110   71    2.45098    0.00000   100%  10.9 2686s\n",
      " 5704991 339603 infeasible  129         2.45098    0.00000   100%  10.9 2690s\n",
      " 5715643 340292    0.16903  179   82    2.45098    0.00000   100%  10.9 2695s\n",
      " 5728031 341276    0.00000  174   30    2.45098    0.00000   100%  10.9 2702s\n",
      " 5733483 341866     cutoff  157         2.45098    0.00000   100%  10.9 2705s\n",
      " 5747470 342891     cutoff  133         2.45098    0.00000   100%  10.9 2710s\n",
      " 5756563 343248 infeasible  153         2.45098    0.00000   100%  10.9 2715s\n",
      " 5766863 343970    0.00000  156   88    2.45098    0.00000   100%  10.9 2721s\n",
      " 5775660 344393    0.00000  126   78    2.45098    0.00000   100%  10.9 2725s\n",
      " 5789803 345544     cutoff  193         2.45098    0.00000   100%  10.9 2730s\n",
      " 5797437 346142    0.18845  164   74    2.45098    0.00000   100%  10.9 2735s\n",
      " 5811480 347319    0.00000  180   37    2.45098    0.00000   100%  10.9 2740s\n",
      " 5819965 347889    0.12315  164   66    2.45098    0.00000   100%  10.9 2745s\n",
      " 5828525 348638 infeasible  191         2.45098    0.00000   100%  10.9 2750s\n",
      " 5840831 349258 infeasible  197         2.45098    0.00000   100%  10.9 2757s\n",
      " 5847301 349299    0.00000  182  101    2.45098    0.00000   100%  10.9 2760s\n",
      " 5862071 350041    0.78129  180  127    2.45098    0.00000   100%  10.9 2765s\n",
      " 5870525 350410    0.04216  139   75    2.45098    0.00000   100%  10.9 2770s\n",
      " 5884565 351550    0.00000  177   65    2.45098    0.00000   100%  10.9 2775s\n",
      " 5892311 351965     cutoff  167         2.45098    0.00000   100%  10.9 2780s\n",
      " 5905031 352713    0.06738  195   66    2.45098    0.00000   100%  10.9 2787s\n",
      " 5911685 353099    0.00000  199   71    2.45098    0.00000   100%  10.9 2790s\n",
      " 5925527 353400 infeasible  179         2.45098    0.00000   100%  10.9 2795s\n",
      " 5930994 353863    0.00000  194   42    2.45098    0.00000   100%  10.9 2800s\n",
      " 5942185 354261 infeasible  167         2.45098    0.00000   100%  10.9 2806s\n",
      " 5947637 354484 infeasible  151         2.45098    0.00000   100%  10.9 2813s\n",
      " 5951466 354791     cutoff  180         2.45098    0.00000   100%  10.9 2815s\n",
      " 5965741 355518     cutoff  192         2.45098    0.00000   100%  10.9 2820s\n",
      " 5979786 356153    0.29863  160   54    2.45098    0.00000   100%  10.9 2825s\n",
      " 5988941 356524    0.00000  189   41    2.45098    0.00000   100%  10.9 2831s\n",
      " 6000132 357822    0.00000  138   40    2.45098    0.00000   100%  10.9 2835s\n",
      " 6012089 358163    0.00000  160   47    2.45098    0.00000   100%  10.9 2842s\n",
      " 6019973 358657    0.00000  151   86    2.45098    0.00000   100%  10.9 2845s\n",
      " 6031880 359250    0.00000  172   24    2.45098    0.00000   100%  10.9 2851s\n",
      " 6042185 359604     cutoff  138         2.45098    0.00000   100%  10.9 2855s\n",
      " 6047925 360113    0.00000  181   34    2.45098    0.00000   100%  10.9 2860s\n",
      " 6061592 360445    0.01690  186   42    2.45098    0.00000   100%  10.9 2865s\n",
      " 6070622 360927 infeasible  178         2.45098    0.00000   100%  10.9 2870s\n",
      " 6083042 361487 infeasible  164         2.45098    0.00000   100%  10.9 2875s\n",
      " 6090124 361714    0.00000  153   90    2.45098    0.00000   100%  10.9 2880s\n",
      " 6104214 362605 infeasible  156         2.45098    0.00000   100%  10.9 2887s\n",
      " 6110567 362976    0.04137  150   37    2.45098    0.00000   100%  10.9 2890s\n",
      " 6121848 363500     cutoff  148         2.45098    0.00000   100%  10.9 2897s\n",
      " 6129055 363702    0.48621  134  115    2.45098    0.00000   100%  10.9 2900s\n",
      " 6143522 364370    0.00000  154  104    2.45098    0.00000   100%  10.9 2905s\n",
      " 6153235 364628 infeasible  139         2.45098    0.00000   100%  10.9 2910s\n",
      " 6166272 365056    0.00000  115  100    2.45098    0.00000   100%  10.9 2915s\n",
      " 6172699 365273    0.64743  131  144    2.45098    0.00000   100%  10.9 2920s\n",
      " 6184962 366038    0.00000  220   36    2.45098    0.00000   100%  10.9 2927s\n",
      "H6184970 366038                       2.4509804    0.00000   100%  10.9 2927s\n",
      " 6191935 366534 infeasible  224         2.45098    0.00000   100%  10.9 2930s\n",
      " 6205656 367183 infeasible  206         2.45098    0.00000   100%  10.9 2935s\n",
      " 6212127 367424    0.00000  221   62    2.45098    0.00000   100%  10.9 2940s\n",
      " 6223293 367725 infeasible  149         2.45098    0.00000   100%  10.9 2945s\n",
      " 6238176 369097     cutoff  194         2.45098    0.00000   100%  10.9 2950s\n",
      " 6246718 369322    0.00000  197  102    2.45098    0.00000   100%  10.9 2955s\n",
      " 6258006 369688 infeasible  189         2.45098    0.00000   100%  10.9 2960s\n",
      " 6267316 369801 infeasible  163         2.45098    0.00000   100%  10.9 2967s\n",
      " 6274707 370076    0.13873  162   93    2.45098    0.00000   100%  10.9 2970s\n",
      " 6289827 371053    0.05916  189   35    2.45098    0.00000   100%  10.9 2975s\n",
      " 6298188 371414 infeasible  146         2.45098    0.00000   100%  10.9 2980s\n",
      " 6306476 372030    0.00000  161  116    2.45098    0.00000   100%  10.9 2985s\n",
      " 6319569 372418    0.00000  167   82    2.45098    0.00000   100%  10.9 2990s\n",
      " 6324526 372464     cutoff  135         2.45098    0.00000   100%  10.9 2995s\n",
      " 6339098 372970    0.00000  180   85    2.45098    0.00000   100%  10.9 3000s\n",
      " 6351540 373436    0.13617  132  113    2.45098    0.00000   100%  10.9 3007s\n",
      " 6358252 373484 infeasible  160         2.45098    0.00000   100%  10.9 3010s\n",
      " 6372353 374334    0.33612  181   81    2.45098    0.00000   100%  10.9 3015s\n",
      " 6377584 374439    0.00000  175   52    2.45098    0.00000   100%  10.9 3020s\n",
      " 6391767 375130 infeasible  229         2.45098    0.00000   100%  10.9 3025s\n",
      " 6405402 375304    0.00000  189   92    2.45098    0.00000   100%  10.9 3030s\n",
      " 6414206 375905 infeasible  210         2.45098    0.00000   100%  10.9 3037s\n",
      " 6421818 376530    0.00000  232   49    2.45098    0.00000   100%  10.9 3040s\n",
      " 6435256 376855     cutoff  181         2.45098    0.00000   100%  10.9 3045s\n",
      " 6446234 377848    0.55722  197   94    2.45098    0.00000   100%  10.9 3050s\n",
      " 6459445 378248 infeasible  134         2.45098    0.00000   100%  10.9 3055s\n",
      " 6467818 378833    0.42643  174  117    2.45098    0.00000   100%  10.9 3060s\n",
      " 6477273 379598 infeasible  185         2.45098    0.00000   100%  10.9 3065s\n",
      " 6486401 380051    0.00000  146   53    2.45098    0.00000   100%  10.9 3071s\n",
      " 6496883 380179    0.04539  111  137    2.45098    0.00000   100%  10.9 3075s\n",
      " 6512046 381427    0.23423  185   51    2.45098    0.00000   100%  10.9 3080s\n",
      " 6523128 381691    1.07023  226   50    2.45098    0.00000   100%  10.9 3085s\n",
      " 6532438 382138    0.00000  178   56    2.45098    0.00000   100%  10.9 3090s\n",
      " 6542823 382445     cutoff  182         2.45098    0.00000   100%  10.9 3095s\n",
      " 6553251 383390 infeasible  180         2.45098    0.00000   100%  10.9 3101s\n",
      " 6562052 385007     cutoff  149         2.45098    0.00000   100%  10.9 3105s\n",
      " 6575867 385752     cutoff  140         2.45098    0.00000   100%  10.9 3110s\n",
      " 6587080 386341    0.00000  161  111    2.45098    0.00000   100%  10.9 3115s\n",
      " 6598942 387294    0.00000  153   62    2.45098    0.00000   100%  10.9 3120s\n",
      " 6604487 387542     cutoff  148         2.45098    0.00000   100%  10.9 3125s\n",
      " 6615783 388086 infeasible  166         2.45098    0.00000   100%  10.9 3130s\n",
      " 6620053 388379     cutoff  141         2.45098    0.00000   100%  10.9 3135s\n",
      " 6630805 388651    0.23911  133   96    2.45098    0.00000   100%  10.9 3140s\n",
      " 6644357 389629    0.13617  139  123    2.45098    0.00000   100%  10.9 3145s\n",
      " 6651819 390065    0.00000  141   50    2.45098    0.00000   100%  10.9 3152s\n",
      " 6657694 389925    0.15689  133   98    2.45098    0.00000   100%  10.9 3155s\n",
      " 6671468 391125    0.21122  134   77    2.45098    0.00000   100%  10.9 3160s\n",
      " 6684290 392188     cutoff  158         2.45098    0.00000   100%  10.9 3165s\n",
      " 6687612 392309    0.00000  142  121    2.45098    0.00000   100%  10.9 3170s\n",
      " 6699483 393375     cutoff  140         2.45098    0.00000   100%  10.9 3175s\n",
      " 6712040 394176    0.00000  145   43    2.45098    0.00000   100%  10.9 3180s\n",
      " 6720509 394716     cutoff  152         2.45098    0.00000   100%  10.9 3185s\n",
      " 6732675 395368    0.07105  142   84    2.45098    0.00000   100%  10.9 3190s\n",
      " 6740760 395890    0.00000  150   74    2.45098    0.00000   100%  11.0 3196s\n",
      " 6748620 396283 infeasible  150         2.45098    0.00000   100%  11.0 3200s\n",
      " 6761662 397072    0.00000  149   66    2.45098    0.00000   100%  11.0 3208s\n",
      " 6764275 397075    0.00000  132   93    2.45098    0.00000   100%  11.0 3210s\n",
      " 6777990 398171    0.00000  130   51    2.45098    0.00000   100%  11.0 3215s\n",
      " 6788832 398615    0.00000  146   83    2.45098    0.00000   100%  11.0 3220s\n",
      " 6797579 399559 infeasible  149         2.45098    0.00000   100%  11.0 3226s\n",
      " 6805865 400217     cutoff  136         2.45098    0.00000   100%  11.0 3230s\n",
      " 6815718 400847     cutoff  159         2.45098    0.00000   100%  11.0 3236s\n",
      " 6823161 401132    0.00000  156   67    2.45098    0.00000   100%  11.0 3240s\n",
      " 6833495 401696    0.39204  145  128    2.45098    0.00000   100%  11.0 3246s\n",
      " 6843329 401922    0.00000  159   90    2.45098    0.00000   100%  11.0 3250s\n",
      " 6852010 401649    0.00000  100  116    2.45098    0.00000   100%  11.0 3255s\n",
      " 6859450 402376     cutoff  160         2.45098    0.00000   100%  11.0 3261s\n",
      " 6865727 402794     cutoff  151         2.45098    0.00000   100%  11.0 3266s\n",
      " 6873967 403115     cutoff  159         2.45098    0.00000   100%  11.0 3270s\n",
      " 6883469 403771    0.02300  147   84    2.45098    0.00000   100%  11.0 3275s\n",
      " 6893441 404373     cutoff  160         2.45098    0.00000   100%  11.0 3280s\n",
      " 6903804 404834 infeasible  136         2.45098    0.00000   100%  11.0 3286s\n",
      " 6912306 405351    0.00000  160   64    2.45098    0.00000   100%  11.0 3290s\n",
      " 6923473 406136    0.00000  124   98    2.45098    0.00000   100%  11.0 3296s\n",
      " 6931699 406439     cutoff  175         2.45098    0.00000   100%  11.0 3300s\n",
      " 6938601 407073    0.40122  160   94    2.45098    0.00000   100%  11.0 3305s\n",
      " 6949828 407359    0.00000  169  102    2.45098    0.00000   100%  11.0 3310s\n",
      " 6956626 407790    0.00000  148  120    2.45098    0.00000   100%  11.0 3315s\n",
      " 6968040 409201    0.00000  177   77    2.45098    0.00000   100%  11.0 3320s\n",
      " 6981402 410181    0.13764  152   58    2.45098    0.00000   100%  11.0 3325s\n",
      " 6986331 410468 infeasible  121         2.45098    0.00000   100%  11.0 3330s\n",
      " 6999680 411051    0.27913  139   82    2.45098    0.00000   100%  11.0 3335s\n",
      " 7005769 411758    0.03859  146  114    2.45098    0.00000   100%  11.0 3340s\n",
      " 7017249 412577     cutoff  141         2.45098    0.00000   100%  11.0 3345s\n",
      " 7028557 413487    0.00000  133  160    2.45098    0.00000   100%  11.0 3351s\n",
      " 7037068 414089 infeasible  148         2.45098    0.00000   100%  11.0 3355s\n",
      " 7046439 414939    0.00000  143   84    2.45098    0.00000   100%  11.0 3360s\n",
      " 7051447 415237    0.24442  133   98    2.45098    0.00000   100%  11.0 3365s\n",
      " 7064438 415785     cutoff  155         2.45098    0.00000   100%  11.1 3370s\n",
      " 7074451 416462    0.00000  147  107    2.45098    0.00000   100%  11.1 3375s\n",
      " 7079926 416760    0.68514  162   78    2.45098    0.00000   100%  11.1 3380s\n",
      " 7092326 417323     cutoff  143         2.45098    0.00000   100%  11.1 3385s\n",
      " 7106040 418429    0.20263  138   79    2.45098    0.00000   100%  11.1 3390s\n",
      " 7111922 418330    0.02258  151   76    2.45098    0.00000   100%  11.1 3395s\n",
      " 7124494 418620    0.00000  144   79    2.45098    0.00000   100%  11.1 3400s\n",
      " 7133302 418693 infeasible  106         2.45098    0.00000   100%  11.1 3405s\n",
      " 7139413 419317    0.00000  120   85    2.45098    0.00000   100%  11.1 3410s\n",
      " 7152694 419867    0.33908  180  125    2.45098    0.00000   100%  11.1 3415s\n",
      " 7161514 420393    0.00000  178   96    2.45098    0.00000   100%  11.1 3421s\n",
      " 7170536 420763    0.00000  148   57    2.45098    0.00000   100%  11.1 3425s\n",
      " 7183613 422002    0.00000  132   66    2.45098    0.00000   100%  11.1 3430s\n",
      " 7188829 422242    0.05248  126   76    2.45098    0.00000   100%  11.1 3435s\n",
      " 7201239 422991     cutoff  151         2.45098    0.00000   100%  11.1 3444s\n",
      " 7202311 422993     cutoff  156         2.45098    0.00000   100%  11.1 3445s\n",
      " 7215498 423933    1.00905  169  129    2.45098    0.00000   100%  11.1 3450s\n",
      " 7229844 424842    0.77503  207   88    2.45098    0.00000   100%  11.1 3455s\n",
      " 7237138 425079    0.02030  188   63    2.45098    0.00000   100%  11.1 3460s\n",
      " 7248937 425592    0.00000  201   33    2.45098    0.00000   100%  11.1 3467s\n",
      " 7255194 425942    0.06692  218   94    2.45098    0.00000   100%  11.1 3470s\n",
      " 7270118 427355    0.60761  149  103    2.45098    0.00000   100%  11.1 3475s\n",
      " 7283988 428725    0.00000  149   64    2.45098    0.00000   100%  11.1 3480s\n",
      " 7294417 429708    0.00000  167   21    2.45098    0.00000   100%  11.1 3486s\n",
      " 7303498 430144 infeasible  178         2.45098    0.00000   100%  11.1 3490s\n",
      " 7318130 431468    0.00000  200   47    2.45098    0.00000   100%  11.1 3495s\n",
      " 7330664 432389 infeasible  219         2.45098    0.00000   100%  11.1 3500s\n",
      " 7342106 432617 infeasible  185         2.45098    0.00000   100%  11.1 3508s\n",
      " 7345689 432589 infeasible  183         2.45098    0.00000   100%  11.1 3510s\n",
      " 7359286 433140    0.00000  169   88    2.45098    0.00000   100%  11.1 3515s\n",
      " 7369386 434337    0.00000  174   72    2.45098    0.00000   100%  11.1 3523s\n",
      " 7372511 434339    0.58914  215   65    2.45098    0.00000   100%  11.1 3525s\n",
      " 7386158 435173     cutoff  169         2.45098    0.00000   100%  11.1 3530s\n",
      " 7400660 436505     cutoff  181         2.45098    0.00000   100%  11.1 3535s\n",
      " 7413962 437170    0.17066  172   62    2.45098    0.00000   100%  11.1 3540s\n",
      " 7422405 437632    0.00000  155   87    2.45098    0.00000   100%  11.1 3545s\n",
      " 7431676 438262    0.18498  132   87    2.45098    0.00000   100%  11.1 3550s\n",
      " 7444018 438509    0.00000  158   77    2.45098    0.00000   100%  11.1 3556s\n",
      " 7454582 439130 infeasible  181         2.45098    0.00000   100%  11.1 3560s\n",
      " 7458582 439359    0.00000  155  123    2.45098    0.00000   100%  11.1 3565s\n",
      " 7472646 439727    0.00000   92  120    2.45098    0.00000   100%  11.1 3570s\n",
      " 7485688 439837     cutoff  170         2.45098    0.00000   100%  11.1 3575s\n",
      " 7493473 439953     cutoff  158         2.45098    0.00000   100%  11.1 3580s\n",
      " 7500671 440201     cutoff  139         2.45098    0.00000   100%  11.1 3585s\n",
      " 7511825 440972    0.04392  171  118    2.45098    0.00000   100%  11.1 3590s\n",
      " 7521020 441674    0.31404  202   64    2.45098    0.00000   100%  11.1 3596s\n",
      " 7530872 441996     cutoff  178         2.45098    0.00000   100%  11.1 3600s\n",
      " 7540021 442461     cutoff  164         2.45098    0.00000   100%  11.1 3605s\n",
      " 7551517 442701 infeasible  209         2.45098    0.00000   100%  11.1 3610s\n",
      " 7559634 443068    1.65070  149   68    2.45098    0.00000   100%  11.1 3615s\n",
      " 7571166 443565     cutoff  169         2.45098    0.00000   100%  11.1 3620s\n",
      " 7578386 443908    0.00000  164   45    2.45098    0.00000   100%  11.1 3626s\n",
      " 7587634 444071    0.00000  164   86    2.45098    0.00000   100%  11.1 3630s\n",
      " 7595460 444471    0.00000  168   75    2.45098    0.00000   100%  11.1 3635s\n",
      " 7600491 444575    0.00000  155   66    2.45098    0.00000   100%  11.1 3640s\n",
      " 7609783 444881    0.00000  200   75    2.45098    0.00000   100%  11.1 3646s\n",
      " 7617110 445221    0.38338  163   99    2.45098    0.00000   100%  11.1 3650s\n",
      " 7629124 445482    0.00000  161  106    2.45098    0.00000   100%  11.1 3655s\n",
      " 7634792 445593     cutoff  155         2.45098    0.00000   100%  11.1 3660s\n",
      " 7645878 446061    1.03658  162   64    2.45098    0.00000   100%  11.1 3665s\n",
      " 7656693 446657     cutoff  195         2.45098    0.00000   100%  11.1 3670s\n",
      " 7667167 447109    0.00000  168   76    2.45098    0.00000   100%  11.2 3675s\n",
      " 7675144 447523    0.00000  156  102    2.45098    0.00000   100%  11.2 3680s\n",
      " 7681132 447703     cutoff  168         2.45098    0.00000   100%  11.2 3685s\n",
      " 7692906 447867     cutoff  187         2.45098    0.00000   100%  11.2 3690s\n",
      " 7704775 448701     cutoff  197         2.45098    0.00000   100%  11.2 3695s\n",
      " 7711477 448837    0.14155  169   85    2.45098    0.00000   100%  11.2 3700s\n",
      " 7723835 449195 infeasible  179         2.45098    0.00000   100%  11.2 3705s\n",
      " 7729884 449418    0.24442  178   58    2.45098    0.00000   100%  11.2 3710s\n",
      " 7742757 450098     cutoff  159         2.45098    0.00000   100%  11.2 3715s\n",
      " 7750174 450581    0.00000  168  119    2.45098    0.00000   100%  11.2 3720s\n",
      " 7761886 451254    0.00000  171   64    2.45098    0.00000   100%  11.2 3725s\n",
      " 7766566 451433    0.00000  173  118    2.45098    0.00000   100%  11.2 3730s\n",
      " 7779660 451669     cutoff  181         2.45098    0.00000   100%  11.2 3735s\n",
      " 7784853 451777     cutoff  195         2.45098    0.00000   100%  11.2 3740s\n",
      " 7796688 452209    0.51910  161   63    2.45098    0.00000   100%  11.2 3745s\n",
      " 7805020 452079     cutoff  195         2.45098    0.00000   100%  11.2 3750s\n",
      " 7811497 452238    0.61409  166  105    2.45098    0.00000   100%  11.2 3755s\n",
      " 7823108 452739    0.00000  162   80    2.45098    0.00000   100%  11.2 3761s\n",
      " 7832238 453051    0.00000  167  140    2.45098    0.00000   100%  11.2 3765s\n",
      " 7840662 453297    0.00000  140   93    2.45098    0.00000   100%  11.2 3770s\n",
      " 7852614 452838    0.00000  186  112    2.45098    0.00000   100%  11.2 3776s\n",
      " 7860302 452906     cutoff  166         2.45098    0.00000   100%  11.2 3780s\n",
      " 7871769 453009    0.00000  151   94    2.45098    0.00000   100%  11.2 3787s\n",
      " 7876957 452941    0.00000   78  130    2.45098    0.00000   100%  11.2 3790s\n",
      " 7890631 454254 infeasible  185         2.45098    0.00000   100%  11.2 3795s\n",
      " 7896311 454655    0.00000  177   62    2.45098    0.00000   100%  11.2 3800s\n",
      " 7910328 455783 infeasible  194         2.45098    0.00000   100%  11.2 3805s\n",
      " 7925110 457528    0.00000  187   66    2.45098    0.00000   100%  11.2 3810s\n",
      " 7935184 458790     cutoff  167         2.45098    0.00000   100%  11.2 3815s\n",
      " 7941667 458979    0.00000  192   38    2.45098    0.00000   100%  11.2 3820s\n",
      " 7954186 459388    0.00000  174   79    2.45098    0.00000   100%  11.2 3825s\n",
      " 7964391 460056    1.02383  135   86    2.45098    0.00000   100%  11.2 3830s\n",
      " 7973370 460486     cutoff  172         2.45098    0.00000   100%  11.2 3836s\n",
      " 7983736 461236    0.15147  215   94    2.45098    0.00000   100%  11.2 3840s\n",
      " 7998117 462602    1.28277  185   72    2.45098    0.00000   100%  11.2 3845s\n",
      " 8007050 463653    0.00000  155   37    2.45098    0.00000   100%  11.2 3850s\n",
      " 8020688 464668 infeasible  157         2.45098    0.00000   100%  11.2 3856s\n",
      " 8029004 465104     cutoff  142         2.45098    0.00000   100%  11.2 3860s\n",
      " 8038088 466104    0.00000  158   95    2.45098    0.00000   100%  11.2 3865s\n",
      " 8050101 466294    0.72684  158   99    2.45098    0.00000   100%  11.2 3870s\n",
      " 8059651 467001    0.43558  220   83    2.45098    0.00000   100%  11.2 3875s\n",
      " 8074561 467973    0.00000  157   51    2.45098    0.00000   100%  11.2 3880s\n",
      " 8082672 468610    0.00000  183   55    2.45098    0.00000   100%  11.2 3885s\n",
      " 8093205 469439    0.00000  190   64    2.45098    0.00000   100%  11.2 3890s\n",
      " 8105241 470260    0.00000  175   84    2.45098    0.00000   100%  11.2 3895s\n",
      " 8113658 470651 infeasible  158         2.45098    0.00000   100%  11.2 3900s\n",
      " 8127363 471455 infeasible  176         2.45098    0.00000   100%  11.2 3907s\n",
      " 8135871 471790    0.32767  183   76    2.45098    0.00000   100%  11.2 3910s\n",
      " 8143594 472125     cutoff  181         2.45098    0.00000   100%  11.2 3915s\n",
      " 8157336 472917     cutoff  151         2.45098    0.00000   100%  11.2 3920s\n",
      " 8163631 473651 infeasible  190         2.45098    0.00000   100%  11.2 3925s\n",
      " 8173544 474631    0.00000  177   62    2.45098    0.00000   100%  11.2 3930s\n",
      " 8186180 475393     cutoff  167         2.45098    0.00000   100%  11.2 3935s\n",
      " 8192258 475591    0.00000  144   75    2.45098    0.00000   100%  11.2 3940s\n",
      " 8207237 476076    0.00000  158  114    2.45098    0.00000   100%  11.2 3945s\n",
      " 8215971 476868    0.00000  164   48    2.45098    0.00000   100%  11.2 3950s\n",
      " 8227643 477227     cutoff  209         2.45098    0.00000   100%  11.2 3956s\n",
      " 8237509 477461    0.00000  186   59    2.45098    0.00000   100%  11.2 3960s\n",
      " 8242208 477600 infeasible  219         2.45098    0.00000   100%  11.2 3965s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 7\n",
      "  MIR: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Outputs\n",
    "\n",
    "While the verbose output of Gurobi confirms that the optimizer ran successfully, I confirm as much below via a call to JuMP.  As expected, we see that the optimizer ended once it reached its time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIME_LIMIT::TerminationStatusCode = 12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_status(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now pull the various optimized outputs of the function.  $L_t$ gives the loss values at each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 8-element Array{Float64,1}:\n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.9999999995775993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{j,t}$ gives the feature $j$ used to apply a split at node $t$, and $b_t$ gives the split-point value of variable $j$ at node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13Ã—7 Array{Float64,2}:\n",
       "  0.0  -0.0  -0.0   1.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0  -0.0   0.0  -0.0\n",
       " -0.0  -0.0   0.0  -0.0  -0.0   0.0   0.0\n",
       "  0.0   1.0  -0.0   0.0  -0.0   0.0   0.0\n",
       " -0.0  -0.0  -0.0  -0.0  -0.0   0.0  -0.0\n",
       "  0.0  -0.0   0.0   0.0  -0.0  -0.0  -0.0\n",
       "  1.0  -0.0  -0.0   0.0  -0.0  -0.0   0.0\n",
       "  0.0   0.0   0.0  -0.0   0.0  -0.0   0.0\n",
       " -0.0  -0.0   0.0   0.0   0.0   0.0  -0.0\n",
       "  0.0  -0.0   0.0   0.0   1.0  -0.0   1.0\n",
       "  0.0  -0.0  -0.0   0.0   0.0   0.0  -0.0\n",
       " -0.0  -0.0  -0.0  -0.0   0.0  -0.0   0.0\n",
       "  0.0  -0.0   1.0  -0.0  -0.0   1.0   0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [1, 2, 3, 4, 5, 6, 7]\n",
       "And data, a 7-element Array{Float64,1}:\n",
       " 0.2616033755274261 \n",
       " 0.3556701030927836 \n",
       " 0.2867332382310994 \n",
       " 0.46052631578947356\n",
       " 0.23208191126279853\n",
       " 0.09129814550641951\n",
       " 0.1774744027303755 "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_t$ gives the total number of $x_i$ input points assigned to leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 8-element Array{Float64,1}:\n",
       "  6.0\n",
       "  0.0\n",
       "  5.0\n",
       " 30.0\n",
       "  9.0\n",
       " 26.0\n",
       "  4.0\n",
       " 45.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we see the values for $N_{k,t}$, which stores the total number of inputs with label $k$ in leaf node $t$, and $c_{k,t}$, which gives the prediction $k$ in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 3Ã—8 Array{Float64,2}:\n",
       " -0.0  0.0  -0.0  -0.0  0.0  -0.0  -0.0  44.0\n",
       "  6.0  0.0   5.0  -0.0  9.0  26.0   4.0   1.0\n",
       " -0.0  0.0   0.0  30.0  0.0   0.0  -0.0   0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 3Ã—8 Array{Float64,2}:\n",
       "  0.0  0.0  -0.0  -0.0  -0.0  -0.0  -0.0   1.0\n",
       "  1.0  0.0   1.0   0.0   1.0   1.0   1.0  -0.0\n",
       " -0.0  0.0  -0.0   1.0  -0.0  -0.0   0.0  -0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(c_kt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "With the model now optimized, I will use the variable outputs from Gurobi via the JuMP API to determine how well the model performs.  To do so, I first establish a set of helper functions.  Firstly, I will extract the label predictions for each leaf node $t_l$, then build a function that will test whether a given $x_i$ input fits into a given leaf node, and lastly, build an aggregate function that iterates over each leaf node and attempts to fit every point into a leaf node until a match is found.  This final function will output a total accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelLeafNodePredictions (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function labelLeafNodePredictions(N_kt)\n",
    "    #extract dimensions of N_kt array\n",
    "    rows = axes(N_kt)[1]\n",
    "    cols = axes(N_kt)[2]\n",
    "    \n",
    "    #initialize empty array to store label of prediction at each leaf node\n",
    "    #note that here we re-index leaf nodes such that the first leaf node is index 1\n",
    "    prediction = zeros(Int8, length(cols))\n",
    "    #prediction_index = cols .- (length(cols) - 1)\n",
    "    #iterate over each row in N_kt, saving max value\n",
    "    for c in cols\n",
    "        current_max = 0\n",
    "        current_prediction = 0\n",
    "        for r in rows\n",
    "            if value(N_kt[r,c]) > current_max\n",
    "                current_max = value(N_kt[r,c])\n",
    "                current_prediction = r\n",
    "            end\n",
    "        end\n",
    "        #push best prediction to output array\n",
    "        prediction[c- (length(cols) - 1)] = current_prediction\n",
    "    end\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "function checkTreePoint(left_turns, right_turns, feature_row)\n",
    "    #check if left turns are correct for leaf node\n",
    "    for node in left_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row < value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    #check if right turns are correct for leaf node\n",
    "    for node in right_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row >= value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end            \n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function predictTestPoints(test_features, test_labels)\n",
    "    #ext variable\n",
    "    correct_predictions = length(test_labels)\n",
    "    \n",
    "    #copy test labels\n",
    "    lbls = copy(test_labels)\n",
    "    \n",
    "    #extract prediction from each leaf node\n",
    "    leaf_predictions = labelLeafNodePredictions(N_kt)\n",
    "    \n",
    "    #iterate over leaf nodes\n",
    "    for (leaf_index, leaf_value) in enumerate(t_l)\n",
    "        left_turns = A_left[leaf_value]\n",
    "        right_turns = A_right[leaf_value]\n",
    "        #skip empty leaves\n",
    "        if leaf_predictions[leaf_index] == 0\n",
    "            #println(\"no predictions at node \", leaf_value)\n",
    "            continue\n",
    "        end\n",
    "        for (label_index, label_value) in enumerate(lbls)\n",
    "            if label_value != leaf_predictions[leaf_index]\n",
    "                #println(\"skipping row - not matched to prediction of node\")\n",
    "                continue\n",
    "            end\n",
    "            if label_value == 0\n",
    "                println(\"skipping row - already correctly placed\")\n",
    "                continue\n",
    "            end       \n",
    "            #println(\"prediction at node \", leaf_value, \" is \", leaf_predictions[leaf_index])\n",
    "            #println(\"left ancestors are \", left_turns, \" ,right ancestors are \", right_turns)\n",
    "            if checkTreePoint(left_turns, right_turns, test_features[label_index,:])\n",
    "                lbls[label_index] = 0\n",
    "                #println(\"found one!\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    errors = correct_predictions - countmap(lbls)[0]\n",
    "    return 1 - (errors/correct_predictions)\n",
    "end\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these scoring functions in place, I can score the model's performance against the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433962264150944"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
