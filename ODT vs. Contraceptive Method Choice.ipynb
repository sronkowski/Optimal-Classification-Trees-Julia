{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-04 08:45:52.9"
     ]
    }
   ],
   "source": [
    "using Dates\n",
    "print(Dates.today(), \" \", Dates.Time(Dates.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Decision Trees - Contraceptive Method Choice Dataset\n",
    "\n",
    "## Stephen Ronkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load needed modules\n",
    "using JuMP\n",
    "using CSV\n",
    "using DecisionTree\n",
    "using StatsBase\n",
    "using DataFrames\n",
    "using MLDataUtils\n",
    "\n",
    "#note - Gurobi is not FOSS - licensing required!\n",
    "#this model can be solved using any MIO solver compatible with JuMP\n",
    "#see http://www.juliaopt.org/JuMP.jl/v0.20.0/installation/#Getting-Solvers-1\n",
    "using Gurobi\n",
    "\n",
    "#data path declaration\n",
    "FILEDIR = \"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion & Processing\n",
    "\n",
    "This dataset is available [here](https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice).  This dataset requires little additionally processing in order to prepare it for use in the ODT Optimizer, although we do have to generate dummy headers for usability enhancement and force the typing of each column so that the feature columns can be converted into floats during the unit range transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>col1</th><th>col2</th><th>col3</th><th>col4</th><th>col5</th><th>col6</th><th>col7</th><th>col8</th><th>col9</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>1,473 rows × 10 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>24.0</td><td>2.0</td><td>3.0</td><td>3.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>0.0</td></tr><tr><th>2</th><td>45.0</td><td>1.0</td><td>3.0</td><td>10.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>4.0</td><td>0.0</td></tr><tr><th>3</th><td>43.0</td><td>2.0</td><td>3.0</td><td>7.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>4.0</td><td>0.0</td></tr><tr><th>4</th><td>42.0</td><td>3.0</td><td>2.0</td><td>9.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>3.0</td><td>0.0</td></tr><tr><th>5</th><td>36.0</td><td>3.0</td><td>3.0</td><td>8.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>2.0</td><td>0.0</td></tr><tr><th>6</th><td>19.0</td><td>4.0</td><td>4.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>3.0</td><td>0.0</td></tr><tr><th>7</th><td>38.0</td><td>2.0</td><td>3.0</td><td>6.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>2.0</td><td>0.0</td></tr><tr><th>8</th><td>21.0</td><td>3.0</td><td>3.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>2.0</td><td>0.0</td></tr><tr><th>9</th><td>27.0</td><td>2.0</td><td>3.0</td><td>3.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>4.0</td><td>0.0</td></tr><tr><th>10</th><td>45.0</td><td>1.0</td><td>1.0</td><td>8.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>2.0</td><td>1.0</td></tr><tr><th>11</th><td>38.0</td><td>1.0</td><td>3.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>3.0</td><td>1.0</td></tr><tr><th>12</th><td>42.0</td><td>1.0</td><td>4.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>0.0</td></tr><tr><th>13</th><td>44.0</td><td>4.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>14</th><td>42.0</td><td>2.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>3.0</td><td>0.0</td></tr><tr><th>15</th><td>38.0</td><td>3.0</td><td>4.0</td><td>2.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>0.0</td></tr><tr><th>16</th><td>26.0</td><td>2.0</td><td>4.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>4.0</td><td>1.0</td><td>0.0</td></tr><tr><th>17</th><td>48.0</td><td>1.0</td><td>1.0</td><td>7.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>4.0</td><td>0.0</td></tr><tr><th>18</th><td>39.0</td><td>2.0</td><td>2.0</td><td>6.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>4.0</td><td>0.0</td></tr><tr><th>19</th><td>37.0</td><td>2.0</td><td>2.0</td><td>8.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>0.0</td></tr><tr><th>20</th><td>39.0</td><td>2.0</td><td>1.0</td><td>5.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>1.0</td><td>1.0</td></tr><tr><th>21</th><td>26.0</td><td>3.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>4.0</td><td>1.0</td><td>0.0</td></tr><tr><th>22</th><td>24.0</td><td>3.0</td><td>4.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>1.0</td><td>0.0</td></tr><tr><th>23</th><td>46.0</td><td>4.0</td><td>4.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>24</th><td>39.0</td><td>4.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>25</th><td>48.0</td><td>4.0</td><td>4.0</td><td>5.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>26</th><td>40.0</td><td>2.0</td><td>4.0</td><td>8.0</td><td>1.0</td><td>1.0</td><td>3.0</td><td>3.0</td><td>0.0</td></tr><tr><th>27</th><td>38.0</td><td>4.0</td><td>4.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>28</th><td>29.0</td><td>4.0</td><td>4.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>4.0</td><td>0.0</td></tr><tr><th>29</th><td>24.0</td><td>4.0</td><td>4.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>2.0</td><td>0.0</td></tr><tr><th>30</th><td>43.0</td><td>1.0</td><td>2.0</td><td>8.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>4.0</td><td>0.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& col1 & col2 & col3 & col4 & col5 & col6 & col7 & col8 & col9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 24.0 & 2.0 & 3.0 & 3.0 & 1.0 & 1.0 & 2.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t2 & 45.0 & 1.0 & 3.0 & 10.0 & 1.0 & 1.0 & 3.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t3 & 43.0 & 2.0 & 3.0 & 7.0 & 1.0 & 1.0 & 3.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t4 & 42.0 & 3.0 & 2.0 & 9.0 & 1.0 & 1.0 & 3.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t5 & 36.0 & 3.0 & 3.0 & 8.0 & 1.0 & 1.0 & 3.0 & 2.0 & 0.0 & $\\dots$ \\\\\n",
       "\t6 & 19.0 & 4.0 & 4.0 & 0.0 & 1.0 & 1.0 & 3.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t7 & 38.0 & 2.0 & 3.0 & 6.0 & 1.0 & 1.0 & 3.0 & 2.0 & 0.0 & $\\dots$ \\\\\n",
       "\t8 & 21.0 & 3.0 & 3.0 & 1.0 & 1.0 & 0.0 & 3.0 & 2.0 & 0.0 & $\\dots$ \\\\\n",
       "\t9 & 27.0 & 2.0 & 3.0 & 3.0 & 1.0 & 1.0 & 3.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t10 & 45.0 & 1.0 & 1.0 & 8.0 & 1.0 & 1.0 & 2.0 & 2.0 & 1.0 & $\\dots$ \\\\\n",
       "\t11 & 38.0 & 1.0 & 3.0 & 2.0 & 1.0 & 0.0 & 3.0 & 3.0 & 1.0 & $\\dots$ \\\\\n",
       "\t12 & 42.0 & 1.0 & 4.0 & 4.0 & 1.0 & 1.0 & 1.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t13 & 44.0 & 4.0 & 4.0 & 1.0 & 1.0 & 0.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t14 & 42.0 & 2.0 & 4.0 & 1.0 & 1.0 & 0.0 & 3.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t15 & 38.0 & 3.0 & 4.0 & 2.0 & 1.0 & 1.0 & 2.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t16 & 26.0 & 2.0 & 4.0 & 0.0 & 1.0 & 1.0 & 4.0 & 1.0 & 0.0 & $\\dots$ \\\\\n",
       "\t17 & 48.0 & 1.0 & 1.0 & 7.0 & 1.0 & 1.0 & 2.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t18 & 39.0 & 2.0 & 2.0 & 6.0 & 1.0 & 1.0 & 2.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t19 & 37.0 & 2.0 & 2.0 & 8.0 & 1.0 & 1.0 & 2.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t20 & 39.0 & 2.0 & 1.0 & 5.0 & 1.0 & 1.0 & 2.0 & 1.0 & 1.0 & $\\dots$ \\\\\n",
       "\t21 & 26.0 & 3.0 & 4.0 & 1.0 & 1.0 & 0.0 & 4.0 & 1.0 & 0.0 & $\\dots$ \\\\\n",
       "\t22 & 24.0 & 3.0 & 4.0 & 0.0 & 1.0 & 0.0 & 3.0 & 1.0 & 0.0 & $\\dots$ \\\\\n",
       "\t23 & 46.0 & 4.0 & 4.0 & 1.0 & 0.0 & 1.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t24 & 39.0 & 4.0 & 4.0 & 1.0 & 1.0 & 1.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t25 & 48.0 & 4.0 & 4.0 & 5.0 & 1.0 & 1.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t26 & 40.0 & 2.0 & 4.0 & 8.0 & 1.0 & 1.0 & 3.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t27 & 38.0 & 4.0 & 4.0 & 1.0 & 1.0 & 0.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t28 & 29.0 & 4.0 & 4.0 & 0.0 & 1.0 & 0.0 & 1.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t29 & 24.0 & 4.0 & 4.0 & 0.0 & 1.0 & 0.0 & 2.0 & 2.0 & 0.0 & $\\dots$ \\\\\n",
       "\t30 & 43.0 & 1.0 & 2.0 & 8.0 & 1.0 & 1.0 & 2.0 & 4.0 & 0.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "1473×10 DataFrame. Omitted printing of 3 columns\n",
       "│ Row  │ col1    │ col2    │ col3    │ col4    │ col5    │ col6    │ col7    │\n",
       "│      │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├──────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1    │ 24.0    │ 2.0     │ 3.0     │ 3.0     │ 1.0     │ 1.0     │ 2.0     │\n",
       "│ 2    │ 45.0    │ 1.0     │ 3.0     │ 10.0    │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 3    │ 43.0    │ 2.0     │ 3.0     │ 7.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 4    │ 42.0    │ 3.0     │ 2.0     │ 9.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 5    │ 36.0    │ 3.0     │ 3.0     │ 8.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 6    │ 19.0    │ 4.0     │ 4.0     │ 0.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 7    │ 38.0    │ 2.0     │ 3.0     │ 6.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 8    │ 21.0    │ 3.0     │ 3.0     │ 1.0     │ 1.0     │ 0.0     │ 3.0     │\n",
       "│ 9    │ 27.0    │ 2.0     │ 3.0     │ 3.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 10   │ 45.0    │ 1.0     │ 1.0     │ 8.0     │ 1.0     │ 1.0     │ 2.0     │\n",
       "⋮\n",
       "│ 1463 │ 21.0    │ 4.0     │ 4.0     │ 1.0     │ 0.0     │ 1.0     │ 2.0     │\n",
       "│ 1464 │ 30.0    │ 1.0     │ 3.0     │ 2.0     │ 1.0     │ 1.0     │ 3.0     │\n",
       "│ 1465 │ 23.0    │ 2.0     │ 2.0     │ 1.0     │ 1.0     │ 1.0     │ 2.0     │\n",
       "│ 1466 │ 25.0    │ 2.0     │ 4.0     │ 3.0     │ 1.0     │ 1.0     │ 1.0     │\n",
       "│ 1467 │ 42.0    │ 2.0     │ 4.0     │ 6.0     │ 1.0     │ 1.0     │ 2.0     │\n",
       "│ 1468 │ 29.0    │ 4.0     │ 4.0     │ 3.0     │ 1.0     │ 1.0     │ 1.0     │\n",
       "│ 1469 │ 33.0    │ 4.0     │ 4.0     │ 2.0     │ 1.0     │ 0.0     │ 2.0     │\n",
       "│ 1470 │ 33.0    │ 4.0     │ 4.0     │ 3.0     │ 1.0     │ 1.0     │ 1.0     │\n",
       "│ 1471 │ 39.0    │ 3.0     │ 3.0     │ 8.0     │ 1.0     │ 0.0     │ 1.0     │\n",
       "│ 1472 │ 33.0    │ 3.0     │ 3.0     │ 4.0     │ 1.0     │ 0.0     │ 2.0     │\n",
       "│ 1473 │ 17.0    │ 3.0     │ 3.0     │ 1.0     │ 1.0     │ 1.0     │ 2.0     │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate header\n",
    "num_header = collect(1:10)\n",
    "my_header = [\"col\"*string(i) for i in num_header]\n",
    "\n",
    "#load king-rook file\n",
    "csv = CSV.File(FILEDIR * \"cmc.data\", header = my_header, types=[Float64,Float64,Float64,Float64,Float64,Float64,Float64,Float64,Float64,Int64])\n",
    "df = DataFrame(csv)\n",
    "\n",
    "#df = convert(DataArrays.DataArray{Float6464,10},df[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Real,1}:\n",
       " 24.0\n",
       "  2.0\n",
       "  3.0\n",
       "  1.0\n",
       "  0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review current data formatting\n",
    "unique(df[1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442-element Array{Int64,1}:\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " ⋮\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract feature array, fit to UnitTransformer\n",
    "features_to_fit = Matrix(select(df, Not(:col10)))\n",
    "unit_transformer = fit(UnitRangeTransform, transpose(features_to_fit))\n",
    "\n",
    "#shuffle observations and split into train/test\n",
    "df = shuffleobs(df)\n",
    "train, test = splitobs(df, at = 0.7)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "features = Matrix(select(train, Not(:col10)))\n",
    "features = StatsBase.transform(unit_transformer, transpose(features))\n",
    "features = Matrix(transpose(features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "labels = Array(train.col10)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "test_features = Matrix(select(test, Not(:col10)))\n",
    "test_features = StatsBase.transform(unit_transformer, transpose(test_features))\n",
    "test_features = Matrix(transpose(test_features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "test_labels = Array(test.col10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels must be one-indexed for the model to function properly, so I confirm as much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 3\n",
       " 2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm that labels are 1-indexed\n",
    "#labels = labels .- 2\n",
    "unique(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of comparison, I will now fit a simple Decision Tree over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 0.03125\n",
      "L-> Feature 7, Threshold 0.8333333333333333\n",
      "    L-> 1 : 62/62\n",
      "    R-> 1 : 2/3\n",
      "R-> Feature 2, Threshold 0.8333333333333333\n",
      "    L-> 1 : 285/590\n",
      "    R-> 2 : 143/376\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=2)\n",
    "DecisionTree.fit!(dt_model, features, labels)\n",
    "print_tree(dt_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeYMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function findEpsilon(array)\n",
    "    #capture array length for iteration\n",
    "    array_len = size(array, 1)\n",
    "    \n",
    "    #sort array\n",
    "    sorted_array = sort(array, rev = true)\n",
    "    \n",
    "    #initialize epsilon with arbitrarily large value\n",
    "    epsilon = 1e5\n",
    "    \n",
    "    #iterate through consecutive values to find smallest non-zero difference\n",
    "    for i in 2:(array_len - 1)\n",
    "        diff = abs(sorted_array[i-1] - sorted_array[i])\n",
    "        if 0 < diff < epsilon\n",
    "            epsilon = diff\n",
    "        end\n",
    "    end\n",
    "    #output smallest non-zero difference\n",
    "    return epsilon\n",
    "end\n",
    "\n",
    "function epsilonArrayGenerator(matrix)\n",
    "    #capture number of features in dataset, i.e. the number of columns\n",
    "    matrix_rows = size(matrix, 2)\n",
    "    \n",
    "    #initialize output DataFrame\n",
    "    epsilon_array = Vector{Float64}(undef,matrix_rows)\n",
    "    \n",
    "    #iterate findEpsilon function over each column\n",
    "    for col in 1:matrix_rows\n",
    "        epsilon_array[col] = findEpsilon(matrix[:,col])\n",
    "    end\n",
    "    return epsilon_array\n",
    "end\n",
    "\n",
    "function makeAncestorDict(max_nodes)\n",
    "    #initialize empty dictionaries\n",
    "    A_left = Dict{Int64, Vector{Int64}}()\n",
    "    A_right = Dict{Int64, Vector{Int64}}()\n",
    "    #A_left[1] = [1]\n",
    "    #A_right[1] = [1]\n",
    "    #generate keys with empty array values for each node\n",
    "    for i in 1:max_nodes\n",
    "        A_left[i] = []\n",
    "        A_right[i] = []\n",
    "    end\n",
    "    #loop over all nodes, copying the left and right ancestors of the node above it\n",
    "    for i in 2:max_nodes\n",
    "        left_ancestors = copy(A_left[i ÷ 2])\n",
    "        right_ancestors = copy(A_right[i ÷ 2])\n",
    "        direct_ancestor = i ÷ 2\n",
    "        A_left[i] = left_ancestors\n",
    "        A_right[i] = right_ancestors\n",
    "        #add a left ancestor to even nodes\n",
    "        if i/2 == i ÷ 2\n",
    "            append!(left_ancestors, direct_ancestor)\n",
    "            A_left[i] = left_ancestors\n",
    "        #add a right ancestor to odd nodes\n",
    "        else\n",
    "            append!(right_ancestors, direct_ancestor)\n",
    "            A_right[i] = right_ancestors\n",
    "        end\n",
    "    end\n",
    "    return A_left, A_right\n",
    "end\n",
    "\n",
    "function makeYMatrix(labels)    \n",
    "    #extract dimensions for Y from label array\n",
    "    num_labels = length(unique(labels))\n",
    "    len_df = length(labels)\n",
    "    \n",
    "    #initialize empty matrix\n",
    "    Y = zeros(len_df, num_labels)\n",
    "    \n",
    "    #set all values to -1 - this will apply a penalty to incorrect predictions\n",
    "    Y = Y  .- 1\n",
    "    \n",
    "    #iterate n over each column, setting Y[n,k] = 1 when the label for x[i] = k\n",
    "    for k in 1:num_labels\n",
    "        for n in 1:len_df\n",
    "            if labels[n] == k\n",
    "                Y[n,k] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration of Model Constants\n",
    "\n",
    "In a given model space for this optimization function, there are a variety of constants that are either set by the user as a hyperparameter or generated dynamically based on those hyperparameters or the input data set.\n",
    "\n",
    "I will begin by establishing the numerical values related to the structure of the tree, $D, N_{min}, t$, and $\\alpha$  Here, $D$ is the maximum depth of the Tree, $N_{min}$ is the minimum number of samples needed to compose a leaf node, and $t$ is the total possible number of nodes in a tree.\n",
    "\n",
    "The last of these user-declared variables, $\\alpha$, is a complexity parameter that penalizes overly complex tree structures. (elaborate here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set maximum depth of tree as a constant\n",
    "max_depth = 2\n",
    "\n",
    "#minimum number of values for a given leaf node\n",
    "leaf_n_min = 20\n",
    "\n",
    "#declare complexity parameter alpha\n",
    "alpha = 0.1\n",
    "\n",
    "#find total number of nodes in tree using max_depth, t\n",
    "max_nodes = 2^(max_depth+1) - 1\n",
    "\n",
    "#initialize branch and leaf node arrays - first, find the split point between branch and leaf indices\n",
    "#split point is by definition the number of nodes integer divided by two\n",
    "leaf_branch_split = max_nodes ÷ 2\n",
    "\n",
    "#total number of branches\n",
    "t_b = collect(1:leaf_branch_split)\n",
    "\n",
    "#total number of leaves\n",
    "t_l = collect(leaf_branch_split+1:max_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will establish the model constants that are derived from the input dataset: $n, k$, and $p$.  Here, $n$ represents the number of samples within the dataset, $p$ represents the number of features in the dataset, and $k$ represents the total number of labels.\n",
    "\n",
    "I will also establish the constant $\\hat{L}$. This constant represents the \"naive\" prediction, namely, that every value in the dataset is a member of the most common class.  This value is then used within the optimization function (elaborate here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42870999030067897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull number of samples in dataset, n\n",
    "num_samples = size(features, 1)\n",
    "\n",
    "#find total number of columns in the feature space, p\n",
    "num_features = size(features, 2)\n",
    "\n",
    "#find total number of labels, k\n",
    "num_labels = length(unique(labels))\n",
    "\n",
    "#create dictionary with prediction labels as key and count of each prediction as value\n",
    "output_count = countmap(labels)\n",
    "\n",
    "#extract the count for most common label to form l_hat, which is baseline accuracy rate\n",
    "l_hat = sort(collect(output_count), by = tuple -> last(tuple), rev=true)[1,1][2]/length(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, I manually set $\\epsilon_j$ to $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031×3 Array{Float64,2}:\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  ⋮              \n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build ancestor dictionaries using definitions given above\n",
    "A_left, A_right = makeAncestorDict(max_nodes)\n",
    "\n",
    "#generate the epsilon array as defined earlier\n",
    "epsilon_array = epsilonArrayGenerator(features)\n",
    "\n",
    "#M_1 constant - defined as 1 plus the largest epsilon value\n",
    "M_1 = 1 + maximum(epsilon_array)\n",
    "\n",
    "#M constant - set equal to number of samples as rule of thumb\n",
    "M = length(labels)\n",
    "\n",
    "#generate Y matrix\n",
    "Y = makeYMatrix(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable and Constraint Declarations\n",
    "\n",
    "With the constants now fixed, the model itself can now be built inside JuMP.  The first step is to declare the model itself, at which point I also limit the runtime of the optimizer to two hours.\n",
    "\n",
    "From there, I will begin building the model by establishing the variables that model the structure of the tree itself.  The first of these are $b$, an array that captures the decision point for each node that applies a split, and $a_{j,t}$, a hot-coded matrix that indicates when feature $j$ is used to split at node $t$. Additionally, we initialize array $d$, which is hot-coded to indicate when a given branch is active (i.e. a split is applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model = Model(with_optimizer(Gurobi.Optimizer, Presolve=0, OutputFlag=1, TimeLimit=14400))\n",
    "\n",
    "#b is the decision point for each branch node\n",
    "#s/t a.T*x < b at a given split \n",
    "@variable(model, b[i=t_b])\n",
    "\n",
    "#a is a hot-coded matrix that captures the variable being used to split at given branch node\n",
    "@variable(model, a[j = 1:num_features, t = 1:leaf_branch_split])\n",
    "\n",
    "#4 - establish binary constraint on a\n",
    "for j in 1:num_features\n",
    "    for t in t_b\n",
    "        @constraint(model, a[j,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#d is an indicator array equal to one when a split is applied at a given node\n",
    "@variable(model, d[1:leaf_branch_split])\n",
    "\n",
    "#constrain d to binary values\n",
    "for t in t_b\n",
    "    @constraint(model, d[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#2 - establish that row-wise sum of a must equal 1 for all rows - yes\n",
    "for t in t_b\n",
    "    #@constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == 1)\n",
    "end\n",
    "\n",
    "#3 - establish that b split point must be 0 <= b[i] <= d[i] - yes\n",
    "for t in t_b\n",
    "    @constraint(model, b[t] >= 0)\n",
    "    @constraint(model, d[t] >= b[t])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is possible that certain branches will not be needed to achieve an optimal solution, I will establish that only those nodes which have a split applied above them can also apply a split.  This constraint ensures that once the optimizer no longer needs to split along a given branch path (i.e. that a given branch has already achieved perfect purity) it will simply route the input features to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- constrain d s/t splits cannot be applied below a node that does not also split\n",
    "#this does not apply to d[1] since that is the parent node and must always split\n",
    "@constraint(model, d[1] == 1)\n",
    "\n",
    "for t in 2:leaf_branch_split\n",
    "    parent = t ÷ 2\n",
    "    @constraint(model, d[t] <= d[parent])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z is a hot-coded matrix captures which values are assigned to which node\n",
    "@variable(model, z[i = 1:num_samples, t = t_l])\n",
    "\n",
    "#constrain z to binary values {0,1}\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#l is a hot-coded array s/t l(t) = 1 when leaf node t contains any values \n",
    "@variable(model, l[t = t_l])\n",
    "\n",
    "#contrain l to binary values {0,1}\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#6 - constrain predictions to only be fit into nodes containing points\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] <= l[t])\n",
    "    end\n",
    "end\n",
    "\n",
    "#7- constrain number of samples assigned to a given leaf by lower bound \n",
    "#s/t number of samples is always greater/equal to min leaf size constant\n",
    "for t in t_l\n",
    "    @constraint(model, sum(z[i,t] for i in 1:num_samples) >= leaf_n_min * l[t])\n",
    "end\n",
    "\n",
    "#8 - constrain each point in data set so it can only be assigned to one leaf node\n",
    "for i in 1:num_samples\n",
    "    @constraint(model, sum(z[i,t] for t in t_l) == 1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tree structure and variable spaces now established, I move on to the splitting constraints.  These constraints capture the \"path\" that leads to each leaf node.  For leaf node $t$, the left-hand path is taken at ancestor node(s) $A_{left}(t) = m$ when $A^T(x_i + \\epsilon) \\leq b(m)$, and the right-hand path is taken at ancestor node(s) $A_{right}(t) = m$ when $A^Tx_i \\geq b(m)$.\n",
    "\n",
    "(Note that equations 9-12 are intermediate steps that give the derivations below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 - establish left split constraints\n",
    "for i in 1:num_samples  \n",
    "    for t in t_l\n",
    "        for m in A_left[t]\n",
    "            @constraint(model, transpose(a[:,m]) * (features[i,:] + epsilon_array) <= b[m] + M_1*(1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "#14 - establish right split contraints\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        for m in A_right[t]      \n",
    "            @constraint(model, transpose(a[:,m]) * features[i,:] >= b[m] - (1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the branch nodes now fully modelled, we turn to a series of variables that capture the $x_i$ features present within each node.  The first of these is $N_{k,t}$, which gives the total number of inputs with label $k$ in leaf node $t$.  Paired with this is $N_t$, which gives the sum total of inputs assigned to each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_kt is the number of points with label k in leaf node t\n",
    "@variable(model, N_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#15 - establish values for N_kt[t]\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, N_kt[k,t] == 0.5 * sum((1 + Y[i,k])*z[i,t] for i = 1:num_samples))\n",
    "    end\n",
    "end\n",
    "\n",
    "#N_t is the total number of values in a leaf node t\n",
    "@variable(model, N_t[i = t_l])\n",
    "\n",
    "#16 - establish values for N_t[t] as sum of z[i,t] for each t\n",
    "for t in t_l\n",
    "    @constraint(model, N_t[t] == sum(z[i,t] for i = 1:num_samples))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with these variables, we capture the prediction made by each node in matrix $c_{k,t}$, which is hot-coded such that the prediction for leaf $t$ is $k$ when $c_{k,t} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_kt is a matrix that holds the label count of each variable within a given leaf nodes\n",
    "@variable(model, c_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#constrain c_kt to binary values {0,1}\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, c_kt[k,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#18 - force prediction for each node with values\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] == sum(c_kt[k,t] for k = 1:num_labels))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define loss array $L_t$, which is derived by applying a penalty factor for each prediction not in the majority class present in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L is the loss for a given leaf node t\n",
    "@variable(model, L[i = t_l])\n",
    "\n",
    "#20 - set loss function lower bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] >= N_t[t] - N_kt[k,t] - (M * (1 - c_kt[k,t])))\n",
    "    end\n",
    "end\n",
    "\n",
    "#21 - set loss function upper bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] <= N_t[t] - N_kt[k,t] + (M * c_kt[k,t]))\n",
    "    end\n",
    "end\n",
    "\n",
    "#22 - set all L values to be positive\n",
    "for t in t_l\n",
    "   @constraint(model, L[t] >= 0) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now set the objective function, which is to minimize loss relative to the complexity of the tree as measured by the number of active branch nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ 2.3325791855203617 L_{4} + 2.3325791855203617 L_{5} + 2.3325791855203617 L_{6} + 2.3325791855203617 L_{7} + 0.1 d_{1} + 0.1 d_{2} + 0.1 d_{3} $$"
      ],
      "text/plain": [
       "2.3325791855203617 L[4] + 2.3325791855203617 L[5] + 2.3325791855203617 L[6] + 2.3325791855203617 L[7] + 0.1 d[1] + 0.1 d[2] + 0.1 d[3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@objective(model, Min, (1/l_hat) * sum(L[t] for t in t_l)) + (alpha * sum(d[t] for t in t_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective function now established, we call the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Optimize a model with 13470 rows, 4193 columns and 107815 nonzeros\n",
      "Variable types: 23 continuous, 4170 integer (4170 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [3e-02, 1e+03]\n",
      "  Objective range  [2e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Variable types: 7 continuous, 4186 integer (4170 binary)\n",
      "Found heuristic solution: objective 1373.8891403\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 2203 iterations, 0.30 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0 1127 1373.88914    0.00000   100%     -    2s\n",
      "     0     0    0.00000    0 1500 1373.88914    0.00000   100%     -    2s\n",
      "     0     0    0.00000    0 1454 1373.88914    0.00000   100%     -    2s\n",
      "     0     0    0.00000    0  386 1373.88914    0.00000   100%     -    4s\n",
      "     0     0    0.00000    0  386 1373.88914    0.00000   100%     -    4s\n",
      "     0     0    0.00000    0  236 1373.88914    0.00000   100%     -    4s\n",
      "     0     0    0.00000    0  322 1373.88914    0.00000   100%     -    4s\n",
      "     0     0    0.00000    0  315 1373.88914    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0  320 1373.88914    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0  312 1373.88914    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0  389 1373.88914    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0  505 1373.88914    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0  518 1373.88914    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0  565 1373.88914    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0  431 1373.88914    0.00000   100%     -    7s\n",
      "     0     2    0.00000    0  415 1373.88914    0.00000   100%     -    7s\n",
      "    11    16    0.00000    4 1633 1373.88914    0.00000   100%   813   10s\n",
      "   314   142  230.19641   18 1448 1373.88914    0.00000   100%   287   15s\n",
      "  1523   499  159.97385   71 1272 1373.88914    0.00000   100%   115   20s\n",
      "  1843   736  217.96141  101 1346 1373.88914    0.00000   100%   130   25s\n",
      "  2006   885  237.57849  111 1195 1373.88914    0.00000   100%   127   31s\n",
      "H 2012   890                    1308.5769231    0.00000   100%   127   31s\n",
      "  2088   948  261.19586  123 1185 1308.57692    0.00000   100%   132   37s\n",
      "  2445  1262  443.92781  171 1392 1308.57692    0.00000   100%   148   40s\n",
      "  2722  1496  352.21946   71 1127 1308.57692    0.00000   100%   157   47s\n",
      "  2723  1497  102.56280  306 1142 1308.57692    0.00000   100%   156   52s\n",
      "  2727  1500  111.85777   53 1107 1308.57692    0.00000   100%   156   55s\n",
      "  2735  1505   65.10016   64  977 1308.57692    0.00000   100%   156   60s\n",
      "  2745  1515    0.00000   12  631 1308.57692    0.00000   100%   168   65s\n",
      "  2783  1526    0.00000   18 1973 1308.57692    0.00000   100%   176   70s\n",
      "  3255  1599    0.00000   29 1302 1308.57692    0.00000   100%   173   75s\n",
      "  4266  1533  366.50871  121 1525 1308.57692    0.00000   100%   146   80s\n",
      "H 4515  1398                    1299.2466063    0.00000   100%   139   80s\n",
      "  5610  1301   14.59629  255  532 1299.24661    0.00000   100%   117   85s\n",
      "  6250  1121  399.64279  165 1351 1299.24661    0.00000   100%   109   92s\n",
      "  6408  1079  406.74655  166 1347 1299.24661    0.00000   100%   108   99s\n",
      "H 6409  1006                    1268.9230769    0.00000   100%   108   99s\n",
      "  6412   987  410.06871  167 1345 1268.92308    0.00000   100%   108  103s\n",
      "  6631   923  579.71083  193 1234 1268.92308    0.00000   100%   105  120s\n",
      "  8169   874  962.68370  305  505 1268.92308    0.00000   100%  90.3  126s\n",
      "  9877   964 infeasible   49      1268.92308    0.00000   100%  80.1  130s\n",
      " 11386  1127 infeasible   53      1268.92308    0.00000   100%  73.7  135s\n",
      " 12830  1252 infeasible  138      1268.92308    0.00000   100%  69.8  140s\n",
      "H14506  1406                    1266.5904977    0.00000   100%  65.1  173s\n",
      " 14617  1438  289.09403   40 1270 1266.59050    0.00000   100%  65.1  176s\n",
      " 15917  1628  794.24321  309  546 1266.59050    0.00000   100%  62.4  180s\n",
      " 18145  1808    0.00000   49 1015 1266.59050    0.00000   100%  59.0  185s\n",
      " 20453  2125  271.49808  147 1001 1266.59050    0.00000   100%  55.6  284s\n",
      " 20907  2196    0.00000   54  885 1266.59050    0.00000   100%  55.4  286s\n",
      " 22358  2526 infeasible  105      1266.59050    0.00000   100%  54.7  290s\n",
      " 23880  2748    0.00000  294  631 1266.59050    0.00000   100%  53.5  295s\n",
      " 25902  2987    0.00000  254 1336 1266.59050    0.00000   100%  52.7  300s\n",
      " 26935  3050    0.00000  117  734 1266.59050    0.00000   100%  53.5  306s\n",
      " 27267  3098 infeasible  336      1266.59050    0.00000   100%  53.6  317s\n",
      "H27276  3098                    1252.5950226    0.00000   100%  53.7  317s\n",
      " 27414  3072 infeasible  370      1252.59502    0.00000   100%  53.9  320s\n",
      " 29226  3344    0.00000  245 1704 1252.59502    0.00000   100%  54.3  325s\n",
      " 31020  3358 infeasible  280      1252.59502    0.00000   100%  53.4  331s\n",
      " 33248  3542 infeasible  311      1252.59502    0.00000   100%  51.7  335s\n",
      " 35580  3714    0.00000  223 1663 1252.59502    0.00000   100%  49.7  340s\n",
      " 36451  3816    0.00000  311 1057 1252.59502    0.00000   100%  49.3  371s\n",
      " 37089  3792    0.00000  260 1049 1252.59502    0.00000   100%  49.2  375s\n",
      " 38626  3839 infeasible  304      1252.59502    0.00000   100%  49.3  380s\n",
      " 40201  3896    0.00000  346 1024 1252.59502    0.00000   100%  49.1  385s\n",
      " 41734  3845 infeasible  430      1252.59502    0.00000   100%  49.4  391s\n",
      " 43418  3967  440.11968  436  432 1252.59502    0.00000   100%  49.1  396s\n",
      " 44799  4075    3.52317  162 1323 1252.59502    0.00000   100%  49.2  401s\n",
      " 46449  4167    0.00000  364  503 1252.59502    0.00000   100%  48.7  405s\n",
      " 48311  4343  632.12896  427  148 1252.59502    0.00000   100%  48.5  410s\n",
      " 49194  4556 infeasible   97      1252.59502    0.00000   100%  49.1  415s\n",
      " 50490  5029    0.00000   17 1909 1252.59502    0.00000   100%  50.2  421s\n",
      " 50953  5110 infeasible   82      1252.59502    0.00000   100%  50.4  469s\n",
      " 51222  5148 infeasible  144      1252.59502    0.00000   100%  50.4  470s\n",
      " 52465  5395    2.84283  220 1403 1252.59502    0.00000   100%  50.4  476s\n",
      " 54176  5793 infeasible  115      1252.59502    0.00000   100%  50.3  481s\n",
      " 55757  6070 infeasible   80      1252.59502    0.00000   100%  49.9  486s\n",
      " 57251  6295 infeasible  124      1252.59502    0.00000   100%  49.9  491s\n",
      " 58776  6739    0.00000   29 1548 1252.59502    0.00000   100%  50.2  495s\n",
      " 59813  6936    2.36792   94 1601 1252.59502    0.00000   100%  50.6  500s\n",
      " 61790  7644 infeasible  205      1252.59502    0.00000   100%  50.6  505s\n",
      " 63342  7784    2.33258  145 1367 1252.59502    0.00000   100%  50.5  511s\n",
      " 65196  7953 infeasible  147      1252.59502    0.00000   100%  49.9  516s\n",
      " 66751  8110   14.14126  113 1248 1252.59502    0.00000   100%  49.8  520s\n",
      " 68667  8381    5.08843   73 1629 1252.59502    0.00000   100%  49.5  525s\n",
      " 69814  8607    4.66516   92 1630 1252.59502    0.00000   100%  49.9  530s\n",
      " 70515  8777 infeasible   66      1252.59502    0.00000   100%  50.6  535s\n",
      " 70987  8866    2.73312  103 1836 1252.59502    0.00000   100%  51.5  540s\n",
      " 71701  9056 infeasible   58      1252.59502    0.00000   100%  52.0  545s\n",
      " 72738  9211   11.96981   77 1499 1252.59502    0.00000   100%  52.1  550s\n",
      " 74505  9763 infeasible  189      1252.59502    0.00000   100%  51.8  555s\n",
      " 75777 10057    5.35921   87 1641 1252.59502    0.00000   100%  52.2  560s\n",
      " 76272 10122    0.00000   35 1594 1252.59502    0.00000   100%  52.6  566s\n",
      " 77552 10444    0.03534   46 1995 1252.59502    0.00000   100%  52.9  571s\n",
      " 78482 10592    2.33258   59 1656 1252.59502    0.00000   100%  53.4  575s\n",
      " 79588 10832    0.00000   74 1369 1252.59502    0.00000   100%  53.9  580s\n",
      " 80601 11073   13.26021   48 1672 1252.59502    0.00000   100%  54.4  586s\n",
      " 81678 11405    0.00000   47 1485 1252.59502    0.00000   100%  54.7  591s\n",
      " 82705 11622   97.96833   64 1170 1252.59502    0.00000   100%  55.0  644s\n",
      " 83038 11662    0.00000  198 1331 1252.59502    0.00000   100%  54.9  646s\n",
      " 83601 11658    0.00000   19 1490 1252.59502    0.00000   100%  55.1  651s\n",
      " 84234 11738  241.03318   74 1272 1252.59502    0.00000   100%  55.4  657s\n",
      " 85287 11907    0.00000   34 1785 1252.59502    0.00000   100%  55.5  661s\n",
      " 85897 12118    0.39725   24 1531 1252.59502    0.00000   100%  55.9  665s\n",
      " 87070 12491 infeasible   53      1252.59502    0.00000   100%  56.1  671s\n",
      " 87541 12689 infeasible   33      1252.59502    0.00000   100%  56.3  675s\n",
      " 88762 12890  565.12840  155 1413 1252.59502    0.00000   100%  56.4  682s\n",
      " 89592 13152    2.33258   77  852 1252.59502    0.00000   100%  56.5  685s\n",
      " 90178 13247   31.67179   67 1624 1252.59502    0.00000   100%  56.8  690s\n",
      " 91141 13385    0.00000   36 1773 1252.59502    0.00000   100%  57.2  695s\n",
      " 92340 13639  146.73381  218  696 1252.59502    0.00000   100%  57.1  700s\n",
      " 93372 13918    0.05173   50  952 1252.59502    0.00000   100%  57.4  707s\n",
      " 94024 14040    2.36792  121  833 1252.59502    0.00000   100%  57.7  711s\n",
      " 94647 14175    0.00000   65  888 1252.59502    0.00000   100%  58.1  716s\n",
      " 95684 14418   10.46235   99  913 1252.59502    0.00000   100%  58.2  721s\n",
      " 96421 14564    0.36447   57 1327 1252.59502    0.00000   100%  58.5  726s\n",
      " 97312 14798    0.39416   34 1788 1252.59502    0.00000   100%  58.8  730s\n",
      " 97997 14916 infeasible   80      1252.59502    0.00000   100%  59.1  735s\n",
      " 98787 15043    0.12959   40 1303 1252.59502    0.00000   100%  59.3  740s\n",
      " 99611 15141    0.07099   46  894 1252.59502    0.00000   100%  60.0  747s\n",
      " 100170 15234    0.00000   87 1535 1252.59502    0.00000   100%  60.2  750s\n",
      " 101060 15330 infeasible   98      1252.59502    0.00000   100%  60.5  796s\n",
      " 101308 15304    0.14137   49 1304 1252.59502    0.00000   100%  60.6  801s\n",
      " 102333 15508 infeasible   57      1252.59502    0.00000   100%  60.9  807s\n",
      " 102949 15595 infeasible   40      1252.59502    0.00000   100%  61.3  810s\n",
      " 104170 15960    7.25601  160 1348 1252.59502    0.00000   100%  61.5  816s\n",
      " 106127 16472    0.00000  155 1117 1252.59502    0.00000   100%  61.5  822s\n",
      " 107162 16658   11.66290  120 1249 1252.59502    0.00000   100%  61.4  825s\n",
      " 108888 17123    5.18351   63 1460 1252.59502    0.00000   100%  61.6  831s\n",
      " 109505 17360   24.49208   48 1422 1252.59502    0.00000   100%  61.8  835s\n",
      " 110675 17688    0.28274   40 1227 1252.59502    0.00000   100%  62.2  842s\n",
      " 111245 17901    0.00000   18 1651 1252.59502    0.00000   100%  62.6  847s\n",
      " 112238 18104    0.77753   61 1427 1252.59502    0.00000   100%  62.9  852s\n",
      " 113121 18246 infeasible   44      1252.59502    0.00000   100%  63.0  856s\n",
      " 113814 18352    5.17541   37 1390 1252.59502    0.00000   100%  63.3  861s\n",
      " 115206 18595    0.00000  274  801 1252.59502    0.00000   100%  63.1  865s\n",
      " 116734 18611 infeasible  219      1252.59502    0.00000   100%  62.6  870s\n",
      " 118552 18630  702.58170  371  757 1252.59502    0.00000   100%  62.1  876s\n",
      " 120632 18622    0.00000  164  903 1252.59502    0.00000   100%  61.7  882s\n",
      " 122068 18738    0.00000   57 1712 1252.59502    0.00000   100%  61.5  885s\n",
      " 123172 18823  249.23756  167 1312 1252.59502    0.00000   100%  61.4  892s\n",
      " 123568 18827  663.40673  175  811 1252.59502    0.00000   100%  61.3  895s\n",
      " 125182 19004    0.00000   68 1725 1252.59502    0.00000   100%  61.4  900s\n",
      " 126778 19111 infeasible   60      1252.59502    0.00000   100%  61.4  905s\n",
      " 128267 19415    0.17278   76 1543 1252.59502    0.00000   100%  61.7  911s\n",
      " 129097 19337 infeasible  182      1252.59502    0.00000   100%  61.8  915s\n",
      " 130063 19506    2.33258   34  847 1252.59502    0.00000   100%  62.1  920s\n",
      " 131090 19673    6.99774  125 1340 1252.59502    0.00000   100%  62.3  925s\n",
      " 132494 19915    4.66516   38 1987 1252.59502    0.00000   100%  62.4  931s\n",
      " 133371 20032    0.00000   62 1364 1252.59502    0.00000   100%  62.4  935s\n",
      " 134278 20129   30.32353   57 1537 1252.59502    0.00000   100%  62.7  941s\n",
      " 135422 20286    2.33258   62  972 1252.59502    0.00000   100%  62.8  945s\n",
      " 136182 20400    2.33258   53 1587 1252.59502    0.00000   100%  62.8 1001s\n",
      " 136465 20411 infeasible   44      1252.59502    0.00000   100%  62.8 1005s\n",
      " 137370 20514 infeasible   43      1252.59502    0.00000   100%  62.8 1011s\n",
      " 138828 20775    0.00000  112 1250 1252.59502    0.00000   100%  62.6 1015s\n",
      " 140957 20996 infeasible  147      1252.59502    0.00000   100%  62.2 1020s\n",
      " 142660 21423 infeasible  123      1252.59502    0.00000   100%  62.0 1025s\n",
      " 144276 21646   51.38964   87 1030 1252.59502    0.00000   100%  61.7 1030s\n",
      " 145766 22019    0.00000   68 1173 1252.59502    0.00000   100%  61.4 1035s\n",
      " 148209 22509    0.77753  157  737 1252.59502    0.00000   100%  60.9 1041s\n",
      " 150034 22727 infeasible  202      1252.59502    0.00000   100%  60.5 1045s\n",
      " 152003 23123    2.33258  183  598 1252.59502    0.00000   100%  60.0 1050s\n",
      " 154287 23221    0.00000  197  424 1252.59502    0.00000   100%  59.4 1055s\n",
      " 155860 23426    0.00000   57  968 1252.59502    0.00000   100%  59.2 1060s\n",
      " 157584 23772 infeasible  167      1252.59502    0.00000   100%  59.0 1065s\n",
      " 159577 24006 infeasible  212      1252.59502    0.00000   100%  58.7 1070s\n",
      " 161021 24453 infeasible  145      1252.59502    0.00000   100%  58.6 1075s\n",
      " 162593 24926    0.00000   65 1699 1252.59502    0.00000   100%  58.5 1080s\n",
      " 164358 25528 infeasible   67      1252.59502    0.00000   100%  58.3 1085s\n",
      " 165768 25547 infeasible   83      1252.59502    0.00000   100%  58.0 1091s\n",
      " 167163 25846   10.54520  200  955 1252.59502    0.00000   100%  57.7 1095s\n",
      " 168909 25710 infeasible  122      1252.59502    0.00000   100%  57.3 1100s\n",
      " 170568 25802 infeasible  160      1252.59502    0.00000   100%  57.0 1105s\n",
      " 172345 26128    0.07289  100 1088 1252.59502    0.00000   100%  56.9 1110s\n",
      " 174158 26469 infeasible  105      1252.59502    0.00000   100%  56.7 1115s\n",
      " 176008 26398    2.33258  153 1005 1252.59502    0.00000   100%  56.4 1120s\n",
      " 177754 26578    2.33258  122 1153 1252.59502    0.00000   100%  56.3 1125s\n",
      " 178542 26695 infeasible  130      1252.59502    0.00000   100%  56.3 1130s\n",
      " 180510 26933 infeasible  145      1252.59502    0.00000   100%  56.1 1135s\n",
      " 182562 26907   13.55812  111  580 1252.59502    0.00000   100%  55.8 1140s\n",
      " 184641 26900    0.00000  118  611 1252.59502    0.00000   100%  55.6 1146s\n",
      " 186535 27102    2.33258  114  567 1252.59502    0.00000   100%  55.4 1150s\n",
      " 188693 27189    2.33258  124  732 1252.59502    0.00000   100%  55.1 1155s\n",
      " 190524 27388 infeasible  172      1252.59502    0.00000   100%  54.8 1160s\n",
      " 191979 27551    0.00000  112  995 1252.59502    0.00000   100%  54.7 1165s\n",
      " 194168 27617 infeasible  183      1252.59502    0.00000   100%  54.3 1170s\n",
      " 196046 27662 infeasible  181      1252.59502    0.00000   100%  54.0 1175s\n",
      " 197717 27636 infeasible  159      1252.59502    0.00000   100%  53.7 1180s\n",
      " 199579 27836    6.99774   88 1308 1252.59502    0.00000   100%  53.5 1185s\n",
      " 200882 28113 infeasible  133      1252.59502    0.00000   100%  53.3 1190s\n",
      " 202953 28277   12.97497  160  939 1252.59502    0.00000   100%  53.1 1195s\n",
      " 204551 28131 infeasible  164      1252.59502    0.00000   100%  52.9 1200s\n",
      " 206462 28444   10.35082  175  828 1252.59502    0.00000   100%  52.7 1205s\n",
      " 208359 28471 infeasible  151      1252.59502    0.00000   100%  52.6 1210s\n",
      " 210020 28391 infeasible  139      1252.59502    0.00000   100%  52.4 1215s\n",
      " 212060 28348    5.92864  121 1030 1252.59502    0.00000   100%  52.2 1220s\n",
      " 213747 28362    5.34549  156  941 1252.59502    0.00000   100%  52.1 1225s\n",
      " 215428 28525    0.00000  161  927 1252.59502    0.00000   100%  51.9 1230s\n",
      " 217719 28378 infeasible  151      1252.59502    0.00000   100%  51.9 1235s\n",
      " 219602 28465 infeasible  166      1252.59502    0.00000   100%  51.7 1240s\n",
      " 221564 28455 infeasible  117      1252.59502    0.00000   100%  51.6 1245s\n",
      " 223480 28594 infeasible  156      1252.59502    0.00000   100%  51.4 1250s\n",
      " 225761 28555   17.68873  191  815 1252.59502    0.00000   100%  51.2 1255s\n",
      " 227951 28704 infeasible  159      1252.59502    0.00000   100%  50.9 1260s\n",
      " 229824 28784 infeasible  100      1252.59502    0.00000   100%  50.7 1265s\n",
      " 231945 28849 infeasible  154      1252.59502    0.00000   100%  50.6 1270s\n",
      " 233969 28796    0.00000  136  914 1252.59502    0.00000   100%  50.4 1276s\n",
      " 236132 28970    0.00000  158  782 1252.59502    0.00000   100%  50.2 1281s\n",
      " 238217 29014    5.15111  175  730 1252.59502    0.00000   100%  49.9 1285s\n",
      " 240411 29060 infeasible  184      1252.59502    0.00000   100%  49.7 1290s\n",
      " 241996 29105 infeasible  107      1252.59502    0.00000   100%  49.6 1295s\n",
      " 242128 29094 infeasible  123      1252.59502    0.00000   100%  49.5 1340s\n",
      " 242328 29095    2.33258  122  799 1252.59502    0.00000   100%  49.5 1364s\n",
      " 242424 29120    2.33258  135  749 1252.59502    0.00000   100%  49.5 1365s\n",
      " 244129 29096   27.65078  204  731 1252.59502    0.00000   100%  49.4 1370s\n",
      " 246636 29215 infeasible  119      1252.59502    0.00000   100%  49.2 1376s\n",
      " 248452 29223 infeasible  170      1252.59502    0.00000   100%  49.0 1380s\n",
      " 250884 29330   16.23086  158  976 1252.59502    0.00000   100%  48.8 1385s\n",
      " 252444 29224   10.93396  125 1051 1252.59502    0.00000   100%  48.7 1390s\n",
      " 254138 29508 infeasible  168      1252.59502    0.00000   100%  48.6 1395s\n",
      " 255645 29473 infeasible  158      1252.59502    0.00000   100%  48.5 1400s\n",
      " 257518 29585    0.00000  154 1034 1252.59502    0.00000   100%  48.3 1405s\n",
      " 259272 29672 infeasible   93      1252.59502    0.00000   100%  48.3 1410s\n",
      " 260624 29766    0.00000  155  963 1252.59502    0.00000   100%  48.2 1415s\n",
      " 261915 29647    0.00000  123 1118 1252.59502    0.00000   100%  48.1 1420s\n",
      " 263714 30064 infeasible  192      1252.59502    0.00000   100%  48.0 1425s\n",
      " 264895 30213 infeasible  254      1252.59502    0.00000   100%  48.0 1430s\n",
      " 266623 30414    0.00000  201  599 1252.59502    0.00000   100%  47.9 1435s\n",
      " 267727 30570    2.33258   82 1280 1252.59502    0.00000   100%  47.8 1440s\n",
      " 269281 30959    0.43449  176  979 1252.59502    0.00000   100%  47.8 1445s\n",
      " 270357 31045   11.66290  161  835 1252.59502    0.00000   100%  47.8 1450s\n",
      " 271961 31068    0.00000  157  763 1252.59502    0.00000   100%  47.7 1455s\n",
      " 273460 31379 infeasible  106      1252.59502    0.00000   100%  47.7 1460s\n",
      " 275158 31502 infeasible   66      1252.59502    0.00000   100%  47.5 1465s\n",
      " 276263 31572 infeasible   99      1252.59502    0.00000   100%  47.4 1470s\n",
      " 277663 31870    0.00000   64  904 1252.59502    0.00000   100%  47.4 1475s\n",
      " 279426 32383    0.00000   93  853 1252.59502    0.00000   100%  47.4 1480s\n",
      " 280290 32358 infeasible  138      1252.59502    0.00000   100%  47.4 1485s\n",
      " 281770 32642    2.33258  148  969 1252.59502    0.00000   100%  47.4 1490s\n",
      " 283688 33012 infeasible   97      1252.59502    0.00000   100%  47.4 1496s\n",
      " 284862 33249    2.33258  191  922 1252.59502    0.00000   100%  47.4 1501s\n",
      " 286080 33568    0.00000  121 1172 1252.59502    0.00000   100%  47.4 1534s\n",
      " 286235 33623    0.00000  158 1107 1252.59502    0.00000   100%  47.5 1535s\n",
      " 287179 33932    0.00000   87  822 1252.59502    0.00000   100%  47.5 1540s\n",
      " 288746 34326    0.00000  106  942 1252.59502    0.00000   100%  47.4 1545s\n",
      " 290263 34408 infeasible   74      1252.59502    0.00000   100%  47.4 1551s\n",
      " 291369 34691 infeasible   38      1252.59502    0.00000   100%  47.5 1555s\n",
      " 292217 34891    0.00000   35 1706 1252.59502    0.00000   100%  47.6 1560s\n",
      " 293252 35032    2.33258   62 1710 1252.59502    0.00000   100%  47.7 1565s\n",
      " 294316 35283    2.33258   88 1354 1252.59502    0.00000   100%  47.8 1570s\n",
      " 295509 35586    0.00000  341 1020 1252.59502    0.00000   100%  47.9 1576s\n",
      " 296755 35776 infeasible  182      1252.59502    0.00000   100%  48.0 1580s\n",
      " 297907 35818    0.00000  212 1089 1252.59502    0.00000   100%  48.0 1585s\n",
      " 299119 36051    0.00000  283  931 1252.59502    0.00000   100%  47.9 1612s\n",
      " 299849 35890 infeasible  242      1252.59502    0.00000   100%  47.9 1615s\n",
      " 301018 35688 infeasible   72      1252.59502    0.00000   100%  48.0 1621s\n",
      " 302007 35846    0.58314   48 1586 1252.59502    0.00000   100%  48.1 1626s\n",
      " 303413 36140    0.00000   94 1550 1252.59502    0.00000   100%  48.1 1631s\n",
      " 304408 36212    0.25918   50 1778 1252.59502    0.00000   100%  48.2 1636s\n",
      " 305980 36509 infeasible  162      1252.59502    0.00000   100%  48.1 1640s\n",
      " 307393 36694    2.33258   64 1494 1252.59502    0.00000   100%  48.3 1645s\n",
      " 308435 36866 infeasible   89      1252.59502    0.00000   100%  48.4 1651s\n",
      " 309450 37135   11.66961  124 1550 1252.59502    0.00000   100%  48.5 1656s\n",
      " 309738 37212   11.68025  179 1476 1252.59502    0.00000   100%  48.5 1695s\n",
      "H309750 37207                    1180.2850679    0.00000   100%  48.6 1695s\n",
      " 310210 37271    4.66516   69 1707 1180.28507    0.00000   100%  48.6 1700s\n",
      " 311365 37573    0.25678   46 1799 1180.28507    0.00000   100%  48.7 1706s\n",
      " 312539 37917    0.00000   91 1265 1180.28507    0.00000   100%  48.9 1711s\n",
      " 313509 38075   32.65611  151 1171 1180.28507    0.00000   100%  49.0 1716s\n",
      " 314386 38153 infeasible   70      1180.28507    0.00000   100%  49.0 1720s\n",
      " 316230 38606 infeasible  226      1180.28507    0.00000   100%  48.9 1725s\n",
      " 317235 38829    6.99774  130 1163 1180.28507    0.00000   100%  49.0 1730s\n",
      " 319407 39387 infeasible  114      1180.28507    0.00000   100%  48.9 1736s\n",
      " 321230 39609    0.00000  194  584 1180.28507    0.00000   100%  48.9 1740s\n",
      " 322905 39489 infeasible  182      1180.28507    0.00000   100%  48.8 1745s\n",
      " 324695 39472 infeasible  169      1180.28507    0.00000   100%  48.7 1750s\n",
      " 326100 39505 infeasible  143      1180.28507    0.00000   100%  48.7 1755s\n",
      " 327868 39604    0.36447  242  494 1180.28507    0.00000   100%  48.7 1760s\n",
      " 329779 39681    8.89296  249  590 1180.28507    0.00000   100%  48.6 1766s\n",
      " 331050 39881 infeasible  109      1180.28507    0.00000   100%  48.7 1770s\n",
      " 332176 39996    2.33258  132  974 1180.28507    0.00000   100%  48.8 1776s\n",
      " 333256 40261 infeasible   75      1180.28507    0.00000   100%  48.8 1780s\n",
      " 334757 40673    2.33258  155 1276 1180.28507    0.00000   100%  48.9 1786s\n",
      " 336095 41094    0.00000   54 1600 1180.28507    0.00000   100%  49.0 1791s\n",
      " 336996 41218 infeasible  104      1180.28507    0.00000   100%  49.1 1795s\n",
      " 338143 41175 infeasible  266      1180.28507    0.00000   100%  49.1 1800s\n",
      " 339151 41372    6.58468   99 1497 1180.28507    0.00000   100%  49.2 1805s\n",
      " 340631 41637 infeasible  129      1180.28507    0.00000   100%  49.3 1811s\n",
      " 341899 41889    0.00000   83 1131 1180.28507    0.00000   100%  49.3 1863s\n",
      " 342298 41995    4.66516  202  882 1180.28507    0.00000   100%  49.3 1865s\n",
      " 343436 42161    0.56433   61 1711 1180.28507    0.00000   100%  49.4 1870s\n",
      " 345254 42670 infeasible   46      1180.28507    0.00000   100%  49.4 1876s\n",
      " 346610 43079 infeasible   55      1180.28507    0.00000   100%  49.4 1881s\n",
      " 347872 43472 infeasible   39      1180.28507    0.00000   100%  49.5 1886s\n",
      " 349260 43753    0.00000  242  730 1180.28507    0.00000   100%  49.4 1890s\n",
      " 351085 43816 infeasible   91      1180.28507    0.00000   100%  49.3 1895s\n",
      " 352474 44057 infeasible   77      1180.28507    0.00000   100%  49.3 1900s\n",
      " 354051 44283 infeasible  329      1180.28507    0.00000   100%  49.3 1905s\n",
      " 355672 44526    6.99774   52 1270 1180.28507    0.00000   100%  49.3 1911s\n",
      " 357026 44885 infeasible  132      1180.28507    0.00000   100%  49.3 1915s\n",
      " 358347 45284 infeasible  111      1180.28507    0.00000   100%  49.3 1921s\n",
      " 359735 45777 infeasible  147      1180.28507    0.00000   100%  49.4 1926s\n",
      " 361069 46175 infeasible   44      1180.28507    0.00000   100%  49.4 1930s\n",
      " 362784 46733 infeasible  133      1180.28507    0.00000   100%  49.4 1935s\n",
      " 364070 46875    0.00000   52 1533 1180.28507    0.00000   100%  49.4 1941s\n",
      " 365102 47260    0.00000  129  932 1180.28507    0.00000   100%  49.5 1945s\n",
      " 366007 47391    0.00000   50 1429 1180.28507    0.00000   100%  49.5 1950s\n",
      " 367363 47662    0.00000  124  761 1180.28507    0.00000   100%  49.6 1956s\n",
      " 369486 48001 infeasible  217      1180.28507    0.00000   100%  49.4 1960s\n",
      " 371026 47761  153.17270  120  923 1180.28507    0.00000   100%  49.3 1965s\n",
      " 372254 47813 infeasible  192      1180.28507    0.00000   100%  49.2 1970s\n",
      " 373872 48055    0.00000  146  665 1180.28507    0.00000   100%  49.1 1975s\n",
      " 375817 48167  182.32994  140  953 1180.28507    0.00000   100%  49.1 1980s\n",
      " 377217 48053  290.79487  143  832 1180.28507    0.00000   100%  49.0 1985s\n",
      " 379316 48216    0.00000  133  727 1180.28507    0.00000   100%  49.0 1990s\n",
      " 381534 48476 infeasible  158      1180.28507    0.00000   100%  48.8 1995s\n",
      " 383191 48371    0.00000  110  715 1180.28507    0.00000   100%  48.7 2000s\n",
      " 384972 48518  293.51621  158  746 1180.28507    0.00000   100%  48.7 2006s\n",
      " 386258 48537    0.00000  127  709 1180.28507    0.00000   100%  48.6 2011s\n",
      " 387211 48606    0.00000  145  666 1180.28507    0.00000   100%  48.6 2016s\n",
      " 388624 48565  196.25473  163  884 1180.28507    0.00000   100%  48.5 2020s\n",
      " 390265 48608    0.00000  137  582 1180.28507    0.00000   100%  48.4 2032s\n",
      " 390714 48604  343.66667  181  588 1180.28507    0.00000   100%  48.4 2035s\n",
      " 393153 48629 infeasible  104      1180.28507    0.00000   100%  48.3 2040s\n",
      " 395212 48698 infeasible  214      1180.28507    0.00000   100%  48.2 2046s\n",
      " 396276 48725  192.43778  113 1093 1180.28507    0.00000   100%  48.2 2050s\n",
      " 397164 48697 infeasible  281      1180.28507    0.00000   100%  48.1 2055s\n",
      " 398075 48808    0.00000  124  855 1180.28507    0.00000   100%  48.1 2060s\n",
      " 398721 48800 infeasible  164      1180.28507    0.00000   100%  48.1 2065s\n",
      " 399847 48731 infeasible  168      1180.28507    0.00000   100%  48.1 2071s\n",
      " 400551 48814    0.00000  198  385 1180.28507    0.00000   100%  48.1 2075s\n",
      " 401171 48862    0.00000  183  677 1180.28507    0.00000   100%  48.1 2080s\n",
      "H401887 48880                    1170.9547511    0.00000   100%  48.1 2082s\n",
      " 402163 48896    0.30293  171 1358 1170.95475    0.00000   100%  48.1 2117s\n",
      " 402660 48894    0.00000  272  842 1170.95475    0.00000   100%  48.1 2121s\n",
      " 404064 49258    0.00000  255  813 1170.95475    0.00000   100%  48.2 2126s\n",
      " 405332 49384    0.00000   50 1372 1170.95475    0.00000   100%  48.2 2131s\n",
      " 406289 49764 infeasible   74      1170.95475    0.00000   100%  48.2 2135s\n",
      " 406996 49938    0.00000  164 1332 1170.95475    0.00000   100%  48.3 2140s\n",
      " 407979 50202 infeasible   48      1170.95475    0.00000   100%  48.5 2146s\n",
      " 408957 50403    0.18849   81 1769 1170.95475    0.00000   100%  48.6 2151s\n",
      " 410291 50658    0.00000   67 1399 1170.95475    0.00000   100%  48.7 2156s\n",
      " 411122 50785    0.00000  181 1326 1170.95475    0.00000   100%  48.7 2160s\n",
      " 412348 50840 infeasible  158      1170.95475    0.00000   100%  48.8 2165s\n",
      " 413272 51003 infeasible  177      1170.95475    0.00000   100%  48.8 2170s\n",
      " 414722 51290    0.00000   82 1518 1170.95475    0.00000   100%  48.9 2177s\n",
      " 415598 51459 infeasible   81      1170.95475    0.00000   100%  49.0 2221s\n",
      " 416081 51491    0.53013   70 1372 1170.95475    0.00000   100%  49.1 2225s\n",
      " 416579 51560    2.33258  126 1384 1170.95475    0.00000   100%  49.1 2230s\n",
      " 418101 51968 infeasible   87      1170.95475    0.00000   100%  49.3 2235s\n",
      " 418907 52039    0.00000   60 1382 1170.95475    0.00000   100%  49.4 2242s\n",
      " 420207 52408   32.65611  284 1093 1170.95475    0.00000   100%  49.5 2246s\n",
      " 421581 52904   23.32579  319  962 1170.95475    0.00000   100%  49.5 2250s\n",
      " 423022 53234    0.00000   43 1614 1170.95475    0.00000   100%  49.6 2256s\n",
      " 423751 53449    0.00000   83 1318 1170.95475    0.00000   100%  49.7 2260s\n",
      " 425272 53716 infeasible   99      1170.95475    0.00000   100%  49.8 2266s\n",
      " 426345 53770    2.33258  109 1166 1170.95475    0.00000   100%  49.8 2271s\n",
      " 427432 54021    7.06842  250  897 1170.95475    0.00000   100%  49.9 2276s\n",
      " 429153 54346    0.00000  137 1238 1170.95475    0.00000   100%  49.9 2281s\n",
      " 430124 54662    0.00000   72 1570 1170.95475    0.00000   100%  49.9 2285s\n",
      " 431267 54909 infeasible   86      1170.95475    0.00000   100%  50.0 2290s\n",
      " 432927 55276    2.33258  241  781 1170.95475    0.00000   100%  50.0 2295s\n",
      " 434420 55395 infeasible  199      1170.95475    0.00000   100%  50.0 2300s\n",
      " 435658 55465 infeasible  137      1170.95475    0.00000   100%  50.2 2305s\n",
      " 436805 55765    4.80653  133 1151 1170.95475    0.00000   100%  50.2 2311s\n",
      " 437863 55947    9.72350  297  612 1170.95475    0.00000   100%  50.2 2315s\n",
      " 440080 56062 infeasible  254      1170.95475    0.00000   100%  50.2 2320s\n",
      " 442080 56289    0.56864  295  649 1170.95475    0.00000   100%  50.2 2325s\n",
      " 442839 56346    9.93113  208  794 1170.95475    0.00000   100%  50.2 2330s\n",
      " 444811 56696 infeasible  183      1170.95475    0.00000   100%  50.1 2337s\n",
      " 445671 56753    0.00000  109 1299 1170.95475    0.00000   100%  50.1 2340s\n",
      " 447547 57152    2.33258  134 1038 1170.95475    0.00000   100%  50.1 2346s\n",
      " 448577 57424 infeasible   85      1170.95475    0.00000   100%  50.1 2351s\n",
      " 449343 57419   13.99548  112  809 1170.95475    0.00000   100%  50.2 2355s\n",
      " 450592 57585    0.00000  220  707 1170.95475    0.00000   100%  50.2 2360s\n",
      " 451544 57652 infeasible  154      1170.95475    0.00000   100%  50.3 2366s\n",
      " 452489 57774 infeasible  136      1170.95475    0.00000   100%  50.3 2370s\n",
      " 453950 58055    2.33258   75 1497 1170.95475    0.00000   100%  50.3 2375s\n",
      " 455069 58344    2.33258  155 1265 1170.95475    0.00000   100%  50.4 2380s\n",
      " 456161 58560    0.00000  196  792 1170.95475    0.00000   100%  50.4 2385s\n",
      " 457712 58955    0.00000  222  810 1170.95475    0.00000   100%  50.4 2390s\n",
      " 458880 59308    4.66516  130 1037 1170.95475    0.00000   100%  50.4 2396s\n",
      " 459551 59542    0.14579  157  884 1170.95475    0.00000   100%  50.5 2401s\n",
      " 460705 59868   11.66290  157 1361 1170.95475    0.00000   100%  50.6 2407s\n",
      " 461162 59916    2.33258   44 1421 1170.95475    0.00000   100%  50.6 2410s\n",
      " 462014 60126 infeasible   70      1170.95475    0.00000   100%  50.6 2472s\n",
      " 462088 60136 infeasible   62      1170.95475    0.00000   100%  50.7 2475s\n",
      " 462293 60144    0.00000  100 1319 1170.95475    0.00000   100%  50.7 2481s\n",
      " 463092 60371 infeasible  206      1170.95475    0.00000   100%  50.7 2487s\n",
      " 464051 60715    0.00000   90 1312 1170.95475    0.00000   100%  50.8 2492s\n",
      " 464766 60806    0.00000  120  589 1170.95475    0.00000   100%  50.8 2496s\n",
      " 465762 61034    0.00000  114 1484 1170.95475    0.00000   100%  50.9 2502s\n",
      " 466638 61216 infeasible  110      1170.95475    0.00000   100%  50.9 2505s\n",
      " 467270 61251    4.66516  186  985 1170.95475    0.00000   100%  50.9 2510s\n",
      " 469125 61612 infeasible  139      1170.95475    0.00000   100%  50.9 2515s\n",
      " 470592 61966 infeasible   84      1170.95475    0.00000   100%  51.0 2522s\n",
      " 471747 62202    0.06085  155 1283 1170.95475    0.00000   100%  51.1 2526s\n",
      " 473094 62577    2.33258  184 1064 1170.95475    0.00000   100%  51.1 2531s\n",
      " 474140 62803    2.33258  153 1197 1170.95475    0.00000   100%  51.2 2536s\n",
      " 475407 63151    4.66516  127 1157 1170.95475    0.00000   100%  51.2 2540s\n",
      " 477769 63641    2.33258  190  934 1170.95475    0.00000   100%  51.1 2546s\n",
      " 478947 63743 infeasible   73      1170.95475    0.00000   100%  51.2 2551s\n",
      " 479924 63802 infeasible   51      1170.95475    0.00000   100%  51.2 2555s\n",
      " 480678 63947    2.33258   56 1734 1170.95475    0.00000   100%  51.3 2560s\n",
      " 481780 64250 infeasible  109      1170.95475    0.00000   100%  51.3 2566s\n",
      " 483604 64694 infeasible   38      1170.95475    0.00000   100%  51.3 2571s\n",
      " 484904 64912    0.38876  134  649 1170.95475    0.00000   100%  51.3 2575s\n",
      " 486973 65195    9.33032   78 1333 1170.95475    0.00000   100%  51.2 2581s\n",
      " 487422 65306    2.33258  122 1654 1170.95475    0.00000   100%  51.3 2585s\n",
      " 488773 65712    2.33258   43 1660 1170.95475    0.00000   100%  51.4 2590s\n",
      " 490369 66215 infeasible  110      1170.95475    0.00000   100%  51.4 2595s\n",
      " 491827 66744    0.32269   40 1807 1170.95475    0.00000   100%  51.4 2600s\n",
      " 492486 66904 infeasible   40      1170.95475    0.00000   100%  51.4 2605s\n",
      " 493439 67231    0.00000  116 1594 1170.95475    0.00000   100%  51.5 2611s\n",
      " 494275 67455    0.00000  210 1401 1170.95475    0.00000   100%  51.5 2616s\n",
      " 494995 67617    0.00000  130 1053 1170.95475    0.00000   100%  51.5 2622s\n",
      " 495284 67665    0.00000  155 1264 1170.95475    0.00000   100%  51.5 2625s\n",
      " 496874 68531    0.00000  127  672 1170.95475    0.00000   100%  51.4 2631s\n",
      " 498569 68887 infeasible  169      1170.95475    0.00000   100%  51.3 2636s\n",
      " 499825 69085 infeasible  181      1170.95475    0.00000   100%  51.2 2640s\n",
      " 501496 68784 infeasible  117      1170.95475    0.00000   100%  51.1 2645s\n",
      " 502714 68954 infeasible  141      1170.95475    0.00000   100%  51.1 2650s\n",
      " 504728 69194   10.88537  166  720 1170.95475    0.00000   100%  51.0 2655s\n",
      " 505956 69150 infeasible  203      1170.95475    0.00000   100%  50.9 2660s\n",
      " 507025 69632    4.66516  106 1403 1170.95475    0.00000   100%  51.0 2665s\n",
      " 507798 69804    0.00000   81 1650 1170.95475    0.00000   100%  51.0 2670s\n",
      " 508458 69990    4.66516  105 1136 1170.95475    0.00000   100%  51.1 2728s\n",
      " 508734 70014 infeasible  118      1170.95475    0.00000   100%  51.1 2731s\n",
      " 508954 70035 infeasible  201      1170.95475    0.00000   100%  51.1 2735s\n",
      " 510084 70246 infeasible  223      1170.95475    0.00000   100%  51.2 2740s\n",
      " 511814 70684   62.97964  491  818 1170.95475    0.00000   100%  51.2 2746s\n",
      " 513112 70961    0.00000  145 1023 1170.95475    0.00000   100%  51.2 2752s\n",
      " 514180 71244    0.00000   63 1110 1170.95475    0.00000   100%  51.2 2755s\n",
      " 516344 71643 infeasible  170      1170.95475    0.00000   100%  51.1 2761s\n",
      " 517964 71935 infeasible  116      1170.95475    0.00000   100%  51.2 2766s\n",
      " 519590 72163 infeasible  255      1170.95475    0.00000   100%  51.1 2770s\n",
      " 521521 72339 infeasible  243      1170.95475    0.00000   100%  51.0 2776s\n",
      " 523446 72774    0.00000  274  700 1170.95475    0.00000   100%  51.0 2781s\n",
      " 524716 72903    0.00000  178 1478 1170.95475    0.00000   100%  51.0 2786s\n",
      " 526253 73103 infeasible  214      1170.95475    0.00000   100%  51.0 2790s\n",
      " 528148 73519 infeasible  194      1170.95475    0.00000   100%  51.0 2795s\n",
      " 529522 73707    0.00000   52 1146 1170.95475    0.00000   100%  51.0 2800s\n",
      " 530584 73834  108.07617   65 1323 1170.95475    0.00000   100%  51.1 2805s\n",
      " 531310 74030    0.21724  101 1737 1170.95475    0.00000   100%  51.1 2811s\n",
      " 531648 74128 infeasible   90      1170.95475    0.00000   100%  51.2 2815s\n",
      " 532431 74380    0.00000   59 1162 1170.95475    0.00000   100%  51.2 2821s\n",
      " 532963 74539 infeasible   35      1170.95475    0.00000   100%  51.2 2825s\n",
      " 534216 74847 infeasible  138      1170.95475    0.00000   100%  51.3 2830s\n",
      " 535478 75040    0.00000   72 1024 1170.95475    0.00000   100%  51.4 2836s\n",
      " 536872 75273    0.00000   89  925 1170.95475    0.00000   100%  51.4 2841s\n",
      " 538163 75477    0.00000  168  741 1170.95475    0.00000   100%  51.4 2846s\n",
      " 539919 75627    0.00000  171  648 1170.95475    0.00000   100%  51.4 2851s\n",
      " 541399 75704 infeasible  219      1170.95475    0.00000   100%  51.3 2855s\n",
      " 543076 75664 infeasible   80      1170.95475    0.00000   100%  51.3 2860s\n",
      " 544466 75814 infeasible   41      1170.95475    0.00000   100%  51.4 2865s\n",
      " 546392 75995    0.00000  182  571 1170.95475    0.00000   100%  51.3 2871s\n",
      " 547457 76157 infeasible  118      1170.95475    0.00000   100%  51.4 2876s\n",
      " 548457 76327 infeasible   40      1170.95475    0.00000   100%  51.4 2880s\n",
      " 550042 76512 infeasible  120      1170.95475    0.00000   100%  51.5 2886s\n",
      " 550828 76684    0.00000   74 1153 1170.95475    0.00000   100%  51.5 2890s\n",
      " 552376 77137    0.00000   38 1154 1170.95475    0.00000   100%  51.6 2896s\n",
      " 554193 77675    2.33258  107  959 1170.95475    0.00000   100%  51.6 2901s\n",
      " 555849 78083   30.32353  166  707 1170.95475    0.00000   100%  51.6 2906s\n",
      " 557631 78682    2.33258  141 1007 1170.95475    0.00000   100%  51.5 2910s\n",
      " 559021 79114    9.51625   98 1464 1170.95475    0.00000   100%  51.5 2953s\n",
      " 559225 79193    9.51625  114 1445 1170.95475    0.00000   100%  51.5 2955s\n",
      " 560356 79652    4.66516   77 1595 1170.95475    0.00000   100%  51.6 2960s\n",
      " 560944 79721 infeasible  119      1170.95475    0.00000   100%  51.7 2965s\n",
      " 562132 79971    0.11202   58 1817 1170.95475    0.00000   100%  51.7 2970s\n",
      " 563405 80312    0.00000   40 1710 1170.95475    0.00000   100%  51.8 2976s\n",
      " 563974 80471    2.44650   85 1869 1170.95475    0.00000   100%  51.9 2981s\n",
      " 565033 80652    9.33032   70 1594 1170.95475    0.00000   100%  52.0 2986s\n",
      " 566072 80818    2.33258   82 1307 1170.95475    0.00000   100%  52.1 2991s\n",
      " 567064 80956 infeasible   71      1170.95475    0.00000   100%  52.1 2995s\n",
      " 568826 81016 infeasible  173      1170.95475    0.00000   100%  52.1 3001s\n",
      " 570566 81320    0.00000  153  731 1170.95475    0.00000   100%  52.1 3005s\n",
      " 571884 81348   28.42831  142 1664 1170.95475    0.00000   100%  52.1 3011s\n",
      " 572773 81531 infeasible  137      1170.95475    0.00000   100%  52.2 3015s\n",
      " 573845 81948    2.33258   46 1740 1170.95475    0.00000   100%  52.2 3020s\n",
      " 574946 82209 infeasible  105      1170.95475    0.00000   100%  52.3 3026s\n",
      " 575999 82374    0.00000  131 1259 1170.95475    0.00000   100%  52.4 3030s\n",
      " 577027 82439    0.64973   92 1652 1170.95475    0.00000   100%  52.4 3036s\n",
      " 577612 82596    2.40547   48 1352 1170.95475    0.00000   100%  52.4 3040s\n",
      " 578747 82875   27.99095   93 1140 1170.95475    0.00000   100%  52.5 3045s\n",
      " 579911 83108   11.66290  101 1056 1170.95475    0.00000   100%  52.6 3050s\n",
      " 580624 83295    2.33258   70 1480 1170.95475    0.00000   100%  52.6 3055s\n",
      " 581546 83478   20.99321   90 1147 1170.95475    0.00000   100%  52.7 3060s\n",
      " 582215 83558   21.02910   75 1151 1170.95475    0.00000   100%  52.8 3066s\n",
      " 582902 83674 infeasible   46      1170.95475    0.00000   100%  52.8 3070s\n",
      " 583132 83715   32.65611   83 1262 1170.95475    0.00000   100%  52.9 3102s\n",
      " 583459 83761   13.99548  136  970 1170.95475    0.00000   100%  52.9 3106s\n",
      " 584836 83951    0.00000  158 1499 1170.95475    0.00000   100%  53.0 3111s\n",
      " 585525 84026    0.00000  160 1383 1170.95475    0.00000   100%  53.0 3117s\n",
      " 586838 84256    9.34588   89 1282 1170.95475    0.00000   100%  53.0 3120s\n",
      " 587722 84114    4.88520   93 1731 1170.95475    0.00000   100%  53.1 3126s\n",
      " 588326 84326    0.03534   52 1420 1170.95475    0.00000   100%  53.1 3130s\n",
      " 589367 84683    0.00000   30 1665 1170.95475    0.00000   100%  53.2 3137s\n",
      " 589755 84797    0.00000   39 1605 1170.95475    0.00000   100%  53.3 3140s\n",
      " 590539 85013    2.33258   59 1684 1170.95475    0.00000   100%  53.3 3147s\n",
      " 591407 85241 infeasible  103      1170.95475    0.00000   100%  53.4 3151s\n",
      " 592028 85391    2.33258   70 1642 1170.95475    0.00000   100%  53.5 3156s\n",
      " 592949 85765 infeasible   37      1170.95475    0.00000   100%  53.5 3160s\n",
      " 593927 85895    2.33258  140 1194 1170.95475    0.00000   100%  53.6 3166s\n",
      " 594529 86058    4.66516   84 1787 1170.95475    0.00000   100%  53.6 3170s\n",
      " 595578 86431 infeasible   43      1170.95475    0.00000   100%  53.7 3178s\n",
      " 595869 86514    2.33258   32 1727 1170.95475    0.00000   100%  53.7 3219s\n",
      " 595873 86507    2.33258   33 1765 1170.95475    0.00000   100%  53.7 3222s\n",
      " 595960 86505    4.66516   29 1405 1170.95475    0.00000   100%  53.8 3225s\n",
      " 596715 86677 infeasible   54      1170.95475    0.00000   100%  53.8 3230s\n",
      " 597726 86825 infeasible  213      1170.95475    0.00000   100%  53.8 3235s\n",
      " 598332 86857 infeasible   38      1170.95475    0.00000   100%  53.9 3240s\n",
      " 599088 87059 infeasible  218      1170.95475    0.00000   100%  54.0 3245s\n",
      " 600286 87399    2.33258   64 1379 1170.95475    0.00000   100%  54.0 3250s\n",
      " 601203 87653    2.33258   24 1522 1170.95475    0.00000   100%  54.1 3255s\n",
      " 601821 87754 infeasible   32      1170.95475    0.00000   100%  54.1 3260s\n",
      " 602828 88076    7.21642   47 1406 1170.95475    0.00000   100%  54.2 3266s\n",
      " 604270 88367 infeasible   52      1170.95475    0.00000   100%  54.2 3270s\n",
      " 605180 88632   11.66290   57 1624 1170.95475    0.00000   100%  54.3 3275s\n",
      " 606039 88796    0.00000   86 1118 1170.95475    0.00000   100%  54.3 3280s\n",
      " 607181 89053    2.33258   31 1528 1170.95475    0.00000   100%  54.4 3287s\n",
      " 607665 89193 infeasible   53      1170.95475    0.00000   100%  54.4 3290s\n",
      " 608328 89390 infeasible  206      1170.95475    0.00000   100%  54.5 3295s\n",
      " 609856 89817    2.33258   26 1692 1170.95475    0.00000   100%  54.5 3350s\n",
      " 610119 89842    0.88411  176 1048 1170.95475    0.00000   100%  54.5 3357s\n",
      " 611177 90057    2.33258   35 1014 1170.95475    0.00000   100%  54.5 3361s\n",
      " 612205 90196 infeasible  246      1170.95475    0.00000   100%  54.5 3366s\n",
      " 613499 90557 infeasible   40      1170.95475    0.00000   100%  54.5 3371s\n",
      " 614524 90696    2.38620   29 1389 1170.95475    0.00000   100%  54.6 3375s\n",
      " 616389 91107    0.00000  232  483 1170.95475    0.00000   100%  54.5 3381s\n",
      " 618029 91394    0.00000  181  599 1170.95475    0.00000   100%  54.4 3385s\n",
      " 619484 91423    0.00000  234  332 1170.95475    0.00000   100%  54.4 3391s\n",
      " 621863 91946 infeasible  252      1170.95475    0.00000   100%  54.3 3395s\n",
      " 623117 92113    2.76994   54 1346 1170.95475    0.00000   100%  54.3 3401s\n",
      " 623965 92441 infeasible  158      1170.95475    0.00000   100%  54.3 3406s\n",
      " 624968 92669    0.00000  101 1528 1170.95475    0.00000   100%  54.4 3411s\n",
      " 625893 92635 infeasible   74      1170.95475    0.00000   100%  54.4 3416s\n",
      " 626162 92640    2.33258   99 1230 1170.95475    0.00000   100%  54.4 3420s\n",
      " 626903 92871    0.00000   50  941 1170.95475    0.00000   100%  54.4 3426s\n",
      " 627523 93085    0.00000  214  355 1170.95475    0.00000   100%  54.4 3430s\n",
      " 628020 93111    0.00000   74  689 1170.95475    0.00000   100%  54.5 3435s\n",
      " 628809 93092    0.00000  234  295 1170.95475    0.00000   100%  54.5 3440s\n",
      " 629797 93109 infeasible  238      1170.95475    0.00000   100%  54.5 3446s\n",
      " 630744 93175 infeasible   65      1170.95475    0.00000   100%  54.4 3450s\n",
      " 631464 93335   27.83691   85 1018 1170.95475    0.00000   100%  54.5 3455s\n",
      " 632140 93434    5.04314   76 1623 1170.95475    0.00000   100%  54.6 3461s\n",
      " 632617 93473    5.23063  156 1493 1170.95475    0.00000   100%  54.6 3465s\n",
      " 633805 93919    9.33032  145 1447 1170.95475    0.00000   100%  54.7 3471s\n",
      " 634547 94107    0.00000  135 1194 1170.95475    0.00000   100%  54.7 3476s\n",
      " 635835 94432 infeasible  280      1170.95475    0.00000   100%  54.7 3481s\n",
      " 637153 94510 infeasible  185      1170.95475    0.00000   100%  54.8 3485s\n",
      " 638516 94492    2.33258  153 1293 1170.95475    0.00000   100%  54.8 3490s\n",
      " 639729 94467    0.00000  282  591 1170.95475    0.00000   100%  54.8 3495s\n",
      " 640408 94519    0.00000   38 1404 1170.95475    0.00000   100%  54.9 3500s\n",
      " 641381 94799    0.00000   39 1062 1170.95475    0.00000   100%  54.9 3507s\n",
      " 642539 95232 infeasible   81      1170.95475    0.00000   100%  55.0 3510s\n",
      " 643450 95278 infeasible   91      1170.95475    0.00000   100%  55.0 3516s\n",
      " 644461 95426    3.49887   58 1263 1170.95475    0.00000   100%  55.1 3521s\n",
      " 645232 95663    7.77356   87 1788 1170.95475    0.00000   100%  55.1 3526s\n",
      " 646397 96090    7.58088   35 1403 1170.95475    0.00000   100%  55.1 3530s\n",
      " 647759 96547    2.33258  256  846 1170.95475    0.00000   100%  55.1 3568s\n",
      " 648200 96715    0.00000  209  801 1170.95475    0.00000   100%  55.1 3570s\n",
      " 648777 96739    0.00000  413  599 1170.95475    0.00000   100%  55.1 3575s\n",
      " 651357 97152 infeasible  142      1170.95475    0.00000   100%  55.0 3581s\n",
      " 653093 97217 infeasible  206      1170.95475    0.00000   100%  54.9 3585s\n",
      " 654769 97477 infeasible   52      1170.95475    0.00000   100%  54.9 3590s\n",
      " 655756 97660    2.33258   48 1414 1170.95475    0.00000   100%  54.9 3595s\n",
      " 657132 98018    2.33258  128  933 1170.95475    0.00000   100%  54.9 3600s\n",
      " 659060 98243    0.00000  154  691 1170.95475    0.00000   100%  54.8 3605s\n",
      " 661130 98358    9.33032  137  866 1170.95475    0.00000   100%  54.8 3611s\n",
      " 662646 98413 infeasible   83      1170.95475    0.00000   100%  54.7 3616s\n",
      " 663686 98748    0.00000  149 1265 1170.95475    0.00000   100%  54.7 3620s\n",
      " 665238 99397    0.00000   72 1474 1170.95475    0.00000   100%  54.7 3625s\n",
      " 666830 100253 infeasible   92      1170.95475    0.00000   100%  54.7 3630s\n",
      " 668631 100591 infeasible  112      1170.95475    0.00000   100%  54.6 3635s\n",
      " 670534 100988 infeasible  247      1170.95475    0.00000   100%  54.5 3640s\n",
      " 671507 100937    0.10291   68 1442 1170.95475    0.00000   100%  54.5 3646s\n",
      " 672780 101537    0.00000   91  892 1170.95475    0.00000   100%  54.5 3651s\n",
      " 674182 102099    2.47837  119  730 1170.95475    0.00000   100%  54.5 3655s\n",
      " 676166 102651 infeasible  238      1170.95475    0.00000   100%  54.4 3660s\n",
      " 677196 102963   13.71779   73 1542 1170.95475    0.00000   100%  54.4 3665s\n",
      " 678444 103309 infeasible  123      1170.95475    0.00000   100%  54.5 3670s\n",
      " 679632 103513 infeasible   74      1170.95475    0.00000   100%  54.5 3675s\n",
      " 680391 103539    0.00000  115 1037 1170.95475    0.00000   100%  54.5 3726s\n",
      " 680608 103581    2.33258  132 1520 1170.95475    0.00000   100%  54.5 3730s\n",
      " 681953 103876    0.00000  140  121 1170.95475    0.00000   100%  54.5 3735s\n",
      " 683384 103946 infeasible   72      1170.95475    0.00000   100%  54.5 3740s\n",
      " 684509 104023    0.00000  112  166 1170.95475    0.00000   100%  54.5 3745s\n",
      " 685411 104270  696.72870  146 1222 1170.95475    0.00000   100%  54.5 3750s\n",
      " 686384 104361   20.99321   85  168 1170.95475    0.00000   100%  54.5 3755s\n",
      " 687783 104576 infeasible  167      1170.95475    0.00000   100%  54.4 3760s\n",
      " 689469 104720 infeasible  180      1170.95475    0.00000   100%  54.4 3766s\n",
      " 690448 104886    0.00000  224  873 1170.95475    0.00000   100%  54.4 3770s\n",
      " 692180 105380    6.99774  160 1613 1170.95475    0.00000   100%  54.3 3776s\n",
      " 692984 105488    0.00000   72 1412 1170.95475    0.00000   100%  54.3 3780s\n",
      " 694064 105785    0.00000  122 1349 1170.95475    0.00000   100%  54.4 3785s\n",
      " 695229 105961 infeasible  205      1170.95475    0.00000   100%  54.4 3790s\n",
      " 696685 106385    2.33258   69 1410 1170.95475    0.00000   100%  54.4 3796s\n",
      " 697442 106446    0.00000   83 1678 1170.95475    0.00000   100%  54.4 3800s\n",
      " 698340 106546    0.00000  129 1468 1170.95475    0.00000   100%  54.4 3806s\n",
      " 698435 106509    0.00000  136 1442 1170.95475    0.00000   100%  54.5 3813s\n",
      " 698567 106497 infeasible  177      1170.95475    0.00000   100%  54.5 3815s\n",
      " 699085 106511 infeasible  186      1170.95475    0.00000   100%  54.5 3852s\n",
      " 699413 106461 infeasible  226      1170.95475    0.00000   100%  54.5 3855s\n",
      " 700552 106511 infeasible  193      1170.95475    0.00000   100%  54.5 3860s\n",
      " 701246 106432    0.00000  288  274 1170.95475    0.00000   100%  54.5 3866s\n",
      " 702423 106726    0.21248  117 1427 1170.95475    0.00000   100%  54.5 3870s\n",
      " 703940 106905   16.32805  333  179 1170.95475    0.00000   100%  54.6 3876s\n",
      " 705065 107060    9.33032  107 1590 1170.95475    0.00000   100%  54.6 3881s\n",
      " 705781 107279    2.33258   27 1798 1170.95475    0.00000   100%  54.6 3885s\n",
      " 706378 107463    2.33258  118 1696 1170.95475    0.00000   100%  54.6 3890s\n",
      " 707637 107867 infeasible  147      1170.95475    0.00000   100%  54.7 3897s\n",
      " 708505 108089    0.10603  103 1369 1170.95475    0.00000   100%  54.7 3901s\n",
      " 709414 108255   83.97285  108 1381 1170.95475    0.00000   100%  54.7 3905s\n",
      " 710489 108239 infeasible   75      1170.95475    0.00000   100%  54.7 3911s\n",
      " 711314 108422 infeasible   67      1170.95475    0.00000   100%  54.7 3916s\n",
      " 711818 108608    0.00000   44 1631 1170.95475    0.00000   100%  54.7 3948s\n",
      " 712128 108713 infeasible  156      1170.95475    0.00000   100%  54.7 3951s\n",
      " 712795 108796 infeasible  102      1170.95475    0.00000   100%  54.7 3955s\n",
      " 714134 109246 infeasible  151      1170.95475    0.00000   100%  54.7 3960s\n",
      " 715342 109309 infeasible  138      1170.95475    0.00000   100%  54.7 3965s\n",
      " 716333 109579    0.00000   53 1606 1170.95475    0.00000   100%  54.7 3972s\n",
      " 717943 110203 infeasible  277      1170.95475    0.00000   100%  54.7 3976s\n",
      " 719478 110668 infeasible  145      1170.95475    0.00000   100%  54.7 3981s\n",
      " 721030 111150    6.99774  116 1408 1170.95475    0.00000   100%  54.7 3986s\n",
      " 721740 111374    0.38876   58 1218 1170.95475    0.00000   100%  54.7 3990s\n",
      " 723217 111477     cutoff  298      1170.95475    0.00000   100%  54.7 3995s\n",
      " 724302 111760    2.33258   30 1928 1170.95475    0.00000   100%  54.7 4000s\n",
      " 725579 112093    0.00000   36 1909 1170.95475    0.00000   100%  54.8 4006s\n",
      " 726882 112582 infeasible  148      1170.95475    0.00000   100%  54.7 4010s\n",
      " 728526 112921 infeasible   96      1170.95475    0.00000   100%  54.7 4015s\n",
      " 729992 113059    4.66516  104  886 1170.95475    0.00000   100%  54.7 4020s\n",
      " 731497 113375 infeasible  175      1170.95475    0.00000   100%  54.7 4025s\n",
      " 732928 113611    0.00000  115 1636 1170.95475    0.00000   100%  54.7 4030s\n",
      " 733930 114013    0.00000   44 1005 1170.95475    0.00000   100%  54.7 4035s\n",
      " 734722 114234    2.33258  164 1437 1170.95475    0.00000   100%  54.7 4090s\n",
      " 735018 114271    4.66516  190 1522 1170.95475    0.00000   100%  54.7 4095s\n",
      " 735595 114332    2.33258  147 1234 1170.95475    0.00000   100%  54.7 4100s\n",
      " 736551 114563    6.99774   81 1883 1170.95475    0.00000   100%  54.8 4105s\n",
      " 737238 114785   11.66290   78 1662 1170.95475    0.00000   100%  54.8 4110s\n",
      " 737965 114909   14.33304   97 1549 1170.95475    0.00000   100%  54.9 4116s\n",
      " 738966 115027    0.04665  116 1468 1170.95475    0.00000   100%  54.9 4121s\n",
      " 739910 115217 infeasible   76      1170.95475    0.00000   100%  54.9 4127s\n",
      " 740641 115408    2.33258  165  793 1170.95475    0.00000   100%  54.9 4131s\n",
      " 741389 115621   13.99548  112  695 1170.95475    0.00000   100%  55.0 4135s\n",
      " 742246 115699    2.33258   71  815 1170.95475    0.00000   100%  55.0 4141s\n",
      " 743194 115807    3.11011   86  707 1170.95475    0.00000   100%  55.0 4145s\n",
      " 744303 115914 infeasible  124      1170.95475    0.00000   100%  55.0 4150s\n",
      " 745720 115925 infeasible  157      1170.95475    0.00000   100%  54.9 4156s\n",
      " 746914 116058 infeasible  177      1170.95475    0.00000   100%  54.9 4160s\n",
      " 748155 116129    2.33258  175  590 1170.95475    0.00000   100%  54.9 4165s\n",
      " 749366 116278    0.00000  130  583 1170.95475    0.00000   100%  54.8 4171s\n",
      " 750453 116211    0.00000   85  635 1170.95475    0.00000   100%  54.8 4175s\n",
      " 751795 116168 infeasible  118      1170.95475    0.00000   100%  54.8 4181s\n",
      " 752520 116174 infeasible  140      1170.95475    0.00000   100%  54.8 4185s\n",
      " 753713 116183 infeasible   62      1170.95475    0.00000   100%  54.8 4190s\n",
      " 755184 116302   25.00896  232  646 1170.95475    0.00000   100%  54.8 4196s\n",
      " 756531 116715    0.84821  179  620 1170.95475    0.00000   100%  54.7 4200s\n",
      " 758579 116735    2.33258   97  589 1170.95475    0.00000   100%  54.6 4205s\n",
      " 759913 116921 infeasible   90      1170.95475    0.00000   100%  54.6 4210s\n",
      " 761209 116980 infeasible  134      1170.95475    0.00000   100%  54.6 4215s\n",
      " 763106 117028 infeasible  180      1170.95475    0.00000   100%  54.5 4221s\n",
      " 764681 117162 infeasible  401      1170.95475    0.00000   100%  54.4 4225s\n",
      " 766348 117150 infeasible  141      1170.95475    0.00000   100%  54.4 4231s\n",
      " 767481 117204   16.54011  161  460 1170.95475    0.00000   100%  54.4 4235s\n",
      " 768945 117317    0.00000   60  784 1170.95475    0.00000   100%  54.4 4241s\n",
      " 770606 117422    0.00000  114  718 1170.95475    0.00000   100%  54.3 4246s\n",
      " 771933 117454 infeasible   91      1170.95475    0.00000   100%  54.3 4251s\n",
      " 773241 117543 infeasible  135      1170.95475    0.00000   100%  54.3 4256s\n",
      " 774314 117697    4.66516   66 1858 1170.95475    0.00000   100%  54.3 4260s\n",
      " 774793 117903   62.53273  221 1718 1170.95475    0.00000   100%  54.4 4265s\n",
      " 775337 118146 infeasible  119      1170.95475    0.00000   100%  54.4 4270s\n",
      " 775908 118358 infeasible   73      1170.95475    0.00000   100%  54.5 4275s\n",
      " 776341 118457 infeasible  109      1170.95475    0.00000   100%  54.5 4280s\n",
      " 777214 118696   34.98869  163 1616 1170.95475    0.00000   100%  54.6 4286s\n",
      " 778062 118969    0.38876   59 1855 1170.95475    0.00000   100%  54.6 4290s\n",
      " 778744 119121    1.41182   41 1809 1170.95475    0.00000   100%  54.6 4295s\n",
      " 780906 119647    0.10603  135 1280 1170.95475    0.00000   100%  54.6 4301s\n",
      " 782598 119710    0.00000  329  318 1170.95475    0.00000   100%  54.5 4305s\n",
      " 784373 119869 infeasible  137      1170.95475    0.00000   100%  54.5 4311s\n",
      " 785328 120130    2.33258  148 1026 1170.95475    0.00000   100%  54.5 4353s\n",
      " 785404 120165    2.33258  149 1024 1170.95475    0.00000   100%  54.5 4355s\n",
      " 786086 120339    0.00000  180 1624 1170.95475    0.00000   100%  54.5 4360s\n",
      " 786936 120324 infeasible  148      1170.95475    0.00000   100%  54.6 4365s\n",
      " 788026 120624    2.33258   64 1440 1170.95475    0.00000   100%  54.6 4370s\n",
      " 788906 120733    2.33258   47 1548 1170.95475    0.00000   100%  54.6 4375s\n",
      " 789885 120970 infeasible  101      1170.95475    0.00000   100%  54.6 4380s\n",
      " 790653 121048    6.99774   91 1690 1170.95475    0.00000   100%  54.7 4386s\n",
      " 791289 121167    0.00000   48 1333 1170.95475    0.00000   100%  54.7 4391s\n",
      " 791775 121310    0.00000  172  981 1170.95475    0.00000   100%  54.8 4395s\n",
      " 792637 121513   16.40346   88 1813 1170.95475    0.00000   100%  54.8 4400s\n",
      " 793577 121626    0.00000   76  993 1170.95475    0.00000   100%  54.9 4406s\n",
      " 794271 121790 infeasible   55      1170.95475    0.00000   100%  54.9 4411s\n",
      " 794809 121920    0.00000   47 1165 1170.95475    0.00000   100%  54.9 4415s\n",
      " 795339 121979    0.38876   67 1443 1170.95475    0.00000   100%  55.0 4420s\n",
      " 796465 122176 infeasible  153      1170.95475    0.00000   100%  55.0 4427s\n",
      " 796830 122241    2.33258   63  883 1170.95475    0.00000   100%  55.1 4430s\n",
      " 798007 122576 infeasible  100      1170.95475    0.00000   100%  55.1 4436s\n",
      " 798723 122589 infeasible  114      1170.95475    0.00000   100%  55.1 4440s\n",
      " 799538 122537 infeasible   82      1170.95475    0.00000   100%  55.1 4445s\n",
      " 800376 122522 infeasible   88      1170.95475    0.00000   100%  55.2 4451s\n",
      " 801361 122621 infeasible  189      1170.95475    0.00000   100%  55.2 4456s\n",
      " 802166 122706   17.09836   68 1888 1170.95475    0.00000   100%  55.2 4460s\n",
      " 803162 122824 infeasible  160      1170.95475    0.00000   100%  55.2 4465s\n",
      " 804733 123050    9.33032  143 1212 1170.95475    0.00000   100%  55.3 4472s\n",
      " 806267 123279 infeasible  157      1170.95475    0.00000   100%  55.2 4476s\n",
      " 807804 123633    3.56391   54 1529 1170.95475    0.00000   100%  55.2 4480s\n",
      " 809748 123903 infeasible   87      1170.95475    0.00000   100%  55.1 4485s\n",
      " 811911 124413 infeasible  206      1170.95475    0.00000   100%  55.0 4490s\n",
      " 814302 124547 infeasible  237      1170.95475    0.00000   100%  54.8 4495s\n",
      " 815797 124523 infeasible  193      1170.95475    0.00000   100%  54.8 4500s\n",
      " 816199 124448 infeasible  141      1170.95475    0.00000   100%  54.7 4560s\n",
      " 816923 124686   16.76541   51 1119 1170.95475    0.00000   100%  54.7 4565s\n",
      " 818048 125096    6.99774  111 1845 1170.95475    0.00000   100%  54.7 4570s\n",
      " 818635 125289    0.00000   83 1586 1170.95475    0.00000   100%  54.8 4575s\n",
      " 819688 125719    4.66516  138 1347 1170.95475    0.00000   100%  54.8 4581s\n",
      " 820341 126046    2.33258   46 1669 1170.95475    0.00000   100%  54.8 4585s\n",
      " 820779 126202    9.33032  150 1464 1170.95475    0.00000   100%  54.8 4590s\n",
      " 821225 126346 infeasible   59      1170.95475    0.00000   100%  54.8 4595s\n",
      " 821755 126462   30.90599  156 1364 1170.95475    0.00000   100%  54.9 4600s\n",
      " 822323 126607    0.53522   54 1775 1170.95475    0.00000   100%  54.9 4605s\n",
      " 822996 126833   16.32805   91 1609 1170.95475    0.00000   100%  54.9 4610s\n",
      " 823740 126916    0.00000  222 1209 1170.95475    0.00000   100%  55.0 4615s\n",
      " 825051 127151 infeasible  145      1170.95475    0.00000   100%  54.9 4621s\n",
      " 826011 127135 infeasible  240      1170.95475    0.00000   100%  54.9 4625s\n",
      " 826704 127099 infeasible   45      1170.95475    0.00000   100%  54.9 4631s\n",
      " 827152 127249    4.66516  139 1520 1170.95475    0.00000   100%  54.9 4675s\n",
      " 827471 127356    4.66516  159 1507 1170.95475    0.00000   100%  55.0 4736s\n",
      " 827662 127370    4.66516  181 1495 1170.95475    0.00000   100%  54.9 4740s\n",
      " 829069 127530 infeasible  139      1170.95475    0.00000   100%  54.9 4746s\n",
      " 830292 127822 infeasible  142      1170.95475    0.00000   100%  54.9 4750s\n",
      " 831722 127963 infeasible  237      1170.95475    0.00000   100%  54.9 4755s\n",
      " 832864 127902    0.00000  204  812 1170.95475    0.00000   100%  54.8 4760s\n",
      " 833811 127816 infeasible  213      1170.95475    0.00000   100%  54.8 4765s\n",
      " 835039 127905   19.43816  139  952 1170.95475    0.00000   100%  54.8 4770s\n",
      " 836257 127941 infeasible  221      1170.95475    0.00000   100%  54.8 4775s\n",
      " 837082 127942    6.99774  135  887 1170.95475    0.00000   100%  54.8 4780s\n",
      " 837667 128069    2.33258   43 1593 1170.95475    0.00000   100%  54.8 4786s\n",
      " 838378 128206    0.00000  140  969 1170.95475    0.00000   100%  54.8 4791s\n",
      " 838903 128306    7.42184   78 1652 1170.95475    0.00000   100%  54.9 4795s\n",
      " 839627 128350    0.00000  142  836 1170.95475    0.00000   100%  54.9 4800s\n",
      " 842119 128772    0.00000  153  892 1170.95475    0.00000   100%  54.8 4805s\n",
      " 842818 128860    6.99774   70 1731 1170.95475    0.00000   100%  54.8 4811s\n",
      " 844184 128921    4.66516  147  895 1170.95475    0.00000   100%  54.8 4815s\n",
      " 845231 128867    2.33258   55 1581 1170.95475    0.00000   100%  54.8 4820s\n",
      " 846735 129240    0.14137  144 1424 1170.95475    0.00000   100%  54.8 4826s\n",
      " 847676 129394    0.00000  101 1521 1170.95475    0.00000   100%  54.8 4830s\n",
      " 849609 129688   18.66063  166  758 1170.95475    0.00000   100%  54.7 4835s\n",
      " 850847 129792    0.00000  128 1592 1170.95475    0.00000   100%  54.7 4841s\n",
      " 852403 129855    0.00000  175  826 1170.95475    0.00000   100%  54.6 4846s\n",
      " 853372 129873 infeasible  217      1170.95475    0.00000   100%  54.6 4875s\n",
      " 854402 130028    2.33258  159 1579 1170.95475    0.00000   100%  54.6 4881s\n",
      " 855884 130313   19.26424   66 1700 1170.95475    0.00000   100%  54.6 4885s\n",
      " 857378 130392 infeasible   31      1170.95475    0.00000   100%  54.5 4890s\n",
      " 858873 130628    0.00000   88  907 1170.95475    0.00000   100%  54.6 4896s\n",
      " 860257 130791  862.48882  322  249 1170.95475    0.00000   100%  54.5 4900s\n",
      " 861810 131136    0.00000  123  974 1170.95475    0.00000   100%  54.5 4905s\n",
      " 863724 131496 infeasible  128      1170.95475    0.00000   100%  54.4 4910s\n",
      " 865710 131950    0.00000  136  790 1170.95475    0.00000   100%  54.4 4915s\n",
      " 867802 132523   48.98416  316  316 1170.95475    0.00000   100%  54.3 4921s\n",
      " 869331 132773    0.00000  197  523 1170.95475    0.00000   100%  54.3 4925s\n",
      " 869836 132883    0.00000  156  666 1170.95475    0.00000   100%  54.2 5017s\n",
      " 870079 132928    4.66516  201  536 1170.95475    0.00000   100%  54.2 5021s\n",
      " 870589 132982    6.99774  254  337 1170.95475    0.00000   100%  54.2 5025s\n",
      " 872783 133402    0.00000  156  702 1170.95475    0.00000   100%  54.1 5030s\n",
      " 874731 133619 infeasible  162      1170.95475    0.00000   100%  54.1 5035s\n",
      " 875151 133771    6.99774   80 1556 1170.95475    0.00000   100%  54.2 5040s\n",
      " 875871 134044    2.33258  146 1389 1170.95475    0.00000   100%  54.2 5045s\n",
      " 876709 134281 infeasible   48      1170.95475    0.00000   100%  54.2 5050s\n",
      " 877352 134481    2.33258   81 1133 1170.95475    0.00000   100%  54.3 5055s\n",
      " 878524 134801    5.24830   99 1404 1170.95475    0.00000   100%  54.3 5060s\n",
      " 879213 134878    4.66516  125 1450 1170.95475    0.00000   100%  54.4 5065s\n",
      " 879857 134981    2.33258   81 1080 1170.95475    0.00000   100%  54.4 5070s\n",
      " 880511 135150    2.33258  236 1473 1170.95475    0.00000   100%  54.5 5076s\n",
      " 881724 135442 infeasible   77      1170.95475    0.00000   100%  54.5 5081s\n",
      " 882377 135606 infeasible  114      1170.95475    0.00000   100%  54.5 5086s\n",
      " 883226 135692 infeasible   52      1170.95475    0.00000   100%  54.6 5091s\n",
      " 883931 135814 infeasible   61      1170.95475    0.00000   100%  54.6 5096s\n",
      " 884656 136034    0.00000  124 1459 1170.95475    0.00000   100%  54.7 5100s\n",
      " 885858 136201    2.33258  101 1688 1170.95475    0.00000   100%  54.7 5106s\n",
      " 886831 136294    3.00629  159 1408 1170.95475    0.00000   100%  54.7 5110s\n",
      " 888203 136450    0.00000   58 1564 1170.95475    0.00000   100%  54.8 5116s\n",
      " 889502 136607    0.20582  106 1265 1170.95475    0.00000   100%  54.8 5120s\n",
      " 890938 136649    0.00000   71 1582 1170.95475    0.00000   100%  54.9 5126s\n",
      " 891562 136632    0.00000  140 1320 1170.95475    0.00000   100%  54.9 5130s\n",
      " 892927 136717    0.00000   80 1465 1170.95475    0.00000   100%  54.9 5135s\n",
      " 894844 136933    2.33258  112 1179 1170.95475    0.00000   100%  54.9 5141s\n",
      " 896138 137049    2.33258   65 1031 1170.95475    0.00000   100%  55.0 5146s\n",
      " 896845 137061 infeasible   75      1170.95475    0.00000   100%  55.0 5151s\n",
      " 897905 137256    7.14352  172 1571 1170.95475    0.00000   100%  55.1 5156s\n",
      " 898848 137746    0.00000   47 1418 1170.95475    0.00000   100%  55.1 5161s\n",
      " 899866 138308    2.33258  190 1641 1170.95475    0.00000   100%  55.1 5165s\n",
      " 901050 138841 infeasible  160      1170.95475    0.00000   100%  55.1 5171s\n",
      " 901928 139151    2.33258  196 1705 1170.95475    0.00000   100%  55.1 5175s\n",
      " 903194 139500 infeasible  200      1170.95475    0.00000   100%  55.1 5181s\n",
      " 904348 139842    0.00000   33 1533 1170.95475    0.00000   100%  55.1 5186s\n",
      " 905068 140110    4.66516  162 1411 1170.95475    0.00000   100%  55.2 5190s\n",
      " 905890 140426    2.33258  119 1767 1170.95475    0.00000   100%  55.2 5195s\n",
      " 906765 140647    2.33258  162 1664 1170.95475    0.00000   100%  55.3 5201s\n",
      " 907192 140747    2.33258   90 1105 1170.95475    0.00000   100%  55.3 5205s\n",
      " 908129 141021    4.66516  240 1650 1170.95475    0.00000   100%  55.4 5212s\n",
      " 908899 141236    4.66516  136 1530 1170.95475    0.00000   100%  55.4 5217s\n",
      " 909835 141552    4.66516  125 1414 1170.95475    0.00000   100%  55.4 5222s\n",
      " 910919 141873    4.66516  179 1282 1170.95475    0.00000   100%  55.4 5227s\n",
      " 912130 142304    0.00000  123 1589 1170.95475    0.00000   100%  55.4 5232s\n",
      " 913829 142823 infeasible  127      1170.95475    0.00000   100%  55.4 5236s\n",
      " 914971 142984 infeasible   86      1170.95475    0.00000   100%  55.4 5241s\n",
      " 915284 143070 infeasible   64      1170.95475    0.00000   100%  55.5 5245s\n",
      " 916059 143315 infeasible   92      1170.95475    0.00000   100%  55.5 5251s\n",
      " 916756 143524 infeasible   47      1170.95475    0.00000   100%  55.5 5255s\n",
      " 917563 143775 infeasible  115      1170.95475    0.00000   100%  55.6 5261s\n",
      " 918264 144049    2.33258  184 1723 1170.95475    0.00000   100%  55.6 5266s\n",
      " 918863 144311 infeasible   55      1170.95475    0.00000   100%  55.7 5270s\n",
      " 919252 144529    0.25354   66 1795 1170.95475    0.00000   100%  55.7 5275s\n",
      " 920240 144916    2.33258   67 1277 1170.95475    0.00000   100%  55.8 5282s\n",
      " 920866 145191   13.99548  190 1454 1170.95475    0.00000   100%  55.8 5286s\n",
      " 921401 145434    9.33032  122 1696 1170.95475    0.00000   100%  55.9 5292s\n",
      " 922019 145728    6.99774   64 1692 1170.95475    0.00000   100%  55.9 5296s\n",
      " 922964 146106    2.59972   62 1694 1170.95475    0.00000   100%  56.0 5302s\n",
      " 923620 146405 infeasible   54      1170.95475    0.00000   100%  56.0 5306s\n",
      " 924114 146613    9.33032  105 1650 1170.95475    0.00000   100%  56.0 5311s\n",
      " 924928 146938    2.55874   73 1800 1170.95475    0.00000   100%  56.1 5317s\n",
      " 925275 147028    9.33032  146 1431 1170.95475    0.00000   100%  56.1 5320s\n",
      " 925924 147237   11.66290  100 1789 1170.95475    0.00000   100%  56.2 5325s\n",
      " 926632 147449 infeasible   61      1170.95475    0.00000   100%  56.2 5330s\n",
      " 927260 147568 infeasible   74      1170.95475    0.00000   100%  56.3 5335s\n",
      " 928165 147736    0.00000   87 1001 1170.95475    0.00000   100%  56.3 5341s\n",
      " 928809 147854 infeasible   99      1170.95475    0.00000   100%  56.3 5346s\n",
      " 929621 148041 infeasible  107      1170.95475    0.00000   100%  56.4 5390s\n",
      " 929770 148026 infeasible   89      1170.95475    0.00000   100%  56.4 5395s\n",
      " 930560 148174    4.66516   59 1458 1170.95475    0.00000   100%  56.4 5401s\n",
      " 931533 148387    0.00000  117 1482 1170.95475    0.00000   100%  56.5 5407s\n",
      " 931919 148507    0.00000   55  880 1170.95475    0.00000   100%  56.5 5410s\n",
      " 932842 148700    0.00000  180 1337 1170.95475    0.00000   100%  56.5 5415s\n",
      " 933673 148767    0.00000   60  899 1170.95475    0.00000   100%  56.6 5421s\n",
      " 934336 148866    2.39478   57 1635 1170.95475    0.00000   100%  56.6 5427s\n",
      " 934635 148938    0.00000   48 1010 1170.95475    0.00000   100%  56.7 5430s\n",
      " 935557 149260 infeasible   71      1170.95475    0.00000   100%  56.7 5436s\n",
      " 936464 149647 infeasible   69      1170.95475    0.00000   100%  56.8 5442s\n",
      " 936738 149758    2.33258   79  922 1170.95475    0.00000   100%  56.8 5445s\n",
      " 937645 150056    2.33258   67 1776 1170.95475    0.00000   100%  56.9 5451s\n",
      " 938645 150488    0.08639   56 1856 1170.95475    0.00000   100%  56.9 5457s\n",
      " 939054 150624 infeasible   83      1170.95475    0.00000   100%  56.9 5460s\n",
      "\n",
      "  Learned: 38\n"
     ]
    }
   ],
   "source": [
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Outputs\n",
    "\n",
    "While the verbose output of Gurobi confirms that the optimizer ran successfully, I confirm as much below via a call to JuMP.  As expected, we see that the optimizer ended once it reached its time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIME_LIMIT::TerminationStatusCode = 12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_status(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now pull the various optimized outputs of the function.  $L_t$ gives the loss values at each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [4, 5, 6, 7]\n",
       "And data, a 4-element Array{Float64,1}:\n",
       "  91.0\n",
       " 265.0\n",
       " 135.0\n",
       "  11.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{j,t}$ gives the feature $j$ used to apply a split at node $t$, and $b_t$ gives the split-point value of variable $j$ at node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Array{Float64,2}:\n",
       "  1.0   0.0  -0.0\n",
       " -0.0  -0.0   0.0\n",
       " -0.0  -0.0   0.0\n",
       " -0.0   1.0   1.0\n",
       " -0.0  -0.0  -0.0\n",
       " -0.0  -0.0   0.0\n",
       " -0.0   0.0   0.0\n",
       " -0.0  -0.0   0.0\n",
       " -0.0   0.0  -0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [1, 2, 3]\n",
       "And data, a 3-element Array{Float64,1}:\n",
       " 0.6666666666666664 \n",
       " 0.12499999999999989\n",
       " 0.5624999999999999 "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_t$ gives the total number of $x_i$ input points assigned to leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [4, 5, 6, 7]\n",
       "And data, a 4-element Array{Float64,1}:\n",
       " 228.0\n",
       " 502.0\n",
       " 276.0\n",
       "  25.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we see the values for $N_{k,t}$, which stores the total number of inputs with label $k$ in leaf node $t$, and $c_{k,t}$, which gives the prediction $k$ in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [4, 5, 6, 7]\n",
       "And data, a 3×4 Array{Float64,2}:\n",
       " 137.0  150.0  141.0  14.0\n",
       "  27.0  115.0   80.0   5.0\n",
       "  64.0  237.0   55.0   6.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [4, 5, 6, 7]\n",
       "And data, a 3×4 Array{Float64,2}:\n",
       "  1.0   0.0   1.0   1.0\n",
       " -0.0  -0.0   0.0  -0.0\n",
       "  0.0   1.0  -0.0   0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(c_kt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "With the model now optimized, I will use the variable outputs from Gurobi via the JuMP API to determine how well the model performs.  To do so, I first establish a set of helper functions.  Firstly, I will extract the label predictions for each leaf node $t_l$, then build a function that will test whether a given $x_i$ input fits into a given leaf node, and lastly, build an aggregate function that iterates over each leaf node and attempts to fit every point into a leaf node until a match is found.  This final function will output a total accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictTestPoints (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function labelLeafNodePredictions(N_kt)\n",
    "    #extract dimensions of N_kt array\n",
    "    rows = axes(N_kt)[1]\n",
    "    cols = axes(N_kt)[2]\n",
    "    \n",
    "    #initialize empty array to store label of prediction at each leaf node\n",
    "    #note that here we re-index leaf nodes such that the first leaf node is index 1\n",
    "    prediction = zeros(Int8, length(cols))\n",
    "    #prediction_index = cols .- (length(cols) - 1)\n",
    "    #iterate over each row in N_kt, saving max value\n",
    "    for c in cols\n",
    "        current_max = 0\n",
    "        current_prediction = 0\n",
    "        for r in rows\n",
    "            if value(N_kt[r,c]) > current_max\n",
    "                current_max = value(N_kt[r,c])\n",
    "                current_prediction = r\n",
    "            end\n",
    "        end\n",
    "        #push best prediction to output array\n",
    "        prediction[c- (length(cols) - 1)] = current_prediction\n",
    "    end\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "function checkTreePoint(left_turns, right_turns, feature_row)\n",
    "    #check if left turns are correct for leaf node\n",
    "    for node in left_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row < value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    #check if right turns are correct for leaf node\n",
    "    for node in right_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row >= value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end            \n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function predictTestPoints(test_features, test_labels)\n",
    "    #ext variable\n",
    "    correct_predictions = length(test_labels)\n",
    "    \n",
    "    #copy test labels\n",
    "    lbls = copy(test_labels)\n",
    "    \n",
    "    #extract prediction from each leaf node\n",
    "    leaf_predictions = labelLeafNodePredictions(N_kt)\n",
    "    \n",
    "    #iterate over leaf nodes\n",
    "    for (leaf_index, leaf_value) in enumerate(t_l)\n",
    "        left_turns = A_left[leaf_value]\n",
    "        right_turns = A_right[leaf_value]\n",
    "        #skip empty leaves\n",
    "        if leaf_predictions[leaf_index] == 0\n",
    "            #println(\"no predictions at node \", leaf_value)\n",
    "            continue\n",
    "        end\n",
    "        for (label_index, label_value) in enumerate(lbls)\n",
    "            if label_value != leaf_predictions[leaf_index]\n",
    "                #println(\"skipping row - not matched to prediction of node\")\n",
    "                continue\n",
    "            end\n",
    "            if label_value == 0\n",
    "                println(\"skipping row - already correctly placed\")\n",
    "                continue\n",
    "            end       \n",
    "            #println(\"prediction at node \", leaf_value, \" is \", leaf_predictions[leaf_index])\n",
    "            #println(\"left ancestors are \", left_turns, \" ,right ancestors are \", right_turns)\n",
    "            if checkTreePoint(left_turns, right_turns, test_features[label_index,:])\n",
    "                lbls[label_index] = 0\n",
    "                #println(\"found one!\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    errors = correct_predictions - countmap(lbls)[0]\n",
    "    return 1 - (errors/correct_predictions)\n",
    "end\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these scoring functions in place, I can score the model's performance against the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5130940834141611"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5407239819004526"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
