{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#package installer to use as needed\n",
    "#import Pkg\n",
    "#Pkg.add(\"MLDataUtils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-05 12:45:42.254"
     ]
    }
   ],
   "source": [
    "using Dates\n",
    "print(Dates.today(), \" \", Dates.Time(Dates.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Decision Trees\n",
    "\n",
    "## Stephen Ronkowski\n",
    "\n",
    "The following is an Julia 1.x implementation of the Optimal Decision Tree model developed by Bertsimas and Dunn in their [2017 paper](https://www.mit.edu/~dbertsim/papers/Machine%20Learning%20under%20a%20Modern%20Optimization%20Lens/Optimal_classification_trees_MachineLearning.pdf).  Traditional Decision Tree models are perhaps the most easily interpretated classification model available to the data scientist. These models, however, suffer from the inherently greedy nature of their solving methods, which use a loss function at each level of depth for the Tree. This methodology is computationally efficient, but it gains that efficiency at the possible cost of accuracy. \n",
    "\n",
    "Given the massive advances in both computational power and solver performance, it is now computationally feasible to compose a Decision Tree using a single loss function using Mixed Integer Optimization, ensuring that splits made in early branches of the tree generate the lowest possible error rates in the leaf nodes.\n",
    "\n",
    "I begin reproducing this model by loading the needed Julia packages.  While I chose Gurobi for a solver, other MIO-compatible solvers can easily be deployed by changing the declaration below and then modifying the model delcaration appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load needed modules\n",
    "using JuMP\n",
    "using CSV\n",
    "using DecisionTree\n",
    "using StatsBase\n",
    "using DataFrames\n",
    "using MLDataUtils\n",
    "\n",
    "#note - Gurobi is not FOSS - licensing required!\n",
    "#this model can be solved using any MIO solver compatible with JuMP\n",
    "#see http://www.juliaopt.org/JuMP.jl/v0.20.0/installation/#Getting-Solvers-1\n",
    "using Gurobi\n",
    "\n",
    "#data path declaration\n",
    "FILEDIR = \"/home/sronk/Downloads/Machine_Learning_MSCA_31009/Homework/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion & Processing\n",
    "\n",
    "Both for ease of reference when establishing the model variables \\& constraints and as a point of comparison with a \"traditional\" Decision Tree, I will load in a dataset to classify.  I chose the UCI \"Wine\" dataset available on [their website](https://archive.ics.uci.edu/ml/datasets/Wine), but as I will demonstrate in other notebooks, this general model should work for any classification dataset.\n",
    "\n",
    "I will load the dataset from CSV, and shuffle the rows to ensure an even distribution of classes in the test and train sets.  Once the data is split into test and train sets, I will extract the label column and place it into a seperate array.  Also, since the model requires that all feature values are $0 \\leq x_i \\leq 1$, I will use a unit range transformation on each feature column to meet this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Array{Int64,1}:\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load wine file\n",
    "csv = CSV.File(FILEDIR * \"wine.data\")\n",
    "df = DataFrame(csv)\n",
    "\n",
    "#extract feature array, fit to UnitTransformer\n",
    "features_to_fit = Matrix(select(df, Not(:Label)))\n",
    "unit_transformer = fit(UnitRangeTransform, transpose(features_to_fit))\n",
    "\n",
    "#shuffle observations and split into train/test\n",
    "df = shuffleobs(df)\n",
    "train, test = splitobs(df, at = 0.7)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "features = Matrix(select(train, Not(:Label)))\n",
    "features = StatsBase.transform(unit_transformer, transpose(features))\n",
    "features = Matrix(transpose(features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "labels = Array(train.Label)\n",
    "\n",
    "#extract features from input matrix, unit transform\n",
    "test_features = Matrix(select(test, Not(:Label)))\n",
    "test_features = StatsBase.transform(unit_transformer, transpose(test_features))\n",
    "test_features = Matrix(transpose(test_features))\n",
    "\n",
    "#extract labels from input matrix\n",
    "test_labels = Array(test.Label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels must be one-indexed for the model to function properly, so I confirm as much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 3\n",
       " 2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = labels .- 2\n",
    "unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of comparison, I will now fit a simple Decision Tree over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 13, Threshold 0.34022824536376606\n",
      "L-> Feature 10, Threshold 0.21715017064846415\n",
      "    L-> 2 : 41/41\n",
      "    R-> Feature 7, Threshold 0.2236286919831223\n",
      "        L-> 3 : 30/30\n",
      "        R-> 2 : 6/7\n",
      "R-> Feature 7, Threshold 0.38502109704641346\n",
      "    L-> Feature 4, Threshold 0.4948453608247423\n",
      "        L-> 3 : 3/3\n",
      "        R-> 2 : 2/2\n",
      "    R-> 1 : 42/42\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "DecisionTree.fit!(dt_model, features, labels)\n",
    "print_tree(dt_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "The algorithm requires a set of dynamically generated constants that are derived from the dataset.  The first of these is epsilon, $\\epsilon_t$. Epsilon is essentially a \"fudge factor\" to overcome MIO solvers' inability to solve strict inequalities. Since the solver will not accept $a^Tx < b_t$, we find the smallest non-zero difference in array $x$ and instead use $a^Tx  + \\epsilon_t \\leq b_t$ for this constraint.\n",
    "\n",
    "$A_L$ and $A_R$ are a way of mathematically modelling the Decision Tree itself.  For a given leaf node $t$ within the Decison Tree, $A_L$ is the set of all left turns made to arrive at that leaf, and $A_L$ is the set of all right turns. For example, in the tree below, $A_L(4) = \\{1, 2\\}$.\n",
    "\n",
    "![title](tree_diagram.png)\n",
    "\n",
    "Additionally, we need to generate matrix $Y$ for use in the loss calculation constraints within the model space.  For a given $x_i$ with known label $k$ in the training set, $Y_{i,k} = 1$ if $x_i = k$; otherwise $Y_{i,k} = -1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeYMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function findEpsilon(array)\n",
    "    #capture array length for iteration\n",
    "    array_len = size(array, 1)\n",
    "    \n",
    "    #sort array\n",
    "    sorted_array = sort(array, rev = true)\n",
    "    \n",
    "    #initialize epsilon with arbitrarily large value\n",
    "    epsilon = 1e5\n",
    "    \n",
    "    #iterate through consecutive values to find smallest non-zero difference\n",
    "    for i in 2:(array_len - 1)\n",
    "        diff = abs(sorted_array[i-1] - sorted_array[i])\n",
    "        if 0 < diff < epsilon\n",
    "            epsilon = diff\n",
    "        end\n",
    "    end\n",
    "    #output smallest non-zero difference\n",
    "    return epsilon\n",
    "end\n",
    "\n",
    "function epsilonArrayGenerator(matrix)\n",
    "    #capture number of features in dataset, i.e. the number of columns\n",
    "    matrix_rows = size(matrix, 2)\n",
    "    \n",
    "    #initialize output DataFrame\n",
    "    epsilon_array = Vector{Float64}(undef,matrix_rows)\n",
    "    \n",
    "    #iterate findEpsilon function over each column\n",
    "    for col in 1:matrix_rows\n",
    "        epsilon_array[col] = findEpsilon(matrix[:,col])\n",
    "    end\n",
    "    return epsilon_array\n",
    "end\n",
    "\n",
    "function makeAncestorDict(max_nodes)\n",
    "    #initialize empty dictionaries\n",
    "    A_left = Dict{Int64, Vector{Int64}}()\n",
    "    A_right = Dict{Int64, Vector{Int64}}()\n",
    "    #A_left[1] = [1]\n",
    "    #A_right[1] = [1]\n",
    "    #generate keys with empty array values for each node\n",
    "    for i in 1:max_nodes\n",
    "        A_left[i] = []\n",
    "        A_right[i] = []\n",
    "    end\n",
    "    #loop over all nodes, copying the left and right ancestors of the node above it\n",
    "    for i in 2:max_nodes\n",
    "        left_ancestors = copy(A_left[i ÷ 2])\n",
    "        right_ancestors = copy(A_right[i ÷ 2])\n",
    "        direct_ancestor = i ÷ 2\n",
    "        A_left[i] = left_ancestors\n",
    "        A_right[i] = right_ancestors\n",
    "        #add a left ancestor to even nodes\n",
    "        if i/2 == i ÷ 2\n",
    "            append!(left_ancestors, direct_ancestor)\n",
    "            A_left[i] = left_ancestors\n",
    "        #add a right ancestor to odd nodes\n",
    "        else\n",
    "            append!(right_ancestors, direct_ancestor)\n",
    "            A_right[i] = right_ancestors\n",
    "        end\n",
    "    end\n",
    "    return A_left, A_right\n",
    "end\n",
    "\n",
    "function makeYMatrix(labels)    \n",
    "    #extract dimensions for Y from label array\n",
    "    num_labels = length(unique(labels))\n",
    "    len_df = length(labels)\n",
    "    \n",
    "    #initialize empty matrix\n",
    "    Y = zeros(len_df, num_labels)\n",
    "    \n",
    "    #set all values to -1 - this will apply a penalty to incorrect predictions\n",
    "    Y = Y  .- 1\n",
    "    \n",
    "    #iterate n over each column, setting Y[n,k] = 1 when the label for x[i] = k\n",
    "    for k in 1:num_labels\n",
    "        for n in 1:len_df\n",
    "            if labels[n] == k\n",
    "                Y[n,k] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration of Model Constants\n",
    "\n",
    "In a given model space for this optimization function, there are a variety of constants that are either set by the user as a hyperparameter or generated dynamically based on those hyperparameters or the input data set.\n",
    "\n",
    "I will begin by establishing the numerical values related to the structure of the tree, $D, N_{min}, t$, and $\\alpha$  Here, $D$ is the maximum depth of the Tree, $N_{min}$ is the minimum number of samples needed to compose a leaf node, and $t$ is the total possible number of nodes in a tree.\n",
    "\n",
    "The last of these user-declared variables, $\\alpha$, is a complexity parameter that penalizes overly complex tree structures. (elaborate here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Int64,1}:\n",
       "  8\n",
       "  9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set maximum depth of tree as a constant\n",
    "max_depth = 3\n",
    "\n",
    "#minimum number of values for a given leaf node\n",
    "leaf_n_min = 3\n",
    "\n",
    "#declare complexity parameter alpha\n",
    "alpha = 0.1\n",
    "\n",
    "#find total number of nodes in tree using max_depth, t\n",
    "max_nodes = 2^(max_depth+1) - 1\n",
    "\n",
    "#initialize branch and leaf node arrays - first, find the split point between branch and leaf indices\n",
    "#split point is by definition the number of nodes integer divided by two\n",
    "leaf_branch_split = max_nodes ÷ 2\n",
    "\n",
    "#total number of branches\n",
    "t_b = collect(1:leaf_branch_split)\n",
    "\n",
    "#total number of leaves\n",
    "t_l = collect(leaf_branch_split+1:max_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will establish the model constants that are derived from the input dataset: $n, k$, and $p$.  Here, $n$ represents the number of samples within the dataset, $p$ represents the number of features in the dataset, and $k$ represents the total number of labels.\n",
    "\n",
    "I will also establish the constant $\\hat{L}$. This constant represents the \"naive\" prediction, namely, that every value in the dataset is a member of the most common class.  This value is then used within the optimization function (elaborate here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull number of samples in dataset, n\n",
    "num_samples = size(features, 1)\n",
    "\n",
    "#find total number of columns in the feature space, p\n",
    "num_features = size(features, 2)\n",
    "\n",
    "#find total number of labels, k\n",
    "num_labels = length(unique(labels))\n",
    "\n",
    "#create dictionary with prediction labels as key and count of each prediction as value\n",
    "output_count = countmap(labels)\n",
    "\n",
    "#extract the count for most common label to form l_hat, which is baseline accuracy rate\n",
    "l_hat = sort(collect(output_count), by = tuple -> last(tuple), rev=true)[1,1][2]/length(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I will build out the other dataset-specific constants - $A_L, A_R$, and $\\epsilon_j$ - using the helper functions declared earlier.  With these values established, we can then define $M_1$ as 1 + $max(\\epsilon_j)$ and $M$ as the number of features in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125×3 Array{Float64,2}:\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       "  ⋮              \n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0\n",
       " -1.0   1.0  -1.0\n",
       " -1.0   1.0  -1.0\n",
       "  1.0  -1.0  -1.0\n",
       " -1.0   1.0  -1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build ancestor dictionaries using definitions given above\n",
    "A_left, A_right = makeAncestorDict(max_nodes)\n",
    "\n",
    "#generate the epsilon array as defined earlier\n",
    "epsilon_array = epsilonArrayGenerator(features)\n",
    "\n",
    "#M_1 constant - defined as 1 plus the largest epsilon value\n",
    "M_1 = 1 + maximum(epsilon_array)\n",
    "\n",
    "#M constant - set equal to number of samples as rule of thumb\n",
    "M = length(labels)\n",
    "\n",
    "#generate Y matrix\n",
    "Y = makeYMatrix(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable and Constraint Declarations\n",
    "\n",
    "With the constants now fixed, the model itself can now be built inside JuMP.  The first step is to declare the model itself, at which point I also limit the runtime of the optimizer to two hours.\n",
    "\n",
    "From there, I will begin building the model by establishing the variables that model the structure of the tree itself.  The first of these are $b$, an array that captures the decision point for each node that applies a split, and $a_{j,t}$, a hot-coded matrix that indicates when feature $j$ is used to split at node $t$. Additionally, we initialize array $d$, which is hot-coded to indicate when a given branch is active (i.e. a split is applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model = Model(with_optimizer(Gurobi.Optimizer, Presolve=0, OutputFlag=1, TimeLimit=7200))\n",
    "\n",
    "#b is the decision point for each branch node\n",
    "#s/t a.T*x < b at a given split \n",
    "@variable(model, b[i=t_b])\n",
    "\n",
    "#a is a hot-coded matrix that captures the variable being used to split at given branch node\n",
    "@variable(model, a[j = 1:num_features, t = 1:leaf_branch_split])\n",
    "\n",
    "#4 - establish binary constraint on a\n",
    "for j in 1:num_features\n",
    "    for t in t_b\n",
    "        @constraint(model, a[j,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#d is an indicator array equal to one when a split is applied at a given node\n",
    "@variable(model, d[1:leaf_branch_split])\n",
    "\n",
    "#constrain d to binary values\n",
    "for t in t_b\n",
    "    @constraint(model, d[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#2 - establish that row-wise sum of a must equal 1 for all rows - yes\n",
    "for t in t_b\n",
    "    #@constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == d[t])\n",
    "    @constraint(model, sum(a[j,t] for j=1:num_features) == 1)\n",
    "end\n",
    "\n",
    "#3 - establish that b split point must be 0 <= b[i] <= d[i] - yes\n",
    "for t in t_b\n",
    "    @constraint(model, b[t] >= 0)\n",
    "    @constraint(model, d[t] >= b[t])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is possible that certain branches will not be needed to achieve an optimal solution, I will establish that only those nodes which have a split applied above them can also apply a split.  This constraint ensures that once the optimizer no longer needs to split along a given branch path (i.e. that a given branch has already achieved perfect purity) it will simply route the input features to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- constrain d s/t splits cannot be applied below a node that does not also split\n",
    "#this does not apply to d[1] since that is the parent node and must always split\n",
    "@constraint(model, d[1] == 1)\n",
    "\n",
    "for t in 2:leaf_branch_split\n",
    "    parent = t ÷ 2\n",
    "    @constraint(model, d[t] <= d[parent])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z is a hot-coded matrix captures which values are assigned to which node\n",
    "@variable(model, z[i = 1:num_samples, t = t_l])\n",
    "\n",
    "#constrain z to binary values {0,1}\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#l is a hot-coded array s/t l(t) = 1 when leaf node t contains any values \n",
    "@variable(model, l[t = t_l])\n",
    "\n",
    "#contrain l to binary values {0,1}\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] in MOI.ZeroOne())\n",
    "end\n",
    "\n",
    "#6 - constrain predictions to only be fit into nodes containing points\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        @constraint(model, z[i,t] <= l[t])\n",
    "    end\n",
    "end\n",
    "\n",
    "#7- constrain number of samples assigned to a given leaf by lower bound \n",
    "#s/t number of samples is always greater/equal to min leaf size constant\n",
    "for t in t_l\n",
    "    @constraint(model, sum(z[i,t] for i in 1:num_samples) >= leaf_n_min * l[t])\n",
    "end\n",
    "\n",
    "#8 - constrain each point in data set so it can only be assigned to one leaf node\n",
    "for i in 1:num_samples\n",
    "    @constraint(model, sum(z[i,t] for t in t_l) == 1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tree structure and variable spaces now established, I move on to the splitting constraints.  These constraints capture the \"path\" that leads to each leaf node.  For leaf node $t$, the left-hand path is taken at ancestor node(s) $A_{left}(t) = m$ when $A^T(x_i + \\epsilon) \\leq b(m)$, and the right-hand path is taken at ancestor node(s) $A_{right}(t) = m$ when $A^Tx_i \\geq b(m)$.\n",
    "\n",
    "(Note that equations 9-12 are intermediate steps that give the derivations below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 - establish left split constraints\n",
    "for i in 1:num_samples  \n",
    "    for t in t_l\n",
    "        for m in A_left[t]\n",
    "            @constraint(model, transpose(a[:,m]) * (features[i,:] + epsilon_array) <= b[m] + M_1*(1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "#14 - establish right split contraints\n",
    "for i in 1:num_samples\n",
    "    for t in t_l\n",
    "        for m in A_right[t]      \n",
    "            @constraint(model, transpose(a[:,m]) * features[i,:] >= b[m] - (1 - z[i,t]))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the branch nodes now fully modelled, we turn to a series of variables that capture the $x_i$ features present within each node.  The first of these is $N_{k,t}$, which gives the total number of inputs with label $k$ in leaf node $t$.  Paired with this is $N_t$, which gives the sum total of inputs assigned to each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_kt is the number of points with label k in leaf node t\n",
    "@variable(model, N_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#15 - establish values for N_kt[t]\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, N_kt[k,t] == 0.5 * sum((1 + Y[i,k])*z[i,t] for i = 1:num_samples))\n",
    "    end\n",
    "end\n",
    "\n",
    "#N_t is the total number of values in a leaf node t\n",
    "@variable(model, N_t[i = t_l])\n",
    "\n",
    "#16 - establish values for N_t[t] as sum of z[i,t] for each t\n",
    "for t in t_l\n",
    "    @constraint(model, N_t[t] == sum(z[i,t] for i = 1:num_samples))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with these variables, we capture the prediction made by each node in matrix $c_{k,t}$, which is hot-coded such that the prediction for leaf $t$ is $k$ when $c_{k,t} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_kt is a matrix that holds the label count of each variable within a given leaf nodes\n",
    "@variable(model, c_kt[i = 1:num_labels, j = t_l])\n",
    "\n",
    "#constrain c_kt to binary values {0,1}\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, c_kt[k,t] in MOI.ZeroOne())\n",
    "    end\n",
    "end\n",
    "\n",
    "#18 - force prediction for each node with values\n",
    "for t in t_l\n",
    "    @constraint(model, l[t] == sum(c_kt[k,t] for k = 1:num_labels))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define loss array $L_t$, which is derived by applying a penalty factor for each prediction not in the majority class present in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L is the loss for a given leaf node t\n",
    "@variable(model, L[i = t_l])\n",
    "\n",
    "#20 - set loss function lower bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] >= N_t[t] - N_kt[k,t] - (M * (1 - c_kt[k,t])))\n",
    "    end\n",
    "end\n",
    "\n",
    "#21 - set loss function upper bound\n",
    "for k in 1:num_labels\n",
    "    for t in t_l\n",
    "        @constraint(model, L[t] <= N_t[t] - N_kt[k,t] + (M * c_kt[k,t]))\n",
    "    end\n",
    "end\n",
    "\n",
    "#22 - set all L values to be positive\n",
    "for t in t_l\n",
    "   @constraint(model, L[t] >= 0) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now set the objective function, which is to minimize loss relative to the complexity of the tree as measured by the number of active branch nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ 2.5510204081632653 L_{8} + 2.5510204081632653 L_{9} + 2.5510204081632653 L_{10} + 2.5510204081632653 L_{11} + 2.5510204081632653 L_{12} + 2.5510204081632653 L_{13} + 2.5510204081632653 L_{14} + 2.5510204081632653 L_{15} + 0.1 d_{1} + 0.1 d_{2} + 0.1 d_{3} + 0.1 d_{4} + 0.1 d_{5} + 0.1 d_{6} + 0.1 d_{7} $$"
      ],
      "text/plain": [
       "2.5510204081632653 L[8] + 2.5510204081632653 L[9] + 2.5510204081632653 L[10] + 2.5510204081632653 L[11] + 2.5510204081632653 L[12] + 2.5510204081632653 L[13] + 2.5510204081632653 L[14] + 2.5510204081632653 L[15] + 0.1 d[1] + 0.1 d[2] + 0.1 d[3] + 0.1 d[4] + 0.1 d[5] + 0.1 d[6] + 0.1 d[7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MOI.set(model, MOI.RawParameter(\"TimeLimit\"), 100.0)\n",
    "#Gurobi.setparam!(backend(model).optimizer.model.inner, \"TimeLimit\", 100.0)\n",
    "@objective(model, Min, (1/l_hat) * sum(L[t] for t in t_l)) + (alpha * sum(d[t] for t in t_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function warmStartODT()\n",
    "    set_start_value(a[1], 13)\n",
    "    set_start_value(b[1], 0.3402)\n",
    "    set_start_value(a[2], 10)\n",
    "    set_start_value(b[2], 0.21715)\n",
    "    set_start_value(a[3], 7)\n",
    "    set_start_value(b[3], 0.21715)\n",
    "end\n",
    "\n",
    "warmStartODT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective function now established, we call the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] pass_constraints(::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, ::MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}, ::Bool, ::MathOptInterface.Utilities.IndexMap, ::Array{DataType,1}, ::Array{Array{MathOptInterface.ConstraintIndex{MathOptInterface.SingleVariable,MathOptInterface.ZeroOne},1},1}, ::Array{DataType,1}, ::Array{Array{T,1} where T,1}, ::typeof(MathOptInterface.Utilities.copy_constraints), ::typeof(MathOptInterface.set)) at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/copy.jl:265",
      " [2] pass_constraints(::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, ::MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}, ::Bool, ::MathOptInterface.Utilities.IndexMap, ::Array{DataType,1}, ::Array{Array{MathOptInterface.ConstraintIndex{MathOptInterface.SingleVariable,MathOptInterface.ZeroOne},1},1}, ::Array{DataType,1}, ::Array{Array{T,1} where T,1}) at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/copy.jl:240",
      " [3] default_copy_to(::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, ::MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}, ::Bool) at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/copy.jl:340",
      " [4] #automatic_copy_to#97 at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/copy.jl:15 [inlined]",
      " [5] #automatic_copy_to at ./none:0 [inlined]",
      " [6] #copy_to#3 at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Bridges/bridge_optimizer.jl:254 [inlined]",
      " [7] (::getfield(MathOptInterface, Symbol(\"#kw##copy_to\")))(::NamedTuple{(:copy_names,),Tuple{Bool}}, ::typeof(MathOptInterface.copy_to), ::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, ::MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}) at ./none:0",
      " [8] attach_optimizer(::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.AbstractOptimizer,MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}) at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/cachingoptimizer.jl:149",
      " [9] optimize!(::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.AbstractOptimizer,MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.Model{Float64}}}) at /home/sronk/.julia/packages/MathOptInterface/A2UPd/src/Utilities/cachingoptimizer.jl:185",
      " [10] #optimize!#78(::Bool, ::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(optimize!), ::Model, ::Nothing) at /home/sronk/.julia/packages/JuMP/MsUSY/src/optimizer_interface.jl:141",
      " [11] optimize! at /home/sronk/.julia/packages/JuMP/MsUSY/src/optimizer_interface.jl:111 [inlined] (repeats 2 times)",
      " [12] top-level scope at In[22]:1"
     ]
    }
   ],
   "source": [
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Outputs\n",
    "\n",
    "While the verbose output of Gurobi confirms that the optimizer ran successfully, I confirm as much below via a call to JuMP.  As expected, we see that the optimizer ended once it reached its time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIME_LIMIT::TerminationStatusCode = 12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_status(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now pull the various optimized outputs of the function.  $L_t$ gives the loss values at each leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 8-element Array{Float64,1}:\n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.0               \n",
       " 0.9999999995775993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{j,t}$ gives the feature $j$ used to apply a split at node $t$, and $b_t$ gives the split-point value of variable $j$ at node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×7 Array{Float64,2}:\n",
       "  0.0  -0.0  -0.0   1.0   0.0   0.0   0.0\n",
       "  0.0   0.0   0.0   0.0  -0.0   0.0  -0.0\n",
       " -0.0  -0.0   0.0  -0.0  -0.0   0.0   0.0\n",
       "  0.0   1.0  -0.0   0.0  -0.0   0.0   0.0\n",
       " -0.0  -0.0  -0.0  -0.0  -0.0   0.0  -0.0\n",
       "  0.0  -0.0   0.0   0.0  -0.0  -0.0  -0.0\n",
       "  1.0  -0.0  -0.0   0.0  -0.0  -0.0   0.0\n",
       "  0.0   0.0   0.0  -0.0   0.0  -0.0   0.0\n",
       " -0.0  -0.0   0.0   0.0   0.0   0.0  -0.0\n",
       "  0.0  -0.0   0.0   0.0   1.0  -0.0   1.0\n",
       "  0.0  -0.0  -0.0   0.0   0.0   0.0  -0.0\n",
       " -0.0  -0.0  -0.0  -0.0   0.0  -0.0   0.0\n",
       "  0.0  -0.0   1.0  -0.0  -0.0   1.0   0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [1, 2, 3, 4, 5, 6, 7]\n",
       "And data, a 7-element Array{Float64,1}:\n",
       " 0.2616033755274261 \n",
       " 0.3556701030927836 \n",
       " 0.2867332382310994 \n",
       " 0.46052631578947356\n",
       " 0.23208191126279853\n",
       " 0.09129814550641951\n",
       " 0.1774744027303755 "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_t$ gives the total number of $x_i$ input points assigned to leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-dimensional DenseAxisArray{Float64,1,...} with index sets:\n",
       "    Dimension 1, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 8-element Array{Float64,1}:\n",
       "  6.0\n",
       "  0.0\n",
       "  5.0\n",
       " 30.0\n",
       "  9.0\n",
       " 26.0\n",
       "  4.0\n",
       " 45.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we see the values for $N_{k,t}$, which stores the total number of inputs with label $k$ in leaf node $t$, and $c_{k,t}$, which gives the prediction $k$ in leaf node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 3×8 Array{Float64,2}:\n",
       " -0.0  0.0  -0.0  -0.0  0.0  -0.0  -0.0  44.0\n",
       "  6.0  0.0   5.0  -0.0  9.0  26.0   4.0   1.0\n",
       " -0.0  0.0   0.0  30.0  0.0   0.0  -0.0   0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(N_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
       "    Dimension 1, 1:3\n",
       "    Dimension 2, [8, 9, 10, 11, 12, 13, 14, 15]\n",
       "And data, a 3×8 Array{Float64,2}:\n",
       "  0.0  0.0  -0.0  -0.0  -0.0  -0.0  -0.0   1.0\n",
       "  1.0  0.0   1.0   0.0   1.0   1.0   1.0  -0.0\n",
       " -0.0  0.0  -0.0   1.0  -0.0  -0.0   0.0  -0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.(c_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_previous_oct(model_name)\n",
    "    variable_space = all_variables(model_name)\n",
    "    variable_values = value.(variable_space)\n",
    "    return variable_values\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "With the model now optimized, I will use the variable outputs from Gurobi via the JuMP API to determine how well the model performs.  To do so, I first establish a set of helper functions.  Firstly, I will extract the label predictions for each leaf node $t_l$, then build a function that will test whether a given $x_i$ input fits into a given leaf node, and lastly, build an aggregate function that iterates over each leaf node and attempts to fit every point into a leaf node until a match is found.  This final function will output a total accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelLeafNodePredictions (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function labelLeafNodePredictions(N_kt)\n",
    "    #extract dimensions of N_kt array\n",
    "    rows = axes(N_kt)[1]\n",
    "    cols = axes(N_kt)[2]\n",
    "    \n",
    "    #initialize empty array to store label of prediction at each leaf node\n",
    "    #note that here we re-index leaf nodes such that the first leaf node is index 1\n",
    "    prediction = zeros(Int8, length(cols))\n",
    "    #prediction_index = cols .- (length(cols) - 1)\n",
    "    #iterate over each row in N_kt, saving max value\n",
    "    for c in cols\n",
    "        current_max = 0\n",
    "        current_prediction = 0\n",
    "        for r in rows\n",
    "            if value(N_kt[r,c]) > current_max\n",
    "                current_max = value(N_kt[r,c])\n",
    "                current_prediction = r\n",
    "            end\n",
    "        end\n",
    "        #push best prediction to output array\n",
    "        prediction[c- (length(cols) - 1)] = current_prediction\n",
    "    end\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "function checkTreePoint(left_turns, right_turns, feature_row)\n",
    "    #check if left turns are correct for leaf node\n",
    "    for node in left_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row < value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    #check if right turns are correct for leaf node\n",
    "    for node in right_turns\n",
    "        if transpose(value.(a[:,node])) * feature_row >= value(b[node])\n",
    "            continue\n",
    "        else\n",
    "            return false\n",
    "        end            \n",
    "    end\n",
    "    return true\n",
    "end\n",
    "\n",
    "function predictTestPoints(test_features, test_labels)\n",
    "    #ext variable\n",
    "    correct_predictions = length(test_labels)\n",
    "    \n",
    "    #copy test labels\n",
    "    lbls = copy(test_labels)\n",
    "    \n",
    "    #extract prediction from each leaf node\n",
    "    leaf_predictions = labelLeafNodePredictions(N_kt)\n",
    "    \n",
    "    #iterate over leaf nodes\n",
    "    for (leaf_index, leaf_value) in enumerate(t_l)\n",
    "        left_turns = A_left[leaf_value]\n",
    "        right_turns = A_right[leaf_value]\n",
    "        #skip empty leaves\n",
    "        if leaf_predictions[leaf_index] == 0\n",
    "            #println(\"no predictions at node \", leaf_value)\n",
    "            continue\n",
    "        end\n",
    "        for (label_index, label_value) in enumerate(lbls)\n",
    "            if label_value != leaf_predictions[leaf_index]\n",
    "                #println(\"skipping row - not matched to prediction of node\")\n",
    "                continue\n",
    "            end\n",
    "            if label_value == 0\n",
    "                println(\"skipping row - already correctly placed\")\n",
    "                continue\n",
    "            end       \n",
    "            #println(\"prediction at node \", leaf_value, \" is \", leaf_predictions[leaf_index])\n",
    "            #println(\"left ancestors are \", left_turns, \" ,right ancestors are \", right_turns)\n",
    "            if checkTreePoint(left_turns, right_turns, test_features[label_index,:])\n",
    "                lbls[label_index] = 0\n",
    "                #println(\"found one!\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    errors = correct_predictions - countmap(lbls)[0]\n",
    "    return 1 - (errors/correct_predictions)\n",
    "end\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these scoring functions in place, I can score the model's performance against the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433962264150944"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictTestPoints(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
